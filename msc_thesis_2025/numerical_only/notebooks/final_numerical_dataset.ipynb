{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qp1-ETbItTxO"
   },
   "source": [
    "<h1><center><u> Code Implementation </u></center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core Thesis Work: Missing Data Imputation on Numerical DHS Dataset\n",
    "---\n",
    "---\n",
    "### Code Implementation Walkthrough\n",
    "\n",
    "This notebook walks through the full implementation used in the main experimental part of the thesis, focusing on missing data imputation for **purely numerical DHS and synthetic datasets**.\n",
    "\n",
    "> • **Goal**: Demonstrate missing data imputation with *purely numerical* DHS and synthetic datasets using a modular, scalable pipeline.\n",
    ">\n",
    "> • **Code Folder**: All core logic for data loading, imputation and evaluation is organized in the `src/` directory for clarity and reusability.\n",
    ">\n",
    "> • **Notebook Purpose**: Designed for running and visualizing the full experimental pipeline. This notebook complements the *Main Experimental Study* section of the thesis by supporting exploratory data analysis (EDA) and final metric evaluation by running the full experimental pipeline.\n",
    ">\n",
    "> • **Explore**: See `src/preprocessing.py`, `src/imputations.py`, `src/evaluation.py`, and `src/utils.py` for detailed implementation of the core components.\n",
    "\n",
    "---\n",
    "\n",
    "## Methodology Overview\n",
    "\n",
    "This project uses a range of classical, machine learning and deep learning methods to impute missing data in real-world and synthetic numerical datasets. Steps are structured to ensure consistency, fairness and clarity in evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## Flowchart Overview\n",
    "\n",
    "Below is a visual representation of the full pipeline used for the thesis:\n",
    "\n",
    "<img src=\"Flowchart_thesis_25_april.jpg\" alt=\"Methodology Flowchart\" width=\"900\"/>\n",
    "\n",
    "---\n",
    "\n",
    "## Key Steps in the Methodology\n",
    "\n",
    "#### 1. Data Preprocessing\n",
    "- **Source:** Data collected from the DHS Program Website.\n",
    "- **Steps Involved:**\n",
    "  1. Initial extensive data preprocessing to clean and organize the data.\n",
    "  2. Filtering data to remove irrelevant or redundant entries.\n",
    "  3. Exploratory Data Analysis (EDA) to understand missingness patterns and distributions are analyzed during EDA.\n",
    "\n",
    "#### 2. Dataset Preparation\n",
    "\n",
    "- **Handling Missing Values:**\n",
    "  - Three strategies:\n",
    "    1. Dropping all NaNs for a more restrictive dataset (Complete Dataset or **'drop_all_nans'** asked in prompt).\n",
    "    2. Dropping entire survey groups (column named 'Meta; GEID_init' having several survey IDs) that have more than 20% missing data or (**'numerical_only_drop_20_percentage_nans'** asked in prompt).\n",
    "    3. Using initial KNN Imputation for initial filling of NaNs in the entire numerical dataset (**'numerical_only'** asked in prompt).\n",
    "\n",
    "- **Cross-Validation:**\n",
    "  - K-Fold cross-validation splits Data into training, validation and test folds.\n",
    "  - Standard scaling is applied to ensure consistent feature ranges.\n",
    "\n",
    "#### 3. Model Training and Validation\n",
    "\n",
    "- **Two types of Missingness Introduced:**\n",
    "  - MAR (Missing At Random) and MCAR (Missing Completely At Random).\n",
    "- **Pre-Imputation:**\n",
    "  - Initial KNN imputation is used to preprocess data for autoencoders (only for AE, DAE, and VAE) as these can not be trained on a dataset with NaNs.\n",
    "- **Training Methods:**\n",
    "  - Statistical, Machine learning and Deep learning methods used:\n",
    "    - Mean, KNN, MICE Ridge\n",
    "    - Autoencoders (AE)\n",
    "    - Denoising Autoencoders (DAE)\n",
    "    - Variational Autoencoders (VAE)\n",
    "    - GAIN\n",
    "  - Methods are trained using either:\n",
    "    1. Datasets with artificial missing values (Mean, KNN, MICE Ridge, GAIN).\n",
    "    2. Pre-KNN Imputed datasets after artificial missingness creation (for Autoencoders only).\n",
    "\n",
    "#### 4. Hyperparameter Tuning and Testing\n",
    "\n",
    "- Methods are tuned using the training folds and validated on separate test folds to ensure unbiased final evaluation.\n",
    "- Final evaluation metrics are recorded under **both MAR and MCAR** scenarios.\n",
    "\n",
    "#### 5. Final Evaluation\n",
    "\n",
    "- **Performance Metrics:** \n",
    "\n",
    "  - RMSE (Root Mean Square Error)\n",
    "  - MAE (Mean Absolute Error)\n",
    "  - R² (R-squared)\n",
    "  - Pearson Correlation\n",
    "  - Results are compared across methods to determine the most effective approach.\n",
    "\n",
    "The methodology ensures robust data handling and model evaluation for missing data imputation. The flowchart visually represents each step, highlighting the workflow and key decisions made in the study.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "This end-to-end pipeline ensures reliable and consistent evaluation of imputation techniques on numerical DHS data. With reusable components and clear structure, the notebook supports both experimentation and reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and Setup\n",
    "\n",
    "Importing necessary libraries for data manipulation, statistical methods, machine learning, deep learning and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required libraries\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import joblib\n",
    "import pickle\n",
    "import random\n",
    "import string\n",
    "import itertools\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from icecream import ic\n",
    "import tensorflow as tf\n",
    "from scipy import stats\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from keras.regularizers import l2\n",
    "from scipy.stats import spearmanr\n",
    "from missforest import MissForest\n",
    "from tensorflow.keras import Model\n",
    "from collections import defaultdict\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import Ridge\n",
    "from tensorflow.keras.losses import mse\n",
    "from scipy.stats import levene, f_oneway\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import L2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from tensorflow.keras.initializers import he_uniform\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.losses import mse as keras_mse\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from tensorflow.keras.layers import Lambda, Layer, Input, Dense\n",
    "from sklearn.linear_model import BayesianRidge, LinearRegression\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense, Input, GaussianNoise, Layer\n",
    "from tensorflow.keras import Sequential, layers, models, optimizers, regularizers\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer, PolynomialFeatures, FunctionTransformer\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "import src.config_loader\n",
    "importlib.reload(src.config_loader)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.utils import *\n",
    "from src.setup_dirs import *\n",
    "from src.evaluation import *\n",
    "from src.run_pipeline import *\n",
    "from src.visualization import *\n",
    "from src.preprocessing import *\n",
    "from src.config_loader import *\n",
    "from src.gain import GAIN_code_v2\n",
    "from src.helper_functions import *\n",
    "from src.single_imputation import *\n",
    "from src.load_data import load_data\n",
    "from src.initial_imputation import *\n",
    "from src.deep_learning_methods import *\n",
    "from src.synthetic_data_generation import *\n",
    "from src.dhs_modelling_functions_new import final_ds_droping_cols, fold_generator\n",
    "\n",
    "# initialization\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setting up random seeds for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PGRLvmmdDrSH"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(6688)\n",
    "random.seed(6688)\n",
    "np.random.seed(6688)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Configuration\n",
    "\n",
    "Configuring TensorFlow to use GPUs and manage memory efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4D_reWvTDuEt"
   },
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu,True)\n",
    "    except RuntimeError as e:\n",
    "        raise e\n",
    "\n",
    "#use multiple GPUs\n",
    "gpus = [0] # may request more, if necessary\n",
    "gpus = [\"GPU:\" + str(i) for i in gpus]\n",
    "# https://keras.io/guides/distributed_training/\n",
    "print('gpus', gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference from https://github.com/gheisenberg/FoodSecurity/tree/main/DHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop All NaNs Function\n",
    "\n",
    "Drops rows and columns with NaNs, ensuring a clean dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_all_nans(df):\n",
    "    non_missing_indices = {}\n",
    "    data_with_missing = df.copy()\n",
    "    for col in data_with_missing.columns:\n",
    "        non_missing_indices[col] = data_with_missing[col].dropna().index\n",
    "    \n",
    "    values_df = pd.DataFrame()\n",
    "    for col, indices in non_missing_indices.items():\n",
    "        values_df[col] = data_with_missing.loc[indices, col].reset_index(drop=True)\n",
    "    \n",
    "    actual_values_df = values_df.dropna()\n",
    "    rows = actual_values_df.shape\n",
    "    print(f\"Shape after dropping all NaNs: {rows}\")\n",
    "    \n",
    "    return actual_values_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Configuration with directory creation\n",
    "\n",
    "Setting up configurations for real and synthetic data handling, including masking and directory setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading base configuration settings (real/synthetic, directory paths etc.)\n",
    "# for either real and (aggregated + numerical) DHS data or synthetic data\n",
    "config = load_config()\n",
    "# prompt user to select how missing values (NaNs) should be handled in the dataset\n",
    "config = select_process_nans_option(config)\n",
    "# prompt user to choose type of missingness simulation: MAR (with masking) or MCAR (without masking)\n",
    "config['masking'] = select_masking_option()\n",
    "# setting a label to be used in filenames and directory names based on the masking choice\n",
    "config['output_file_prefix'] = 'with_masking' if config['masking'] else 'without_masking'\n",
    "# setting up necessary directories for saving results, figures\n",
    "config = setup_directories(config)\n",
    "# extracting paths from the directory structure for task-specific outputs\n",
    "masking_dir, final_imputations_dir_missing_ratio, feature_eval_dir, task3_time_dir = get_final_imputations_dir(config)\n",
    "# loading the DHS or synthetic dataset according to the user-defined config settings\n",
    "input_df, initial_missingness = load_data(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis only for DHS dataset (not for synthetic data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keeping numerical data with missing values\n",
    "numerical_only = input_df.select_dtypes(include=[np.number])\n",
    "numerical_only.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing prefixes from column names for better understanding\n",
    "numerical_only.columns = numerical_only.columns.str.replace(r'^(DHS Num; |DHS Cat; )', '', regex=True)\n",
    "# extracting the initial part of each column name\n",
    "initial_columns = numerical_only.columns.str.extract(r'(^[^:]+)', expand=False)\n",
    "\n",
    "# a mask to keep only the first occurrence of each initial part\n",
    "unique_columns_mask = ~initial_columns.duplicated()\n",
    "\n",
    "# the mask to keep only unique initial columns\n",
    "unique_df = numerical_only.loc[:, unique_columns_mask]\n",
    "\n",
    "# filtering the data to include only the years between 2006 and 2022\n",
    "filtered_df = unique_df[(unique_df['Meta; year'] >= 2001) & (unique_df['Meta; year'] <= 2022)]\n",
    "\n",
    "# calculating the proportion of missing data per year\n",
    "missing_data_yr = filtered_df.groupby('Meta; year').apply(lambda x: x.isnull().mean())\n",
    "\n",
    "# plotting the heatmap\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.heatmap(missing_data_yr, cbar=True, cmap='viridis', yticklabels=True, linecolor='black', linewidths=0.5)\n",
    "plt.xlabel('Features', fontsize=14, labelpad=10)\n",
    "plt.ylabel('Years', fontsize=14)\n",
    "plt.title('Missing Data Heatmap Per Year (2001-2022)', fontsize=16, pad=15)\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "# saving the plot \n",
    "# get current working directory\n",
    "current_dir = os.getcwd()\n",
    "# defining output path in current directory\n",
    "output_path = os.path.join(current_dir, \"heatmap_per_year_missing_data.png\")\n",
    "# saving and showing the plot\n",
    "plt.savefig(output_path, dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up a larger figure with a custom color palette and aesthetic adjustments\n",
    "plt.figure(figsize=(10, 13))  # adjusting size as needed for clarity\n",
    "\n",
    "# customizing colors and add colorbar with label directly in sns.heatmap\n",
    "sns.heatmap(\n",
    "    unique_df.isnull(),\n",
    "    cbar=False,\n",
    "    cmap=\"cividis\",\n",
    "    yticklabels=False,  # removing row labels (sample numbers)\n",
    "    # cbar_kws={'label': 'Missing Data (Yellow) / Complete Data (Blue)'}  # Add colorbar label here\n",
    ")\n",
    "\n",
    "# enhancing plot title, labels and colorbar for a professional presentation\n",
    "plt.title(\"Pattern of Missing Data in DHS Dataset\", fontsize=16, pad=15)\n",
    "plt.xlabel(\"Features\", fontsize=14, labelpad=10)\n",
    "plt.ylabel(\"Samples\", fontsize=14, labelpad=10)\n",
    "\n",
    "# customized the tick parameters for readability\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "# showing and saving the plot\n",
    "plt.tight_layout(pad=3.0)  # ensures the plot fits well without clipping\n",
    "output_path = os.path.join(current_dir, \"all_rows_final_missing_data_pattern.png\")\n",
    "plt.savefig(output_path, dpi=300)  # high resolution for publication\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the numerical data\n",
    "scaler = StandardScaler()\n",
    "df1_scaled = pd.DataFrame(scaler.fit_transform(numerical_only), columns=numerical_only.columns)\n",
    "\n",
    "# applying KNN Imputer\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "df1_scaled = pd.DataFrame(imputer.fit_transform(df1_scaled), columns=df1_scaled.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresholds for correlation categories\n",
    "high_corr_threshold = 0.7\n",
    "moderate_corr_threshold = 0.3\n",
    "\n",
    "# Pearson and Spearman correlations\n",
    "pearson_corr_matrix = df1_scaled.corr()\n",
    "spearman_corr_matrix = df1_scaled.corr(method='spearman')\n",
    "\n",
    "# flattening the matrices to prepare for analysis \n",
    "pearson_pairs = pearson_corr_matrix.unstack().reset_index()\n",
    "spearman_pairs = spearman_corr_matrix.unstack().reset_index()\n",
    "pearson_pairs.columns = [\"Feature_X\", \"Feature_Y\", \"Pearson_Correlation\"]\n",
    "spearman_pairs.columns = [\"Feature_X\", \"Feature_Y\", \"Spearman_Correlation\"]\n",
    "\n",
    "# merging Pearson and Spearman correlations into one df\n",
    "correlation_df = pd.merge(pearson_pairs, spearman_pairs, on=[\"Feature_X\", \"Feature_Y\"])\n",
    "\n",
    "# exclude self-correlations\n",
    "correlation_df = correlation_df[correlation_df[\"Feature_X\"] != correlation_df[\"Feature_Y\"]]\n",
    "\n",
    "# categorize correlations by strength\n",
    "total_pairs = len(correlation_df)\n",
    "high_corr_count = len(correlation_df[correlation_df[\"Pearson_Correlation\"].abs() > high_corr_threshold])\n",
    "moderate_corr_count = len(correlation_df[(correlation_df[\"Pearson_Correlation\"].abs() <= high_corr_threshold) & \n",
    "                                         (correlation_df[\"Pearson_Correlation\"].abs() > moderate_corr_threshold)])\n",
    "no_corr_count = len(correlation_df[correlation_df[\"Pearson_Correlation\"].abs() <= moderate_corr_threshold])\n",
    "\n",
    "# calculating percentages for linear relationships\n",
    "high_corr_percentage = (high_corr_count / total_pairs) * 100\n",
    "moderate_corr_percentage = (moderate_corr_count / total_pairs) * 100\n",
    "no_corr_percentage = (no_corr_count / total_pairs) * 100\n",
    "\n",
    "# calculating percentages for non-linear relationships using Spearman correlations\n",
    "non_linear_count = len(correlation_df[correlation_df[\"Spearman_Correlation\"].abs() < moderate_corr_threshold])\n",
    "linear_count = total_pairs - non_linear_count\n",
    "non_linear_percentage = (non_linear_count / total_pairs) * 100\n",
    "linear_percentage = (linear_count / total_pairs) * 100\n",
    "\n",
    "# printing the results\n",
    "print(f\"Highly correlated features (Pearson > {high_corr_threshold}): {high_corr_percentage:.2f}%\")\n",
    "print(f\"Moderately correlated features (Pearson {moderate_corr_threshold} - {high_corr_threshold}): {moderate_corr_percentage:.2f}%\")\n",
    "print(f\"No correlation (Pearson <= {moderate_corr_threshold}): {no_corr_percentage:.2f}%\")\n",
    "print(f\"Non-linear relationships (Spearman < {moderate_corr_threshold}): {non_linear_percentage:.2f}%\")\n",
    "print(f\"Linear relationships (Spearman >= {moderate_corr_threshold}): {linear_percentage:.2f}%\")\n",
    "\n",
    "# saving correlation data to CSV in the current directory\n",
    "csv_save_path = os.path.join(current_dir, \"pearson_spearman_correlation_analysis.csv\")\n",
    "correlation_df.to_csv(csv_save_path, index=False)\n",
    "\n",
    "# plotting the heatmap for the top 20 correlated pairs (by Pearson correlation)\n",
    "top_20_corr_pairs = correlation_df.nlargest(20, \"Pearson_Correlation\", keep='all')[[\"Feature_X\", \"Feature_Y\"]]\n",
    "top_features = list(set(top_20_corr_pairs[\"Feature_X\"]).union(set(top_20_corr_pairs[\"Feature_Y\"])))\n",
    "top_corr_matrix = pearson_corr_matrix.loc[top_features, top_features]\n",
    "\n",
    "# masking the upper triangle\n",
    "mask = np.triu(np.ones_like(top_corr_matrix, dtype=bool))\n",
    "\n",
    "# removing gridlines and plot the heatmap\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.set(style=\"white\")\n",
    "sns.heatmap(top_corr_matrix, mask=mask, annot=True, cmap='coolwarm', center=0, annot_kws={\"size\": 8}, \n",
    "            linewidths=.5, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Top Correlated Features', pad=20)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "\n",
    "# saving the heatmap in the current directory\n",
    "save_path = os.path.join(current_dir, \"corr_matrix_original_numerical_only_dataset.png\")\n",
    "plt.savefig(save_path, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify relationship complexity based on Pearson and Spearman correlations\n",
    "def classify_relationship(row):\n",
    "    if abs(row['Pearson_Correlation']) > high_corr_threshold and abs(row['Spearman_Correlation']) > high_corr_threshold:\n",
    "        return 'Linear Relationship'\n",
    "    elif abs(row['Pearson_Correlation']) < moderate_corr_threshold and abs(row['Spearman_Correlation']) < moderate_corr_threshold:\n",
    "        return 'Complex Non-linear Relationship'\n",
    "    elif abs(row['Pearson_Correlation']) < high_corr_threshold and abs(row['Spearman_Correlation']) > moderate_corr_threshold:\n",
    "        return 'Simple Non-linear Relationship'\n",
    "    else:\n",
    "        return 'Uncertain Relationship'\n",
    "\n",
    "# applying the function and add the classification to the DataFrame\n",
    "correlation_df['Relationship_Type'] = correlation_df.apply(classify_relationship, axis=1)\n",
    "\n",
    "# summarizing the classification counts\n",
    "classification_counts = correlation_df['Relationship_Type'].value_counts(normalize=True) * 100\n",
    "\n",
    "# the classification breakdown\n",
    "print(\"Relationship complexity breakdown:\")\n",
    "for relationship_type, percentage in classification_counts.items():\n",
    "    print(f\"{relationship_type}: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensuring we work with current directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# the correlation matrix\n",
    "corr_matrix = df1_scaled.corr()\n",
    "\n",
    "# the absolute values of the correlation matrix\n",
    "abs_corr_matrix = corr_matrix.abs()\n",
    "\n",
    "# a heatmap for the correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', center=0, annot_kws={\"size\": 8}, linewidths=.5)\n",
    "plt.title('Correlation Matrix of Scaled Features')\n",
    "corr_save_path = os.path.join(current_dir, 'corr_matrix_numerical_features_scaled_dataset.png')\n",
    "plt.savefig(corr_save_path)\n",
    "plt.show()\n",
    "\n",
    "# the percentage of acceptable correlation for each feature\n",
    "acceptable_threshold = 0.3\n",
    "acceptable_corr_counts = (abs_corr_matrix >= acceptable_threshold).sum() - 1  # Exclude self-correlation\n",
    "acceptable_corr_percentage = (acceptable_corr_counts / (df1_scaled.shape[1] - 1)) * 100\n",
    "\n",
    "# saving the correlation matrix and percentage list\n",
    "corr_matrix_save_path = os.path.join(current_dir, \"final_scaled_correlation_matrix.csv\")\n",
    "corr_matrix.to_csv(corr_matrix_save_path)\n",
    "\n",
    "acceptable_corr_percentage_list_path = os.path.join(current_dir, \"acceptable_corr_percentage_per_feature.csv\")\n",
    "acceptable_corr_percentage.to_csv(acceptable_corr_percentage_list_path, header=[\"Acceptable_Correlation_Percentage\"])\n",
    "\n",
    "# show top 10 for preview\n",
    "corr_matrix_save_path, acceptable_corr_percentage_list_path, acceptable_corr_percentage.tolist()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the Pearson correlation matrix\n",
    "corr_matrix = df1_scaled.corr()\n",
    "\n",
    "# the absolute values of the correlation matrix\n",
    "abs_corr_matrix = corr_matrix.abs()\n",
    "\n",
    "# the percentage of acceptable correlation for each feature\n",
    "acceptable_threshold = 0.3\n",
    "acceptable_corr_counts = (abs_corr_matrix >= acceptable_threshold).sum() - 1  # exclude self-correlation\n",
    "acceptable_corr_percentage = (acceptable_corr_counts / (df1_scaled.shape[1] - 1)) * 100\n",
    "\n",
    "# identifying non-linear relationships using Spearman correlation as a proxy (a threshold of 0.3 for \"acceptable\" nonlinearity)\n",
    "spearman_corr_matrix = df1_scaled.corr(method='spearman')\n",
    "non_linear_corr_counts = (spearman_corr_matrix.abs() < acceptable_threshold).sum() - 1  # Exclude self-correlation\n",
    "non_linear_corr_percentage = (non_linear_corr_counts / (df1_scaled.shape[1] - 1)) * 100\n",
    "\n",
    "# df to save the information\n",
    "correlation_analysis_df = pd.DataFrame({\n",
    "    \"Feature\": df1_scaled.columns,\n",
    "    \"Acceptable_Correlation_Percentage\": acceptable_corr_percentage,\n",
    "    \"Non_Linear_Correlation_Percentage\": non_linear_corr_percentage\n",
    "})\n",
    "\n",
    "# determining linear or non-linear based on non-linear correlation percentage\n",
    "correlation_analysis_df[\"Relation_Type\"] = np.where(correlation_analysis_df[\"Non_Linear_Correlation_Percentage\"] > 50, \"Non-Linear\", \"Linear\")\n",
    "\n",
    "# saving as CSV\n",
    "current_dir = os.getcwd()\n",
    "correlation_analysis_path = os.path.join(current_dir, \"non_linearity_correlation_analysis.csv\")\n",
    "correlation_analysis_df.to_csv(correlation_analysis_path, index=False)\n",
    "correlation_analysis_path, correlation_analysis_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seaborn style and use a built-in palette\n",
    "sns.set(style=\"whitegrid\", font_scale=1.2)\n",
    "palette = sns.color_palette(\"Blues\", n_colors=6)\n",
    "selected_color = palette[4]  # Fifth color from the \"Blues\" palette\n",
    "\n",
    "# list to store Spearman correlation results for all column pairs\n",
    "correlation_results = []\n",
    "\n",
    "# spearman correlation for all pairs, excluding self-correlations (correlation = 1)\n",
    "for col1, col2 in itertools.combinations(df1_scaled.columns, 2):\n",
    "    spearman_corr, _ = spearmanr(df1_scaled[col1], df1_scaled[col2])\n",
    "    if abs(spearman_corr) < 0.85:  # Exclude perfect correlations\n",
    "        correlation_results.append([col1, col2, spearman_corr])\n",
    "\n",
    "# df from correlation results\n",
    "correlation_df = pd.DataFrame(correlation_results, columns=[\"Feature_X\", \"Feature_Y\", \"Spearman_Correlation\"])\n",
    "\n",
    "# converting 'Spearman_Correlation' column to numeric\n",
    "correlation_df[\"Spearman_Correlation\"] = pd.to_numeric(correlation_df[\"Spearman_Correlation\"])\n",
    "\n",
    "# the top 3 pairs with the highest absolute Spearman correlation values\n",
    "top_three_corr = correlation_df.nlargest(3, 'Spearman_Correlation', keep='all')\n",
    "\n",
    "# a figure with 1 row and 3 columns for the plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# iterating over the top pairs and plot each in a separate subplot\n",
    "for i, row in enumerate(top_three_corr.itertuples()):\n",
    "    col1, col2, spearman_corr = row.Feature_X, row.Feature_Y, row.Spearman_Correlation\n",
    "\n",
    "    # plotting non-linear relationship visualization with selected color and coral for regression line\n",
    "    sns.scatterplot(\n",
    "        data=df1_scaled, x=col1, y=col2,\n",
    "        color=selected_color, alpha=0.7, s=60, ax=axes[i]\n",
    "    )\n",
    "\n",
    "    # adding regression line with confidence interval\n",
    "    sns.regplot(\n",
    "        data=df1_scaled, x=col1, y=col2,\n",
    "        scatter=False, ax=axes[i],\n",
    "        color=\"coral\", ci=95, line_kws={\"linewidth\": 1.5, \"linestyle\": \"--\"}\n",
    "    )\n",
    "\n",
    "    # customized the title and labels\n",
    "    axes[i].set_title(f\"\\nSpearman Correlation: {spearman_corr:.2f}\", fontsize=12, weight=\"bold\")\n",
    "    axes[i].set_xlabel(col1, fontsize=10)\n",
    "    axes[i].set_ylabel(col2, fontsize=10)\n",
    "\n",
    "# current working directory for saving files\n",
    "plot_path = os.path.join(current_dir, \"nonlinear_scatter_plots.png\")\n",
    "csv_path = os.path.join(current_dir, \"top_three_spearman_correlation_results.csv\")\n",
    "\n",
    "# adjusting layout for clear display and saving the plot\n",
    "plt.tight_layout()\n",
    "plt.savefig(plot_path)\n",
    "plt.show()\n",
    "\n",
    "# saving correlation results to CSV\n",
    "correlation_df.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_results = []\n",
    "\n",
    "# calculating Spearman correlation for all pairs, excluding self-correlations (correlation = 1)\n",
    "for col1, col2 in itertools.combinations(df1_scaled.columns, 2):\n",
    "    spearman_corr, _ = spearmanr(df1_scaled[col1], df1_scaled[col2])\n",
    "    correlation_results.append([col1, col2, spearman_corr])\n",
    "\n",
    "# creating df from correlation results\n",
    "correlation_df = pd.DataFrame(correlation_results, columns=[\"Feature_X\", \"Feature_Y\", \"Spearman_Correlation\"])\n",
    "\n",
    "# determining linear and non-linear correlations based on threshold\n",
    "linear_threshold = 0.3\n",
    "non_linear_count = correlation_df[correlation_df[\"Spearman_Correlation\"].abs() < linear_threshold].shape[0]\n",
    "linear_count = correlation_df[correlation_df[\"Spearman_Correlation\"].abs() >= linear_threshold].shape[0]\n",
    "total_count = non_linear_count + linear_count\n",
    "\n",
    "# calculating and print percentages\n",
    "non_linear_percentage = (non_linear_count / total_count) * 100\n",
    "linear_percentage = (linear_count / total_count) * 100\n",
    "print(f\"Non-linear percentage: {non_linear_percentage:.2f}%\")\n",
    "print(f\"Linear percentage: {linear_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the top three Spearman correlation results to csv\n",
    "csv_save_path = os.path.join(current_dir, \"top_three_spearman_correlation_results.csv\")\n",
    "correlation_df.to_csv(csv_save_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task1 and Task3: Final Evaluation for RMSE vs Missing Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of imputers\n",
    "imputers = ['Mean', 'KNN', 'MICE_Ridge', 'AE', 'DAE', 'VAE', 'GAIN']\n",
    "\n",
    "# the missing ratios\n",
    "missing_ratios = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "\n",
    "# important columns \n",
    "important_columns = [\n",
    "    'DHS Cat; source of drinking water: piped into dwelling',\n",
    "    'DHS Num; cluster altitude in meters: mean',\n",
    "    'DHS Num; number of mosquito bed nets: mean',\n",
    "    'DHS Num; time to get to water source (minutes): mean',\n",
    "    'DHS Cat; location of source for water: in own dwelling',\n",
    "    'DHS Cat; type of toilet facility: flush to piped sewer system',\n",
    "    'DHS Num; number of household members: mean',\n",
    "    'DHS Cat; has mobile telephone: yes',\n",
    "    'DHS Num; number of mosquito bed nets: mean',\n",
    "    'DHS Cat; has television: yes',\n",
    "    'DHS Cat; type of cooking fuel: lpg',\n",
    "    'DHS Num; hectares of agricultural land (1 decimal): mean',\n",
    "    'DHS Num; owns sheep: mean',\n",
    "    'DHS Num; total adults measured: mean'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# function to handle both with masking (mar) and without masking (mcar)\n",
    "def handle_masking_and_evaluation(input_df, imputers, missing_ratios, config, masking):\n",
    "\n",
    "    # loading or computing data and saving pickles\n",
    "    load_or_compute_data_part(input_df, imputers, missing_ratios, config, config['masking'])\n",
    "    \n",
    "    # evaluating Metrics using Saved Pickles\n",
    "    final_results, extracted_values, time_metrics = evaluate_metrics_part(input_df, imputers, missing_ratios, config)\n",
    "\n",
    "    # plotting RMSE statistics\n",
    "    missing_ratio_vs_stats(final_results, imputers, missing_ratios, config)\n",
    "\n",
    "    # saving time metrics to excel\n",
    "    save_time_metrics_to_excel(time_metrics, config)\n",
    "\n",
    "    # plotting time metrics (for visualizing training and test time per imputation method)\n",
    "    plot_time_vs_missing_ratio(time_metrics, config)\n",
    "\n",
    "    # scatter plots for the entire df\n",
    "    combined_df_scatter_plots(extracted_values, config, missing_ratio=0.3)\n",
    "\n",
    "    # per column scatter plots for the entire df\n",
    "    per_column_scatter_plots(extracted_values, important_columns, imputers, config, missing_ratio=0.3)\n",
    "    \n",
    "handle_masking_and_evaluation(input_df, imputers, missing_ratios, config, config['masking'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task2: Final Evaluation for RMSE vs Num of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the intervals for the number of features\n",
    "feature_intervals = [15, 30, 45, 60, 75, 96]\n",
    "\n",
    "# loading or computing data and saving pickles\n",
    "load_or_compute_data_feature_evaluation(input_df, imputers, feature_intervals, config, config['masking'])\n",
    "\n",
    "# evaluating metrics using saved pickles\n",
    "final_results_features = evaluate_metrics_part_feature_evaluation(input_df, imputers, feature_intervals, config)\n",
    "\n",
    "# running final pipeline for feature evaluaton and saving results\n",
    "num_of_features_vs_stats(final_results_features, imputers, feature_intervals, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

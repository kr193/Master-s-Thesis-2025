{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qp1-ETbItTxO"
   },
   "source": [
    "<h1><center><u> Code Implementation </u></center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core Thesis Work: Missing Data Imputation on Numerical DHS Dataset\n",
    "---\n",
    "---\n",
    "### Code Implementation Walkthrough\n",
    "\n",
    "This notebook walks through the full implementation used in the main experimental part of the thesis, focusing on missing data imputation for **purely numerical DHS and synthetic datasets**.\n",
    "\n",
    "> • **Goal**: Demonstrate missing data imputation with *purely numerical* DHS and synthetic datasets using a modular, scalable pipeline.\n",
    ">\n",
    "> • **Code Folder**: All core logic for data loading, imputation and evaluation is organized in the `src/` directory for clarity and reusability.\n",
    ">\n",
    "> • **Notebook Purpose**: Designed for running and visualizing the full experimental pipeline. This notebook complements the *Main Experimental Study* section of the thesis by supporting exploratory data analysis (EDA) and final metric evaluation by running the full experimental pipeline.\n",
    ">\n",
    "> • **Explore**: See `src/preprocessing.py`, `src/imputations.py`, `src/evaluation.py`, and `src/utils.py` for detailed implementation of the core components.\n",
    "\n",
    "---\n",
    "\n",
    "## Methodology Overview\n",
    "\n",
    "This project uses a range of classical, machine learning and deep learning methods to impute missing data in real-world and synthetic numerical datasets. Steps are structured to ensure consistency, fairness and clarity in evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## Flowchart Overview\n",
    "\n",
    "Below is a visual representation of the full pipeline used for the thesis:\n",
    "\n",
    "<img src=\"Flowchart_thesis_25_april.jpg\" alt=\"Methodology Flowchart\" width=\"900\"/>\n",
    "\n",
    "---\n",
    "\n",
    "## Key Steps in the Methodology\n",
    "\n",
    "#### 1. Data Preprocessing\n",
    "- **Source:** Data collected from the DHS Program Website.\n",
    "- **Steps Involved:**\n",
    "  1. Initial extensive data preprocessing to clean and organize the data.\n",
    "  2. Filtering data to remove irrelevant or redundant entries.\n",
    "  3. Exploratory Data Analysis (EDA) to understand missingness patterns and distributions are analyzed during EDA.\n",
    "\n",
    "#### 2. Dataset Preparation\n",
    "\n",
    "- **Handling Missing Values:**\n",
    "  - Three strategies:\n",
    "    1. Dropping all NaNs for a more restrictive dataset (Complete Dataset or **'drop_all_nans'** asked in prompt).\n",
    "    2. Dropping entire survey groups (column named 'Meta; GEID_init' having several survey IDs) that have more than 20% missing data or (**'numerical_only_drop_20_percentage_nans'** asked in prompt).\n",
    "    3. Using initial KNN Imputation for initial filling of NaNs in the entire numerical dataset (**'numerical_only'** asked in prompt).\n",
    "\n",
    "- **Cross-Validation:**\n",
    "  - K-Fold cross-validation splits Data into training, validation and test folds.\n",
    "  - Standard scaling is applied to ensure consistent feature ranges.\n",
    "\n",
    "#### 3. Model Training and Validation\n",
    "\n",
    "- **Two types of Missingness Introduced:**\n",
    "  - MAR (Missing At Random) and MCAR (Missing Completely At Random).\n",
    "- **Pre-Imputation:**\n",
    "  - Initial KNN imputation is used to preprocess data for autoencoders (only for AE, DAE, and VAE) as these can not be trained on a dataset with NaNs.\n",
    "- **Training Methods:**\n",
    "  - Statistical, Machine learning and Deep learning methods used:\n",
    "    - Mean, KNN, MICE Ridge\n",
    "    - Autoencoders (AE)\n",
    "    - Denoising Autoencoders (DAE)\n",
    "    - Variational Autoencoders (VAE)\n",
    "    - GAIN\n",
    "  - Methods are trained using either:\n",
    "    1. Datasets with artificial missing values (Mean, KNN, MICE Ridge, GAIN).\n",
    "    2. Pre-KNN Imputed datasets after artificial missingness creation (for Autoencoders only).\n",
    "\n",
    "#### 4. Hyperparameter Tuning and Testing\n",
    "\n",
    "- Methods are tuned using the training folds and validated on separate test folds to ensure unbiased final evaluation.\n",
    "- Final evaluation metrics are recorded under **both MAR and MCAR** scenarios.\n",
    "\n",
    "#### 5. Final Evaluation\n",
    "\n",
    "- **Performance Metrics:** \n",
    "\n",
    "  - RMSE (Root Mean Square Error)\n",
    "  - MAE (Mean Absolute Error)\n",
    "  - R² (R-squared)\n",
    "  - Pearson Correlation\n",
    "  - Results are compared across methods to determine the most effective approach.\n",
    "\n",
    "The methodology ensures robust data handling and model evaluation for missing data imputation. The flowchart visually represents each step, highlighting the workflow and key decisions made in the study.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "This end-to-end pipeline ensures reliable and consistent evaluation of imputation techniques on numerical DHS data. With reusable components and clear structure, the notebook supports both experimentation and reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and Setup\n",
    "\n",
    "Importing necessary libraries for data manipulation, statistical methods, machine learning, deep learning and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required libraries\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import joblib\n",
    "import pickle\n",
    "import random\n",
    "import string\n",
    "import itertools\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from icecream import ic\n",
    "import tensorflow as tf\n",
    "from scipy import stats\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from keras.regularizers import l2\n",
    "from scipy.stats import spearmanr\n",
    "from tensorflow.keras import Model\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import Ridge\n",
    "from tensorflow.keras.losses import mse\n",
    "from scipy.stats import levene, f_oneway\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import L2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from tensorflow.keras.initializers import he_uniform\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.losses import mse as keras_mse\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from tensorflow.keras.layers import Lambda, Layer, Input, Dense\n",
    "from sklearn.linear_model import BayesianRidge, LinearRegression\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense, Input, GaussianNoise, Layer\n",
    "from tensorflow.keras import Sequential, layers, models, optimizers, regularizers\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer, PolynomialFeatures, FunctionTransformer\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "import src.config_loader\n",
    "importlib.reload(src.config_loader)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.utils import *\n",
    "from src.setup_dirs import *\n",
    "from src.evaluation import *\n",
    "from src.run_pipeline import *\n",
    "from src.visualization import *\n",
    "from src.preprocessing import *\n",
    "from src.config_loader import *\n",
    "from src.gain import GAIN_code_v2\n",
    "from src.helper_functions import *\n",
    "from src.single_imputation import *\n",
    "from src.load_data import load_data\n",
    "from src.initial_imputation import *\n",
    "from src.deep_learning_methods import *\n",
    "from src.synthetic_data_generation import *\n",
    "from src.config_loader import load_config, find_project_root\n",
    "from src.dhs_modelling_functions_new import final_ds_droping_cols, fold_generator\n",
    "\n",
    "# initialization\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setting up random seeds for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PGRLvmmdDrSH"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(6688)\n",
    "random.seed(6688)\n",
    "np.random.seed(6688)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Configuration\n",
    "\n",
    "Configuring TensorFlow to use GPUs and manage memory efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4D_reWvTDuEt"
   },
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu,True)\n",
    "    except RuntimeError as e:\n",
    "        raise e\n",
    "\n",
    "#use multiple GPUs\n",
    "gpus = [0] # may request more, if necessary\n",
    "gpus = [\"GPU:\" + str(i) for i in gpus]\n",
    "# https://keras.io/guides/distributed_training/\n",
    "print('gpus', gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "references from https://github.com/gheisenberg/FoodSecurity/tree/main/DHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Configuration with directory creation\n",
    "\n",
    "Setting up configurations for real and synthetic data handling, including masking and directory setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### loading base configuration settings (real/synthetic, directory paths etc.)\n",
    "# for either real and (aggregated + numerical) DHS data or synthetic data\n",
    "config = load_config()\n",
    "# prompt user to select how missing values (NaNs) should be handled in the dataset\n",
    "config = select_process_nans_option(config)\n",
    "# prompt user to choose type of missingness simulation: MAR (with masking) or MCAR (without masking)\n",
    "config['masking'] = select_masking_option()\n",
    "# setting a label to be used in filenames and directory names based on the masking choice\n",
    "config['output_file_prefix'] = 'with_masking' if config['masking'] else 'without_masking'\n",
    "# setting up necessary directories for saving results, figures\n",
    "config = setup_directories(config)\n",
    "# extracting paths from the directory structure for task-specific outputs\n",
    "masking_dir, final_imputations_dir_missing_ratio, feature_eval_dir, task3_time_dir = get_final_imputations_dir(config)\n",
    "# loading the DHS or synthetic dataset according to the user-defined config settings\n",
    "input_df, initial_missingness= load_data(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) only for numerical DHS datasets\n",
    "### set \"use_synthetic_data\": false - from config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keeping numerical data with missing values\n",
    "numerical_only = input_df.select_dtypes(include=[np.number])\n",
    "numerical_only.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_only.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing prefixes from column names for better understanding\n",
    "numerical_only.columns = numerical_only.columns.str.replace(r'^(DHS Num; |DHS Cat; )', '', regex=True)\n",
    "# extracting the initial part of each column name\n",
    "initial_columns = numerical_only.columns.str.extract(r'(^[^:]+)', expand=False)\n",
    "\n",
    "# a mask to keep only the first occurrence of each initial part\n",
    "unique_columns_mask = ~initial_columns.duplicated()\n",
    "\n",
    "# the mask to keep only unique initial columns\n",
    "unique_df = numerical_only.loc[:, unique_columns_mask]\n",
    "\n",
    "# filtering the data to include only the years between 2006 and 2022\n",
    "filtered_df = unique_df[(unique_df['Meta; year'] >= 2001) & (unique_df['Meta; year'] <= 2022)]\n",
    "\n",
    "# calculating the proportion of missing data per year\n",
    "missing_data_yr = filtered_df.groupby('Meta; year').apply(lambda x: x.isnull().mean())\n",
    "\n",
    "# plotting the heatmap\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.heatmap(missing_data_yr, cbar=True, cmap='viridis', yticklabels=True, linecolor='black', linewidths=0.5)\n",
    "plt.xlabel('Features', fontsize=14, labelpad=10)\n",
    "plt.ylabel('Years', fontsize=14)\n",
    "plt.title('Missing Data Heatmap Per Year (2001-2022)', fontsize=16, pad=15)\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "# saving the plot \n",
    "# get current working directory\n",
    "current_dir = os.getcwd()\n",
    "# defining output path in current directory\n",
    "output_path = os.path.join(current_dir, \"heatmap_per_year_missing_data.png\")\n",
    "# saving and showing the plot\n",
    "plt.savefig(output_path, dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern of Missing Data in DHS Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up a larger figure with a custom color palette and aesthetic adjustments\n",
    "plt.figure(figsize=(10, 13))  # adjusting size as needed for clarity\n",
    "\n",
    "# customizing colors and add colorbar with label directly in sns.heatmap\n",
    "sns.heatmap(\n",
    "    unique_df.isnull(),\n",
    "    cbar=False,\n",
    "    cmap=\"cividis\",\n",
    "    yticklabels=False,  # removing row labels (sample numbers)\n",
    "    # cbar_kws={'label': 'Missing Data (Yellow) / Complete Data (Blue)'}  # Add colorbar label here\n",
    ")\n",
    "\n",
    "# enhancing plot title, labels and colorbar for a professional presentation\n",
    "plt.title(\"Pattern of Missing Data in DHS Dataset\", fontsize=16, pad=15)\n",
    "plt.xlabel(\"Features\", fontsize=14, labelpad=10)\n",
    "plt.ylabel(\"Samples\", fontsize=14, labelpad=10)\n",
    "\n",
    "# customized the tick parameters for readability\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "# showing and saving the plot\n",
    "plt.tight_layout(pad=3.0)  # ensures the plot fits well without clipping\n",
    "output_path = os.path.join(current_dir, \"all_rows_final_missing_data_pattern.png\")\n",
    "plt.savefig(output_path, dpi=300)  # high resolution for publication\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial KNN imputation to fill NaNs to retain most of the dataset size and diversity instead of dropping all NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying KNN Imputer\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "df1_imputed = pd.DataFrame(imputer.fit_transform(numerical_only), columns=numerical_only.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Matrix for cleaned numerical_only dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresholds for correlation categories\n",
    "high_corr_threshold = 0.7\n",
    "moderate_corr_threshold = 0.3\n",
    "\n",
    "# Pearson and Spearman correlations\n",
    "pearson_corr_matrix = df1_imputed.corr()\n",
    "spearman_corr_matrix = df1_imputed.corr(method='spearman')\n",
    "\n",
    "# flattening the matrices to prepare for analysis \n",
    "pearson_pairs = pearson_corr_matrix.unstack().reset_index()\n",
    "spearman_pairs = spearman_corr_matrix.unstack().reset_index()\n",
    "pearson_pairs.columns = [\"Feature_X\", \"Feature_Y\", \"Pearson_Correlation\"]\n",
    "spearman_pairs.columns = [\"Feature_X\", \"Feature_Y\", \"Spearman_Correlation\"]\n",
    "\n",
    "# merging Pearson and Spearman correlations into one df\n",
    "correlation_df = pd.merge(pearson_pairs, spearman_pairs, on=[\"Feature_X\", \"Feature_Y\"])\n",
    "\n",
    "# exclude self-correlations\n",
    "correlation_df = correlation_df[correlation_df[\"Feature_X\"] != correlation_df[\"Feature_Y\"]]\n",
    "\n",
    "# categorize correlations by strength\n",
    "total_pairs = len(correlation_df)\n",
    "high_corr_count = len(correlation_df[correlation_df[\"Pearson_Correlation\"].abs() > high_corr_threshold])\n",
    "moderate_corr_count = len(correlation_df[(correlation_df[\"Pearson_Correlation\"].abs() <= high_corr_threshold) & \n",
    "                                         (correlation_df[\"Pearson_Correlation\"].abs() > moderate_corr_threshold)])\n",
    "no_corr_count = len(correlation_df[correlation_df[\"Pearson_Correlation\"].abs() <= moderate_corr_threshold])\n",
    "\n",
    "# calculating percentages for linear relationships\n",
    "high_corr_percentage = (high_corr_count / total_pairs) * 100\n",
    "moderate_corr_percentage = (moderate_corr_count / total_pairs) * 100\n",
    "no_corr_percentage = (no_corr_count / total_pairs) * 100\n",
    "\n",
    "# calculating percentages for non-linear relationships using Spearman correlations\n",
    "non_linear_count = len(correlation_df[correlation_df[\"Spearman_Correlation\"].abs() < moderate_corr_threshold])\n",
    "linear_count = total_pairs - non_linear_count\n",
    "non_linear_percentage = (non_linear_count / total_pairs) * 100\n",
    "linear_percentage = (linear_count / total_pairs) * 100\n",
    "\n",
    "# printing the results\n",
    "print(f\"Highly correlated features (Pearson > {high_corr_threshold}): {high_corr_percentage:.2f}%\")\n",
    "print(f\"Moderately correlated features (Pearson {moderate_corr_threshold} - {high_corr_threshold}): {moderate_corr_percentage:.2f}%\")\n",
    "print(f\"No correlation (Pearson <= {moderate_corr_threshold}): {no_corr_percentage:.2f}%\")\n",
    "print(f\"Non-linear relationships (Spearman < {moderate_corr_threshold}): {non_linear_percentage:.2f}%\")\n",
    "print(f\"Linear relationships (Spearman >= {moderate_corr_threshold}): {linear_percentage:.2f}%\")\n",
    "\n",
    "# saving correlation data to CSV in the current directory\n",
    "csv_save_path = os.path.join(current_dir, \"pearson_spearman_correlation_analysis.csv\")\n",
    "correlation_df.to_csv(csv_save_path, index=False)\n",
    "\n",
    "# plotting the heatmap for the top 20 correlated pairs (by Pearson correlation)\n",
    "top_20_corr_pairs = correlation_df.nlargest(20, \"Pearson_Correlation\", keep='all')[[\"Feature_X\", \"Feature_Y\"]]\n",
    "top_features = list(set(top_20_corr_pairs[\"Feature_X\"]).union(set(top_20_corr_pairs[\"Feature_Y\"])))\n",
    "top_corr_matrix = pearson_corr_matrix.loc[top_features, top_features]\n",
    "\n",
    "# masking the upper triangle\n",
    "mask = np.triu(np.ones_like(top_corr_matrix, dtype=bool))\n",
    "\n",
    "# removing gridlines and plot the heatmap\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.set(style=\"white\")\n",
    "sns.heatmap(top_corr_matrix, mask=mask, annot=True, cmap='coolwarm', center=0, annot_kws={\"size\": 8}, \n",
    "            linewidths=.5, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Top Correlated Features', pad=20)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "\n",
    "# saving the heatmap in the current directory\n",
    "save_path = os.path.join(current_dir, \"corr_matrix_original_numerical_only_dataset.png\")\n",
    "plt.savefig(save_path, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task1 and Task3: Final Evaluation for RMSE vs Missing Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of imputers\n",
    "imputers = ['Mean', 'KNN', 'MICE_Ridge', 'AE', 'DAE', 'VAE', 'GAIN']\n",
    "\n",
    "# the missing ratios\n",
    "missing_ratios = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "\n",
    "# important columns \n",
    "important_columns = [\n",
    "    'DHS Cat; source of drinking water: piped into dwelling',\n",
    "    'DHS Num; cluster altitude in meters: mean',\n",
    "    'DHS Num; number of mosquito bed nets: mean',\n",
    "    'DHS Num; time to get to water source (minutes): mean',\n",
    "    'DHS Cat; location of source for water: in own dwelling',\n",
    "    'DHS Cat; type of toilet facility: flush to piped sewer system',\n",
    "    'DHS Num; number of household members: mean',\n",
    "    'DHS Cat; has mobile telephone: yes',\n",
    "    'DHS Num; number of mosquito bed nets: mean',\n",
    "    'DHS Cat; has television: yes',\n",
    "    'DHS Cat; type of cooking fuel: lpg',\n",
    "    'DHS Num; hectares of agricultural land (1 decimal): mean',\n",
    "    'DHS Num; owns sheep: mean',\n",
    "    'DHS Num; total adults measured: mean'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# function to handle both with masking (mar) and without masking (mcar)\n",
    "def handle_masking_and_evaluation(input_df, imputers, missing_ratios, config, masking):\n",
    "\n",
    "    # loading or computing data and saving pickles\n",
    "    load_or_compute_data_part(input_df, imputers, missing_ratios, config, config['masking'])\n",
    "    \n",
    "    # evaluating Metrics using Saved Pickles\n",
    "    final_results, extracted_values, time_metrics = evaluate_metrics_part(input_df, imputers, missing_ratios, config)\n",
    "\n",
    "    # plotting RMSE statistics\n",
    "    missing_ratio_vs_stats(final_results, imputers, missing_ratios, config)\n",
    "\n",
    "    plot_std_boxplot_for_ratio_30(final_results, imputers, config, missing_ratio=0.3)\n",
    "    \n",
    "    # saving time metrics to excel\n",
    "    save_time_metrics_to_excel(time_metrics, config)\n",
    "\n",
    "    # plotting time metrics (for visualizing training and test time per imputation method)\n",
    "    plot_time_vs_missing_ratio(time_metrics, config)\n",
    "\n",
    "    # scatter plots for the entire df\n",
    "    combined_df_scatter_plots(extracted_values, config, missing_ratio=0.3)\n",
    "\n",
    "    # per column scatter plots for the entire df\n",
    "    per_column_scatter_plots(extracted_values, important_columns, imputers, config, missing_ratio=0.3)\n",
    "    \n",
    "handle_masking_and_evaluation(input_df, imputers, missing_ratios, config, config['masking'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task2: Final Evaluation for RMSE vs Num of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the intervals for the number of features\n",
    "feature_intervals = [15, 30, 45, 60, 75, 96]\n",
    "\n",
    "# loading or computing data and saving pickles\n",
    "load_or_compute_data_feature_evaluation(input_df, imputers, feature_intervals, config, config['masking'])\n",
    "\n",
    "# evaluating metrics using saved pickles\n",
    "final_results_features = evaluate_metrics_part_feature_evaluation(input_df, imputers, feature_intervals, config)\n",
    "\n",
    "# running final pipeline for feature evaluaton and saving results\n",
    "num_of_features_vs_stats(final_results_features, imputers, feature_intervals, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

# **Missing Data Imputation for DHS Indicators**
_Exploring Deep Learning and Traditional Approaches for Data Recovery_

Welcome! This repository contains my Master's research project on **missing data imputation** techniques specifically for **Demographic and Health Surveys (DHS)** datasets. The research compares traditional statistical methods and cutting-edge deep learning models, utilizing both real-world and synthetic data to assess performance.

---

## **Project Overview**

This project:

- âœ… Evaluates various **imputation methods** including **Mean, KNN, MICE**, and deep learning models (**Autoencoder (AE), Denoising AE (DAE), Variational AE (VAE), and GAIN**).
- âœ… Compares their effectiveness using metrics: **RMSE**, **NRMSE**, **RÂ²**, **MAE**, and **Correlation**.
- âœ… Explores missingness types: **MAR (Missing At Random)** and **MCAR (Missing Completely At Random)**.
- âœ… Generates detailed **visualizations** and structured **Excel summaries**.

---

## **Repository Structure**
When cloning this repository, create a dedicated local directory (e.g., final_imputation_project/) to keep things organized. The structure inside your project will look like this:

```bash
msc_thesis_2025/
final_imputation_project/
â”œâ”€â”€ config.json                     # Core configuration file for experiments
â”œâ”€â”€ github_readme.txt                # Draft readme (can be deleted/updated)
â”œâ”€â”€ main.py                          # Main script for running the pipeline
â”œâ”€â”€ src/                             # Source code folder
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config_loader.py
â”‚   â”œâ”€â”€ cross_val_models_statistics.py
â”‚   â”œâ”€â”€ deep_learning_methods.py
â”‚   â”œâ”€â”€ dhs_modelling_functions_new.py
â”‚   â”œâ”€â”€ evaluation.py
â”‚   â”œâ”€â”€ features_evaluation.py
â”‚   â”œâ”€â”€ gain.py
â”‚   â”œâ”€â”€ helper_functions.py
â”‚   â”œâ”€â”€ initial_imputation.py
â”‚   â”œâ”€â”€ load_data.py
â”‚   â”œâ”€â”€ preprocessing.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ rmse_cv_num_of_features.py
â”‚   â”œâ”€â”€ run_pipeline.py
â”‚   â”œâ”€â”€ setup_dirs.py
â”‚   â”œâ”€â”€ single_imputation.py
â”‚   â”œâ”€â”€ synthetic_data_generation.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â””â”€â”€ visualization.py
â”œâ”€â”€ data/                            # Contains raw or processed datasets
â”œâ”€â”€ notebooks/                       # Jupyter notebooks for exploration/demos
â”œâ”€â”€ output_images/                   # Contains generated plots and Excel reports
â”œâ”€â”€ results/                         # Auto-generated results (pickles)
â”‚   â”œâ”€â”€ drop_20_percentage/          # For 20% NaN-drop scenario
â”‚   â”‚   â”œâ”€â”€ with_masking/            # MAR missingness scenario
â”‚   â”‚   â”‚   â””â”€â”€ final_methods_pkls/
â”‚   â”‚   â”‚       â”œâ”€â”€ task1_final_imputations_missing_ratio/   # Pickle files for Task 1
â”‚   â”‚   â”‚       â”œâ”€â”€ task2_rmse_vs_num_features/              # Pickle files for Task 2
â”‚   â”‚   â”‚       â””â”€â”€ task3_time_vs_missing_ratio/             # Timing metrics (pickles)
â”‚   â”‚   â””â”€â”€ without_masking/         # MCAR missingness scenario
â”‚   â”‚       â””â”€â”€ final_methods_pkls/
â”‚   â”‚           â”œâ”€â”€ task1_final_imputations_missing_ratio/
â”‚   â”‚           â”œâ”€â”€ task2_rmse_vs_num_features/
â”‚   â”‚           â””â”€â”€ task3_time_vs_missing_ratio/
â”‚   â”œâ”€â”€ drop_all_nans/
â”‚   â”‚   â”œâ”€â”€ with_masking/
â”‚   â”‚   â””â”€â”€ without_masking/
â”‚   â”‚       â””â”€â”€ final_methods_pkls/
â”‚   â”‚           â””â”€â”€ (Same as above)
â”‚   â””â”€â”€ keep_all_numerical/
â”‚       â”œâ”€â”€ with_masking/
â”‚       â””â”€â”€ without_masking/
â”‚           â””â”€â”€ final_methods_pkls/
â”‚               â””â”€â”€ (Same as above)
â””â”€â”€ synthetic_data/                  # Synthetic data results
    â”œâ”€â”€ synthetic_high_correlation/  # High correlation scenario
    â”‚   â”œâ”€â”€ with_masking/
    â”‚   â”‚   â””â”€â”€ final_methods_pkls/
    â”‚   â”‚       â”œâ”€â”€ task1_final_imputations_missing_ratio/
    â”‚   â”‚       â”œâ”€â”€ task2_rmse_vs_num_features/
    â”‚   â”‚       â””â”€â”€ task3_time_vs_missing_ratio/
    â”‚   â””â”€â”€ without_masking/
    â”‚       â””â”€â”€ final_methods_pkls/
    â”‚           â””â”€â”€ (Same as above)
    â”œâ”€â”€ synthetic_medium_correlation/
    â”‚   â””â”€â”€ (Same structure as above)
    â””â”€â”€ synthetic_no_correlation/
        â””â”€â”€ (Same structure as above)
```

### Tips:
The project root for any user can be cloned into any local directory ( ~/projects/, ~/research/, etc.).

---

## âš™ï¸ **Configuration (`config.json`)**

Below is an example configuration (`config.json`) you can use to get started:

```json
{
    "input_dir": "/home/myuser/data/preprocessed_data/DHS_n_more/",
    "dataset_type": "HR",
    "group_by_col": "adm2_gaul",
    "urban_rural_all_mode": "all",
    "drop_agriculture": false,
    "scale_numerical_data": true,
    "drop_countries": true,
    "egypt_dropping": true,
    "drop_columns": [
        "Meta; adm0_gaul",
        "Meta; GEID_init"
    ],
    "countries_to_drop": [
        "Egypt",
        "Comoros",
        "Central African Republic"
    ],
    "missingness_fraction": 0.3,
    "use_synthetic_data": true,
    "correlation_type": "no_correlation",
    "dim": 96,
    "N": 15000
}
```

You can adjust parameters like `use_synthetic_data`, `correlation_type`, `process_nans`, and the masking option interactively through prompts at runtime or directly via the `config.json` file.

---

## âœ… Step 1: Clone the repository

```bash
git clone https://github.com/yourusername/imputation.git
cd imputation
```

---

## âœ… Step 2: Install dependencies

```bash
pip install -r requirements.txt
```

---

## âœ… Step 3: Run the pipeline

```bash
python main.py
```

You will be guided by **interactive prompts** to configure your experiment settings:

- Choose **Synthetic** or **Real** data.
- For synthetic data, select the **correlation type**:
  - `no_correlation`
  - `medium_correlation`
  - `high_correlation`
- Choose the **missingness type**:
  - `MAR` (Missing At Random)
  - `MCAR` (Missing Completely At Random)

---

## ğŸ“Š **Generated Outputs**

After running the experiments, you'll find:

- ğŸ“ˆ **Visualizations**:
  - RMSE & performance metrics across methods.
  - Training and test time comparison.
  - Scatter plots (Actual vs Imputed values).

- ğŸ“ **Excel Reports**:
  - Detailed performance metrics (RMSE, MAE, RÂ², Correlation).

- ğŸ“‚ **Output Directories**:
  - `results/`
  - `output_images/`

---

## ğŸ” **Why is this Important?**

Missing data is a common challenge in large-scale surveys like DHS. This project helps you understand:

- Which **imputation method** works best under various conditions.
- The **trade-offs** between traditional and advanced deep learning methods.
- Provides **practical insights** into handling real-world missing data scenarios.

---

## ğŸ¤ **Want to Collaborate?**

This repository is currently set to private, available only to select collaborators. If you're interested in collaborating or discussing this project, feel free to reach out:

- ğŸ“§ **Email**: your.email@example.com  
- ğŸŒ **LinkedIn**: [Your LinkedIn Profile](https://linkedin.com/in/yourprofile)

---

## ğŸ“„ **License**

This project is open-sourced under the **MIT License**. Feel free to use, adapt, and distribute as needed!

---

## ğŸ™Œ **Acknowledgments**

Thank you for checking out my project! Your feedback and contributions are always welcome. Let's connect!

# ğŸ“‚ **Project Structure: Numerical + Categorical Dataset**

This project handles **missing data imputation** for datasets containing both **numerical** and **categorical** variables. The methods and evaluation differ from the numerical-only pipeline to accommodate categorical data using techniques like **one-hot encoding**, **precision**, **recall**, **F1-score**, and specialized plots.

---

## ğŸ“ Directory Layout

```
/project_root/
|
|â”ƒ-- data/                       # Raw or preprocessed datasets (numerical + categorical)
|â”ƒ-- notebooks/                 # Jupyter notebooks (specific to mixed data)
|â”ƒ-- results/                   # Auto-generated results (pickle files, metrics)
|â”ƒ-- src/                       # Source code for pipeline and utilities
|   |
|   |â”ƒ-- __init__.py
|   |â”ƒ-- config.py                # Configuration parameters for mixed datasets
|   |â”ƒ-- dhs_modelling_functions_new.py
|   |â”ƒ-- evaluation.py            # Evaluation: RMSE (numerical), F1, precision, recall (categorical)
|   |â”ƒ-- gain.py                  # GAIN imputation model
|   |â”ƒ-- imputations.py           # Imputation methods for both numerical & categorical
|   |â”ƒ-- masking.py               # Masking strategies for MAR/MCAR scenarios
|   |â”ƒ-- preprocessing.py         # One-hot encoding, label encoding, scaling
|   |â”ƒ-- requirements.txt         # Python dependencies
|   |â”ƒ-- run_pipeline.py          # Main orchestration for pipeline execution
|   |â”ƒ-- utils.py                 # Helper functions
|   â””-- visualization.py         # Plots: RMSE, precision, recall, F1-score
|â”ƒ-- main.py                    # Main script to run the pipeline (with interactive prompts)
â””-- README.md                   # Project overview & instructions
```

---

## ğŸ“Š **Key Differences (Compared to Numerical-Only Project)**

- **Data Handling**:
  - Includes **categorical columns** (e.g., type of residence, water source).
  - **One-hot encoding** for multi-class categorical columns.
  - **Label encoding** for binary categorical columns.

- **Imputation Methods**:
  - Support both **numerical** and **categorical** columns.
  - Categorical columns are imputed using modified deep learning models (e.g., GAIN) with **sigmoid activations**.

- **Evaluation Metrics**:
  - **Numerical** columns: RMSE, MAE, RÂ², correlation.
  - **Categorical** columns: Precision, Recall, F1-score (column-wise and row-wise).

- **Visualizations**:
  - **RMSE vs Missing Ratio** for numerical columns.
  - **Precision, Recall, F1-score plots** for categorical columns.
  - **Scatter plots** for actual vs imputed values (numerical).

---

## ğŸ”„ Workflow Overview

1. **Preprocessing**:
   - One-hot encode categorical columns.
   - Scale numerical columns.

2. **Imputation**:
   - Apply methods like **Mean**, **KNN**, **MICE**, **GAIN**, etc.
   - For categorical columns, use sigmoid activations in neural nets.

3. **Evaluation**:
   - RMSE, MAE, RÂ², Correlation for numerical.
   - Precision, Recall, F1-score for categorical.

4. **Visualization**:
   - Generate separate plots for numerical and categorical metrics.

---

## ğŸ“š Example Evaluation Metrics

- **Numerical**:
  - RMSE: 0.245 (Mean Â± Std)
  - RÂ²: 0.92

- **Categorical**:
  - Precision: 0.88
  - Recall: 0.85
  - F1-score: 0.86

---

## ğŸ“¢ Notes for Users

- Clone this project in a **dedicated local directory** (e.g., `~/projects/missing_data_imputation/`).
- The **`main.py`** script will guide you through setting **synthetic vs real data**, **correlation types**, **missingness types (MAR/MCAR)** interactively.
- Results and plots are saved automatically under **results/** and **output_images/**.

---

## ğŸ‘¥ Collaboration & License

- ğŸ“§ Email: your.email@example.com
- ğŸ‘¤ LinkedIn: [Your LinkedIn](https://www.linkedin.com/in/yourprofile)
- ğŸ“œ License: MIT License

---

Thank you for exploring this project! Your feedback and collaboration are always welcome.

# ğŸ“š **Missing Data Imputation for DHS Indicators - Master Repository**
_Combining Numerical-Only and Numerical + Categorical Data Pipelines_

Welcome to the **master repository** for my **Master's research project** on **missing data imputation** using both **numerical-only** and **numerical + categorical** datasets. This project compares traditional and deep learning-based imputation techniques, leveraging real-world DHS data and synthetic datasets.

---

## ğŸ“ **Repository Overview**

This repository is organized into two major subprojects:

1. **Numerical-Only Imputation Pipeline**: Focused on numerical features only.
2. **Numerical + Categorical Imputation Pipeline**: Handles datasets with both numerical and categorical variables.

Each pipeline is independent with its own:
- `main.py`
- `src/`
- `results/`
- `output_images/`

---

## ğŸ” **Project Structure**

```bash
msc_thesis_2025/
â”œâ”€â”€ README.md                       # Master repository README
â”œâ”€â”€ numerical_only/                 # Numerical-only imputation pipeline
â”‚   â”œâ”€â”€ README.md                   # README for numerical-only pipeline
â”‚   â”œâ”€â”€ main.py                     # Main script for numerical-only experiments
â”‚   â”œâ”€â”€ src/                        # Source code for numerical-only pipeline
â”‚   â”œâ”€â”€ results/                    # Pickle files for numerical-only results
â”‚   â””â”€â”€ output_images/              # Plots & reports for numerical-only
â”œâ”€â”€ numerical_categorical/          # Numerical + categorical imputation pipeline
â”‚   â”œâ”€â”€ README.md                   # README for numerical + categorical pipeline
â”‚   â”œâ”€â”€ main.py                     # Main script for mixed data experiments
â”‚   â”œâ”€â”€ src/                        # Source code for mixed data pipeline
â”‚   â”œâ”€â”€ results/                    # Pickle files for mixed data results
â”‚   â””â”€â”€ output_images/              # Plots & reports for mixed data
â””â”€â”€ data/                           # Shared datasets (real or synthetic)
```

Each pipeline follows a similar structure for easy navigation and reproducibility.

---

## ğŸ’¡ **How to Use This Repository**

### Step 1: Clone the repository
```bash
git clone https://github.com/yourusername/imputation.git
cd imputation
```

### Step 2: Choose your pipeline
- For **numerical-only** experiments:
  ```bash
  cd numerical_only
  ```
- For **numerical + categorical** experiments:
  ```bash
  cd numerical_categorical
  ```

### Step 3: Install dependencies
```bash
pip install -r src/requirements.txt
```

### Step 4: Run the pipeline
```bash
python main.py
```
You will be guided by **interactive prompts**:
- Choose **Synthetic** or **Real** data.
- Select **correlation type** (for synthetic data).
- Choose **missingness type** (MAR/MCAR).

---

## ğŸ”¹ **Subproject READMEs**
- [Numerical-Only Pipeline README](./numerical_only/README.md)
- [Numerical + Categorical Pipeline README](./numerical_categorical/README.md)

Each README provides detailed explanations specific to that pipeline, including configurations, evaluation metrics, and workflow.

---

## ğŸ’¼ **License**
This project is licensed under the **MIT License**. Feel free to use, adapt, and distribute.

---

## ğŸ‘¥ **Contact & Collaboration**

If you're interested in collaborating or discussing this project:
- ğŸ“§ **Email**: your.email@example.com
- ğŸŒ **LinkedIn**: [Your LinkedIn](https://linkedin.com/in/yourprofile)

---

Thank you for exploring this project! Check individual READMEs for more details.


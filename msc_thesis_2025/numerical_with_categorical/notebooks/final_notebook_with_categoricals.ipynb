{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center><u> Code Implementation </u></center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Work: Numerical with Categorical Dataset  \n",
    "---\n",
    "---\n",
    "### Code Implementation Walkthrough\n",
    "\n",
    "> • **Goal**: Demonstrate missing data imputation with *mixed numerical and categorical* variables using a modular pipeline.\n",
    "> \n",
    "> • **Code Folder**: All core functions are organized in the `src/` directory for clarity and reusability.\n",
    "> \n",
    "> • **Notebook Purpose**: Designed for execution and visualization. This notebook complements the *Additional Work* section of the thesis by supporting exploratory data analysis (EDA) and model evaluation.\n",
    ">  \n",
    "> • **Explore**: See `src/utils.py`, `src/evaluation.py`, `src/preprocessing.py`, etc. for all major implementation logic.\n",
    "\n",
    "---\n",
    "\n",
    "Once we explore the data and understand the structure of our **numerical + categorical DHS dataset**, we can run the **complete evaluation pipeline**.\n",
    "\n",
    "**This step will perform:**\n",
    "- Data loading and preprocessing\n",
    "- Artificial missingness generation (MCAR and MAR)\n",
    "- Imputation using multiple methods: Mean, KNN, MICE, AE, DAE, VAE, GAIN\n",
    "- Performance evaluation (RMSE, MAE, R², Accuracy, F1, Precision, Recall)\n",
    "- Time and scatter plot generation for visual analysis\n",
    "\n",
    "**Core pipeline logic is handled by**:\n",
    "All core logic is handled through the `main.py` script inside the project root and also by calling `src/` modules like:\n",
    "- `config.py`\n",
    "- `load_data.py`\n",
    "- `evaluation.py`\n",
    "- `run_pipeline.py`\n",
    "- `visualization.py` etc.\n",
    "\n",
    "**To run the full pipeline from this notebook:**\n",
    "```python\n",
    "# within the notebooks/ folder\n",
    "%run ../main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and Setup\n",
    "\n",
    "Importing necessary libraries for data manipulation, statistical methods, machine learning, deep learning and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yz4UliritTxP"
   },
   "outputs": [],
   "source": [
    "#importing required libraries\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import joblib\n",
    "import random\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from icecream import ic\n",
    "import tensorflow as tf\n",
    "from scipy import stats\n",
    "from tensorflow import keras\n",
    "from scipy.stats import skew\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from tensorflow.keras import Model\n",
    "from collections import defaultdict\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import Ridge\n",
    "from tensorflow.keras.losses import mse\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.stats import levene, f_oneway\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import L2\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from tensorflow.keras.initializers import he_uniform\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.losses import mse as keras_mse\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from tensorflow.keras.layers import Lambda, Layer, Input, Dense\n",
    "from sklearn.linear_model import BayesianRidge, LinearRegression\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense, Input, GaussianNoise, Layer\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras import Sequential, layers, models, optimizers, regularizers\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer, PolynomialFeatures, FunctionTransformer\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, GaussianNoise, Input, LeakyReLU, Add\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from src.utils import *\n",
    "from src.imputations import *\n",
    "from src.gain import GAIN_code_v2\n",
    "from src.load_data import load_data\n",
    "from src.setup_dirs import setup_directories\n",
    "from src.config import RESULTS_DIR, GET_FINAL_IMPUTATIONS_DIR\n",
    "from src.run_pipeline import compute_data, load_or_compute_data_part\n",
    "from src.masking import apply_masking_for_cat_num, extract_values_using_mask\n",
    "from src.dhs_modelling_functions_new import final_ds_droping_cols, fold_generator\n",
    "from src.visualization import missing_ratio_vs_stats, add_jitter_to_mean_accuracy\n",
    "from src.preprocessing import prepare_data, fold_generator_3_independent_indices, one_hot_encode_for_others, consistent_one_hot_encode\n",
    "from src.evaluation import evaluate_metrics_part, calculate_metrics, save_metrics_to_excel, missing_ratio_vs_stats, handle_masking_and_evaluation\n",
    "\n",
    "# initialization\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling this function in various parts of the loop to track memory consumption\n",
    "import psutil\n",
    "def check_memory():\n",
    "    print(f\"Current memory usage: {psutil.virtual_memory().percent}%\")\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the current working directory\n",
    "current_directory = os.getcwd()\n",
    "print(\"Current Directory:\", current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PGRLvmmdDrSH"
   },
   "outputs": [],
   "source": [
    "# setting up random seeds for reproducibility\n",
    "tf.random.set_seed(6688)\n",
    "random.seed(6688)\n",
    "np.random.seed(6688)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4D_reWvTDuEt"
   },
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu,True)\n",
    "    except RuntimeError as e:\n",
    "        raise e\n",
    "\n",
    "# use multiple GPUs\n",
    "gpus = [0] # may request more, if necessary\n",
    "gpus = [\"GPU:\" + str(i) for i in gpus]\n",
    "# https://keras.io/guides/distributed_training/\n",
    "print('gpus', gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference> https://github.com/gheisenberg/FoodSecurity/tree/main/DHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to display up to 1000 columns in outputs to ensure wide dataframes \n",
    "pd.set_option('display.max_columns', 1000)\n",
    "# loading the preprocessed household dataset from a manually adjusted pickle file.\n",
    "# this dataset version (V3) includes cleaned and harmonized features relevant for imputation experiments\n",
    "df_new = pd.read_pickle('/home/myuser/data/preprocessed_data/DHS_n_more/Manually_adjusted_df_V3_HR.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_new.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selected Common Features from the Original Household Dataset\n",
    "\n",
    "Only the common columns are retained from the DHS's original household dataset to maintain consistency and allow comparable downstream processing.\n",
    "\n",
    "- For having similar columns like original household dataset (numerical+aggregated), prefixes and suffixes are removed from this combined dataset.\n",
    "\n",
    "- These cleaned features above similar like original household dataset serve as the input space for evaluating the performance of missing data imputation methods on combined  numerical with categorical dataset.\n",
    "\n",
    "The variables span across several thematic domains:\n",
    "\n",
    "- **Household Composition & Structure**\n",
    "  - Number of household members\n",
    "  - Number of eligible children for height and weight\n",
    "  - Total adults measured\n",
    "  - Number of eligible women in household\n",
    "  - Number of children 5 and under (de jure)\n",
    "  - Number of rooms used for sleeping\n",
    "  - Household has separate room used as kitchen\n",
    "\n",
    "- **Water Access & Treatment**\n",
    "  - Source of drinking water\n",
    "  - Location of source for water\n",
    "  - Time to get to water source (minutes)\n",
    "  - Anything done to water to make it safe to drink\n",
    "  - Water treatment methods:\n",
    "    - Boiling\n",
    "    - Adding bleach/chlorine\n",
    "    - Straining through cloth\n",
    "    - Letting it stand and settle\n",
    "    - Using a water filter\n",
    "    - Solar disinfection\n",
    "    - Other\n",
    "\n",
    "- **Sanitation Facilities**\n",
    "  - Type of toilet facility\n",
    "  - Location of toilet facility\n",
    "  - Number of households sharing toilet\n",
    "  - Share toilet with other households\n",
    "  - Number of households sharing toilet: applicable\n",
    "\n",
    "- **Asset Ownership**\n",
    "  - Has electricity, radio, television, refrigerator, mobile phone, land-line phone, bicycle, car/truck, motorcycle/scooter, computer, watch, animal-drawn cart, boat with a motor\n",
    "  - Owns land usable for agriculture\n",
    "  - Owns livestock, herds or farm animals (sheep, goats, cows/bulls, horses/donkeys/mules, chickens/poultry)\n",
    "  - Has bank account\n",
    "\n",
    "- **Other Socioeconomic Indicators**\n",
    "  - Type of place of residence (urban/rural)\n",
    "  - Type of cooking fuel\n",
    "  - Food cooked in the house/separate building/outdoors\n",
    "  - Cluster altitude in meters\n",
    "  - Translator used\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Columns Based on Main Aggregated Numerical Dataset\n",
    "\n",
    "This following step ensures consistency between the Original Household Dataset (numerical+aggregated) and the newer combined dataset that includes categorical features. We retain only the selected variables that were previously original numerical dataset. \n",
    "\n",
    "> Note: The columns `'URBAN_RURA: R'` and `'FS; IPC: 0-2y'` were present in the main numerical dataset but are **not** part of the categorical dataset. Their absence triggered a KeyError and they have been excluded from the final filtered list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of selected and cleaned numerical + binary categorical features from the old dataset.\n",
    "# these were confirmed to be relevant and harmonized in earlier preprocessing steps.\n",
    "# columns such as 'URBAN_RURA: R' and 'FS; IPC: 0-2y' caused KeyError as they do not exist in this df_new\n",
    "# hence, they were removed from this list before selection.\n",
    "cleaned_columns_from_old_numerical_dataset = [\n",
    "    'number of household members',\n",
    "    'owns sheep',\n",
    "    'number of eligible children for height and weight',\n",
    "    'total adults measured',\n",
    "    'number of mosquito bed nets',\n",
    "    'cluster altitude in meters',\n",
    "    'number of eligible women in household',\n",
    "    'number of children 5 and under (de jure)',\n",
    "    'number of households sharing toilet',\n",
    "    'hectares of agricultural land (1 decimal)',\n",
    "    'number of rooms used for sleeping',\n",
    "    'owns goats',\n",
    "    'owns horses/ donkeys/ mules',\n",
    "    'time to get to water source (minutes)',\n",
    "    'owns cows/ bulls',\n",
    "    'owns chickens/poultry',\n",
    "    'type of place of residence',\n",
    "    'source of drinking water',\n",
    "    'has electricity',\n",
    "    'has radio',\n",
    "    'has bicycle',\n",
    "    'has car/truck',\n",
    "    'type of toilet facility',\n",
    "    'has refrigerator',\n",
    "    'translator used',\n",
    "    'has telephone (land-line)',\n",
    "    'has bank account',\n",
    "    'share toilet with other households',\n",
    "    'location of source for water',\n",
    "    'anything done to water to make safe to drink',\n",
    "    'water usually treated by: boil',\n",
    "    'water usually treated by: add bleach/chlorine',\n",
    "    'location of toilet facility',\n",
    "    'household has separate room used as kitchen',\n",
    "    'water usually treated by: strain through a cloth',\n",
    "    'water usually treated by: let it stand and settle',\n",
    "    'has watch',\n",
    "    'has boat with a motor',\n",
    "    'has animal-drawn cart',\n",
    "    'has a computer',\n",
    "    'water usually treated by: other',\n",
    "    'water usually treated by: solar disinfection',\n",
    "    'type of cooking fuel',\n",
    "    'food cooked in the house/ separate building/ outdoors',\n",
    "    'has mobile telephone',\n",
    "    'owns land usable for agriculture',\n",
    "    'has television',\n",
    "    'has motorcycle/scooter',\n",
    "    'water usually treated by: use water filter',\n",
    "    'owns livestock, herds or farm animals',\n",
    "    'number of households sharing toilet: applicable'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KeyError: \"['URBAN_RURA: R', 'FS; IPC: 0-2y'] not in index\"\n",
    "# safely select only the columns from df_new that match the cleaned list.\n",
    "# this avoids KeyErrors caused by missing or renamed columns.\n",
    "df2_filtered = df_new[cleaned_columns_from_old_numerical_dataset]  \n",
    "\n",
    "# alternatively, drop all columns that are NOT part of the cleaned list (same effect).\n",
    "# ensures df2_filtered contains only the agreed-upon set of features.\n",
    "df2_filtered = df_new.drop(columns=[col for col in df_new.columns if col not in cleaned_columns_from_old_numerical_dataset])\n",
    "\n",
    "# output retained column names to confirm successful filtering.\n",
    "print(\"Columns retained from old aggregated numerical Dataset 1, in combined categorical Dataset 2:\")\n",
    "print(df2_filtered.columns)\n",
    "# df2_filtered.value_counts\n",
    "# only 51 columns remains as like as main numerical dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning Based on Missingness Threshold\n",
    "\n",
    "To ensure high data quality for imputation and modeling, we apply a missingness threshold of 40%. The logic below performs the following:\n",
    "\n",
    "- Drops columns with more than 40% missing values unless they are essential and listed in `columns_to_keep`.\n",
    "- Drops rows with more than 40% missing values across remaining columns.\n",
    "- Retains all key variables similar to the main numerical dataset for consistency across experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the missingness threshold: 40%\n",
    "# means we tolerate up to 40% missing data in rows or columns before removing them.\n",
    "# setting threshold\n",
    "threshold = 0.40  \n",
    "\n",
    "# defining the essential columns to retain (even if missingness exceeds the threshold).\n",
    "# these are consistent with the main numerical dataset and are considered important for analysis.\n",
    "# list of columns to keep as like as main numerical dataset\n",
    "columns_to_keep = [\n",
    "    'number of household members', 'owns sheep', 'number of eligible children for height and weight',\n",
    "    'total adults measured', 'number of mosquito bed nets', 'cluster altitude in meters',\n",
    "    'number of eligible women in household', 'number of children 5 and under (de jure)',\n",
    "    'number of households sharing toilet', 'hectares of agricultural land (1 decimal)',\n",
    "    'number of rooms used for sleeping', 'owns goats', 'owns horses/ donkeys/ mules',\n",
    "    'time to get to water source (minutes)', 'owns cows/ bulls', 'owns chickens/poultry',\n",
    "    'type of place of residence', 'source of drinking water', 'has electricity', 'has radio',\n",
    "    'has bicycle', 'has car/truck', 'type of toilet facility', 'has refrigerator', 'translator used',\n",
    "    'has telephone (land-line)', 'has bank account', 'share toilet with other households',\n",
    "    'location of source for water', 'anything done to water to make safe to drink',\n",
    "    'water usually treated by: boil', 'water usually treated by: add bleach/chlorine',\n",
    "    'location of toilet facility', 'household has separate room used as kitchen',\n",
    "    'water usually treated by: strain through a cloth', 'water usually treated by: let it stand and settle',\n",
    "    'has watch', 'has boat with a motor', 'has animal-drawn cart', 'has a computer',\n",
    "    'water usually treated by: other', 'water usually treated by: solar disinfection',\n",
    "    'type of cooking fuel', 'food cooked in the house/ separate building/ outdoors',\n",
    "    'has mobile telephone', 'owns land usable for agriculture', 'has television',\n",
    "    'has motorcycle/scooter', 'water usually treated by: use water filter',\n",
    "    'owns livestock, herds or farm animals', 'number of households sharing toilet: applicable'\n",
    "]\n",
    "\n",
    "# the percentage of missing values per column.\n",
    "missing_percentage = df_new.isnull().mean()\n",
    "\n",
    "# columns with more than 40% missing values AND not part of the columns_to_keep list\n",
    "# these columns will be dropped from the dataset\n",
    "columns_to_drop = [col for col in df_new.columns if (missing_percentage[col] > threshold) and (col not in columns_to_keep)]\n",
    "\n",
    "# dropping the selected columns from the dataset\n",
    "df_cleaned = df_new.drop(columns=columns_to_drop)\n",
    "\n",
    "# missing value percentage for each row (axis=1)\n",
    "row_missing_percentage = df_cleaned.isnull().mean(axis=1)\n",
    "\n",
    "# identifying rows where more than 40% of values are missing\n",
    "rows_to_drop = row_missing_percentage[row_missing_percentage > threshold].index\n",
    "\n",
    "# dropping those rows from the dataset.\n",
    "df_cleaned = df_cleaned.drop(index=rows_to_drop)\n",
    "\n",
    "# output dropped columns and number of dropped rows.\n",
    "print(\"Dropped columns with > 40% missing values:\")\n",
    "print(columns_to_drop)\n",
    "print(f\"Dropped {len(rows_to_drop)} rows with > 40% missing values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropping Metadata and Non-Informative Columns\n",
    "\n",
    "The following columns are metadata or technical identifiers (e.g., interview date, household ID, survey version) that do not contribute to the imputation task or predictive modeling. They are dropped from the cleaned dataset to reduce noise and ensure only meaningful categorical variables remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_and_other_columns_to_drop_for_categoricals_data = [\n",
    "    'country code and phase',  \n",
    "    'cluster number', \n",
    "    'household number', \n",
    "    'month of interview', \n",
    "    'day of interview',  \n",
    "    'case identification',\n",
    "    'DHSID', \n",
    "    'DHSID + HHID', \n",
    "    'country code',\n",
    "    'region',\n",
    "    'version_nr', \n",
    "    'subversion_nr'\n",
    "]\n",
    "# dropping the specified columns from the cleaned dataframe\n",
    "df_cleaned.drop(columns=meta_and_other_columns_to_drop_for_categoricals_data, inplace=True)\n",
    "print(df_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cleaned.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Selected Columns to Numeric Format\n",
    "\n",
    "Some columns may have been read as object or string types due to inconsistent formatting or embedded non-numeric values. To ensure consistency and compatibility with numerical analyses and imputation techniques, the selected variables below are explicitly converted to numeric types.\n",
    "\n",
    "- Non-numeric entries will be safely converted to `NaN` using `errors='coerce'`.\n",
    "- This step preserves missing values while ensuring correct data types for numerical processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts the column to numeric while keeping NaN values as NaN\n",
    "# list of columns to convert to numeric\n",
    "columns_to_convert = [\n",
    "    'time to get to water source (minutes)',\n",
    "    'owns chickens/poultry',\n",
    "    'owns horses/ donkeys/ mules',\n",
    "    'owns cows/ bulls',\n",
    "    'owns goats',\n",
    "    'number of households sharing toilet',\n",
    "    'number of rooms used for sleeping',\n",
    "    'hectares of agricultural land (1 decimal)',\n",
    "    'cluster altitude in meters',\n",
    "    'number of eligible women in household',\n",
    "    'number of children 5 and under (de jure)',\n",
    "    'total adults measured',\n",
    "    'owns sheep',\n",
    "    'number of household members',\n",
    "    'year of interview'\n",
    "]\n",
    "\n",
    "# Loop through each column and convert to numeric\n",
    "# Convert each column to numeric type, coercing any invalid entries to NaN.\n",
    "for column in columns_to_convert:\n",
    "    df_cleaned[column] = pd.to_numeric(df_cleaned[column], errors='coerce')\n",
    "\n",
    "# Print the data types of the converted columns to confirm successful transformation.\n",
    "print(df_cleaned[columns_to_convert].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after dropping irrelevant columns and converting data types, we inspect the dataset to see how many missing (`NaN`) values remain in each column. \n",
    "# this summary helps identify which features may still need imputation or further attention\n",
    "df_cleaned.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Cleanup and Filtering for Final Input\n",
    "\n",
    "This section performs the final filtering and preparation steps before using the dataset for modeling or imputation tasks:\n",
    "\n",
    "- Rows with missing `GEID` (a unique household/group identifier) are dropped.\n",
    "- A custom function `drop_all_nans()` is used to drop all rows that contain any missing values across any column.\n",
    "- The cleaned dataset is then randomly sampled (9%) for efficient processing in downstream experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping rows where the 'GEID' column has missing values\n",
    "# 'GEID' is assumed to be a unique group/household identifier essential for analysis.\n",
    "# dropping rows where the 'GEID' column has missing values\n",
    "df_cleaned = df_cleaned.dropna(subset=['GEID'])\n",
    "\n",
    "# verifying that no missing 'GEID' values remain.\n",
    "print(df_cleaned['GEID'].isna().sum())\n",
    "\n",
    "# displaying the count of missing values across each column.\n",
    "# thisgives insight into remaining missingness across the main 37–53 variables.\n",
    "df_cleaned.isnull().sum()\n",
    "\n",
    "# at this point, the dataset has 1,363,097 rows and 53 columns.\n",
    "# defining a function to drop any rows that have missing values in *any* column.\n",
    "\n",
    "def drop_all_nans(df):\n",
    "    \"\"\"\n",
    "    Drops all rows that contain any NaN values across any column.\n",
    "    Returns a completely non-missing dataset and prints the resulting shape.\n",
    "    \"\"\"\n",
    "    non_missing_indices = {}\n",
    "    data_with_missing = df.copy()\n",
    "    \n",
    "    # identifying non-missing indices per column\n",
    "    for col in data_with_missing.columns:\n",
    "        non_missing_indices[col] = data_with_missing[col].dropna().index\n",
    "    \n",
    "    # reconstructing values from non-missing indices, column by column\n",
    "    values_df = pd.DataFrame()\n",
    "    for col, indices in non_missing_indices.items():\n",
    "        values_df[col] = data_with_missing.loc[indices, col].reset_index(drop=True)\n",
    "    \n",
    "    # dropping any remaining rows with NaNs\n",
    "    actual_values_df = values_df.dropna()\n",
    "    \n",
    "    # the final shape of the fully cleaned dataset\n",
    "    rows = actual_values_df.shape\n",
    "    print(f\"Shape after dropping all NaNs: {rows}\")\n",
    "    \n",
    "    return actual_values_df\n",
    "\n",
    "# applying the function to obtain the final dataset with no missing values.\n",
    "input_df = drop_all_nans(df_cleaned)\n",
    "\n",
    "# double-checking that no NaNs remain in the final dataset.\n",
    "input_df.isnull().sum()\n",
    "\n",
    "# randomly sample 9% of the dataset for further experimentation.\n",
    "# this helps reduce computation time while maintaining representativeness.\n",
    "input_df = input_df.sample(frac=0.09, random_state=6688)\n",
    "\n",
    "# displaying the shape of the sampled dataset.\n",
    "print(input_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df.to_csv('/home/myuser/prj/code/final_computation/numerical_with_categorical_dataset_complete_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset contains 'Categorical Imbalance'\n",
    "### Analysis\n",
    "\n",
    "This function detects and summarizes imbalance in categorical variables by computing the percentage representation of the most frequent class in each column.\n",
    "\n",
    "Key Features:\n",
    "- Automatically detects categorical columns based on data types.\n",
    "- Displays the top `N` most imbalanced categorical features (highest percentage of top class).\n",
    "- Helps identify dominant class distributions that may bias learning or require balancing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_categorical_imbalance(df, top_n=30):\n",
    "    \"\"\"\n",
    "    Analyze imbalance in the categorical part of a combined dataset.\n",
    "\n",
    "    Args:\n",
    "        df: Combined DataFrame with categorical and numerical columns\n",
    "        top_n: Number of top most imbalanced categorical columns to display\n",
    "\n",
    "    Returns:\n",
    "        A summary DataFrame of imbalance metrics per categorical column\n",
    "    \"\"\"\n",
    "    \n",
    "    # automatically detect categorical columns (dtype: object or category)\n",
    "    categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    if not categorical_columns:\n",
    "        print(\"No categorical columns found in the dataset.\")\n",
    "        return\n",
    "\n",
    "    imbalance_summary = []\n",
    "\n",
    "    # analyzing imbalance for each categorical column\n",
    "    for col in categorical_columns:\n",
    "        counts = df[col].value_counts(dropna=False)  # include NaNs\n",
    "        total = counts.sum()\n",
    "        top_class = counts.idxmax()  # most frequent class\n",
    "        top_class_pct = (counts.max() / total) * 100\n",
    "\n",
    "        imbalance_summary.append({\n",
    "            \"Column\": col,\n",
    "            \"Top Class\": top_class,\n",
    "            \"Top Class Count\": counts.max(),\n",
    "            \"Total Count\": total,\n",
    "            \"Top Class %\": round(top_class_pct, 2)\n",
    "        })\n",
    "\n",
    "    # converting results to DataFrame and sort by top class dominance\n",
    "    summary_df = pd.DataFrame(imbalance_summary)\n",
    "    summary_df = summary_df.sort_values(by=\"Top Class %\", ascending=False)\n",
    "\n",
    "    print(\"\\n Imbalance Summary for Categorical Columns:\")\n",
    "    display(summary_df.head(top_n))  # using display() for Jupyter; use print(summary_df.head(top_n)) in scripts\n",
    "\n",
    "    return summary_df\n",
    "\n",
    "# running imbalance analysis on the final input DataFrame\n",
    "summary_df = analyze_categorical_imbalance(input_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments:\n",
    "\n",
    "### Class Imbalance in Categorical Columns from combined DHS dataset and Its real Impact\n",
    "---\n",
    "---\n",
    "\n",
    "An imbalance analysis was conducted across all categorical columns in the combined dataset. The results show **severe class imbalance**, where some classes (e.g., \"no\", \"applicable\", \"rural\") dominate more than **90–99%** of the records.\n",
    "\n",
    "### Why It Matters:\n",
    "\n",
    "Such extreme imbalance negatively impacts **classification-based metrics** such as:\n",
    "\n",
    "- **Precision**\n",
    "- **Recall**\n",
    "- **F1-Score**\n",
    "\n",
    "Even when a model performs well, these metrics may appear **low or misleading** for underrepresented classes because:\n",
    "\n",
    "- The classifier learns to **default to the majority class**.\n",
    "- **Minority class instances** are too sparse to influence the model.\n",
    "- Metrics like **F1-Score** become biased due to limited positive cases.\n",
    "\n",
    "> In the real world, especially in **household survey datasets**, this level of imbalance is common and expected. Features like \"Has car/truck\" or \"Water treated by: solar disinfection\" are naturally skewed.\n",
    "\n",
    "### Supporting Literature:\n",
    "- He, H., & Garcia, E. A. (2009). *Learning from imbalanced data.* ACM Computing Surveys (CSUR), 42(1), 1-40.\n",
    "- Japkowicz, N., & Stephen, S. (2002). *The class imbalance problem: A systematic study.* Intelligent data analysis, 6(5), 429-449.\n",
    "\n",
    "### How We Handle This:\n",
    "- All metrics are **interpreted with imbalance in mind**.\n",
    "- Evaluation is performed across **multiple metrics** (RMSE, R², accuracy, F1, precision) for a balanced overview.\n",
    "- Class imbalance was not artificially adjusted to reflect **real-world deployment scenarios**.\n",
    "\n",
    "For a detailed summary, refer to the **Imbalance Summary Table** above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Pipeline Execution – Additional Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To run pipeline for 'numerical with categorical' DHS dataset from notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### to enable masking-based evaluation (MAR) or (MCAR)\n",
    "print(\"\\n========= Starting Evaluation =========\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### via running main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n========= Finished Evaluation =========\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Reference and Explanation\n",
    "\n",
    "---\n",
    "The following codes related to my thesis project's 'Additional Work' section is hidden for clarity. You can find all of these 'important' functions in this folder `src/` → "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation Fold Generator with Independent Train, Validation, and Test Splits\n",
    "\n",
    "This function `fold_generator_3_independent_indices()` is designed to generate **three-way splits** (train, validation, and test) for cross-validation, depending on a specified grouping strategy.\n",
    "\n",
    "It supports the following `split_type` options:\n",
    "- `'survey'`: Splits based on unique **GEID** values (e.g., survey identifiers).\n",
    "- `'year'`: Splits based on **rounded year of interview**, averaged per GEID.\n",
    "- `'unconditional'`: Ignores grouping and applies standard K-Fold split on the full dataset.\n",
    "\n",
    "Key Features:\n",
    "- Uses **K-Fold cross-validation** for outer loop splitting.\n",
    "- Each outer fold produces independent train, validation, and test sets.\n",
    "- Inner training and validation splits are derived using `train_test_split`.\n",
    "- Automatically adjusts `n_splits` if the number of unique groups is insufficient.\n",
    "\n",
    "**Parameters:**\n",
    "- `data` (`pd.DataFrame`): The full dataset.\n",
    "- `split_type` (`str`): Splitting strategy – `'survey'`, `'year'`, or `'unconditional'`.\n",
    "- `n_splits` (`int`): Number of cross-validation folds (outer loop).\n",
    "- `verbose` (`int`): Controls logging output.\n",
    "- `val_size` (`float`): Proportion of training+validation split to allocate for validation.\n",
    "\n",
    "**Yields:**\n",
    "- A tuple of index arrays: `(train_indices, val_indices, test_indices)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def fold_generator_3_independent_indices(data, split_type, n_splits=2, verbose=1, val_size=0.2):\n",
    "    \"\"\"\n",
    "    Generate indices for train, validation and test sets based on the specified split type.\n",
    "\n",
    "    Parameters:\n",
    "    data (DataFrame): The input dataset.\n",
    "    split_type (str): The type of split - 'country', 'survey', or 'year'.\n",
    "    n_splits (int): Number of splits/folds for the outer cross-validation.\n",
    "    verbose (int): Level of verbosity.\n",
    "    val_size (float): Proportion of the dataset to include in the validation split.\n",
    "    \"\"\"\n",
    "    if split_type == 'survey':\n",
    "        split_col = 'GEID'\n",
    "    elif split_type == 'year':\n",
    "        split_col = 'year of interview'\n",
    "        # Ensure 'Meta; rounded year' column is created outside this function or create here based on logic provided\n",
    "        data[split_col] = data.groupby('GEID')['year of interview'].transform(lambda x: round(x.mean()))\n",
    "    elif split_type == 'unconditional':\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        for train_val_idx, test_idx in kf.split(data):\n",
    "            # Split the train_val indices into training and validation indices\n",
    "            train_idx, val_idx = train_test_split(train_val_idx, test_size=val_size, random_state=42)\n",
    "            yield data.index[train_idx], data.index[val_idx], data.index[test_idx]\n",
    "        return\n",
    "    else:\n",
    "        raise ValueError(f'Invalid split_type: {split_type}')\n",
    "\n",
    "    unique_combinations = data[split_col].drop_duplicates().values\n",
    "\n",
    "    # Adjust maximum n_splits based on the number of unique combinations\n",
    "    if len(unique_combinations) < n_splits or n_splits == -1:\n",
    "        n_splits = len(unique_combinations)\n",
    "        if verbose:\n",
    "            print(f'Adjusting n_splits to the length of unique combinations ({n_splits}) for', split_type)\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    for train_val_combinations, test_combinations in kf.split(unique_combinations):\n",
    "        # Split the train_val combinations into training and validation combinations\n",
    "        train_combinations, val_combinations = train_test_split(train_val_combinations, test_size=val_size, random_state=42)\n",
    "        \n",
    "        # Create masks for training, validation, and test sets\n",
    "        train_mask = data[split_col].isin(unique_combinations[train_combinations])\n",
    "        val_mask = data[split_col].isin(unique_combinations[val_combinations])\n",
    "        test_mask = data[split_col].isin(unique_combinations[test_combinations])\n",
    "        \n",
    "        # Get the indices for training, validation, and test sets\n",
    "        train_indices = data[train_mask].index.values\n",
    "        val_indices = data[val_mask].index.values\n",
    "        test_indices = data[test_mask].index.values\n",
    "        \n",
    "        # Ensure train size is always larger than test\n",
    "        assert len(train_indices) > len(test_indices), \\\n",
    "            f\"Train size {len(train_indices)} should be larger than test size {len(test_indices)}\"\n",
    "\n",
    "        yield train_indices, val_indices, test_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking Function: apply_masking_for_cat_num()\n",
    "\n",
    "This function introduces artificial missingness into a dataset for experimental purposes, supporting two common missing data mechanisms:\n",
    "\n",
    "- **MCAR (Missing Completely At Random)**: Randomly masks values across all columns.\n",
    "- **MAR (Missing At Random)**: Masks values only in certain columns, conditioned on socio-economic variables (e.g., electricity, land ownership).\n",
    "\n",
    "**Parameters:**\n",
    "- `X_data` (`DataFrame`): The original dataset to modify.\n",
    "- `masking` (`bool`): \n",
    "  - If `True`, applies MAR logic.\n",
    "  - If `False`, applies MCAR logic.\n",
    "- `missingness_fraction` (`float`): Fraction of values to be masked (set to `NaN`) per column.\n",
    "\n",
    "**Returns:**\n",
    "- A modified version of `X_data` with `NaN` values.\n",
    "- A boolean mask (`missing_mask`) where `True` marks a missing value.\n",
    "\n",
    "This is useful for benchmarking imputation methods under controlled missingness patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def apply_masking_for_cat_num(X_data, masking, missingness_fraction=0.3):\n",
    "    \"\"\"\n",
    "    Apply MAR or MCAR missingness to a given DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - X_data: pd.DataFrame, original dataset.\n",
    "    - masking: bool, True = MAR, False = MCAR.\n",
    "    - missingness_fraction: float, fraction of values to mask.\n",
    "\n",
    "    Returns:\n",
    "    - Modified DataFrame with NaNs introduced.\n",
    "    - missing_mask: Boolean array marking masked positions.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    X_data = X_data.copy()  # Prevent modifying original data\n",
    "    missing_mask = np.zeros(X_data.shape, dtype=bool)  # Mask to track missing entries\n",
    "\n",
    "    # Columns that MAR logic will condition on (only if present)\n",
    "    condition_columns = [\n",
    "        'type of place of residence',\n",
    "        'has electricity',\n",
    "        'has bank account',\n",
    "        'owns land usable for agriculture'\n",
    "    ]\n",
    "    condition_columns = [col for col in condition_columns if col in X_data.columns]\n",
    "\n",
    "    if masking:\n",
    "        print(\"🔶 Applying MAR masking based on fixed socio-economic conditions...\")\n",
    "\n",
    "        # Apply missingness only to columns not used as MAR conditions\n",
    "        target_columns = [col for col in X_data.columns if col not in condition_columns]\n",
    "\n",
    "        # Define MAR condition: mask is applied where any of these conditions hold\n",
    "        conditions = (\n",
    "            (X_data.get('type of place of residence') == 'rural') |\n",
    "            (X_data.get('has electricity') == 0) |\n",
    "            (X_data.get('has bank account') == 0) |\n",
    "            (X_data.get('owns land usable for agriculture') == 1)\n",
    "        )\n",
    "\n",
    "        eligible_indices = X_data[conditions].index  # Eligible rows for MAR masking\n",
    "\n",
    "        # Loop over each target column to apply MAR-style missingness\n",
    "        for col in target_columns:\n",
    "            if col not in X_data.columns:\n",
    "                continue\n",
    "\n",
    "            non_missing = X_data[col].dropna().index\n",
    "            col_eligible = non_missing.intersection(eligible_indices)\n",
    "            num_to_mask = int(np.ceil(missingness_fraction * len(col_eligible)))\n",
    "\n",
    "            if num_to_mask == 0:\n",
    "                continue\n",
    "\n",
    "            # Randomly select rows to mask\n",
    "            sample_indices = np.random.choice(col_eligible, size=num_to_mask, replace=False)\n",
    "            local_idx = X_data.index.get_indexer(sample_indices)\n",
    "\n",
    "            # Update missing mask and set values to NaN\n",
    "            missing_mask[local_idx, X_data.columns.get_loc(col)] = True\n",
    "            X_data.loc[sample_indices, col] = np.nan\n",
    "\n",
    "    else:\n",
    "        print(\"Applying MCAR masking (completely random)...\")\n",
    "\n",
    "        # Apply random masking independently to each column\n",
    "        for col in X_data.columns:\n",
    "            non_missing_indices = X_data[col].dropna().index\n",
    "            num_to_mask = int(np.ceil(missingness_fraction * len(non_missing_indices)))\n",
    "\n",
    "            if num_to_mask == 0:\n",
    "                continue\n",
    "\n",
    "            random_sample = np.random.choice(non_missing_indices, size=num_to_mask, replace=False)\n",
    "            local_indices = X_data.index.get_indexer(random_sample)\n",
    "\n",
    "            # Apply random NaN and update missing mask\n",
    "            missing_mask[local_indices, X_data.columns.get_loc(col)] = True\n",
    "            X_data.loc[random_sample, col] = np.nan\n",
    "\n",
    "    return X_data, missing_mask\n",
    "\n",
    "# def apply_masking_for_cat_num(X_data, masking, missingness_fraction):\n",
    "#     \"\"\"\n",
    "#     Apply random missingness to the dataset, with MAR logic applied \n",
    "#     when 'type of place of residence' is 'rural'.\n",
    "    \n",
    "#     Parameters:\n",
    "#     - X_data: DataFrame containing the data.\n",
    "#     - masking: Boolean indicating whether to apply masking.\n",
    "#     - missingness_fraction: Fraction of data to be set as NaN.\n",
    "    \n",
    "#     Returns:\n",
    "#     - X_data: DataFrame with applied missingness.\n",
    "#     - missing_mask: Boolean array indicating where values are missing.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     missing_mask = np.zeros(X_data.shape, dtype=bool)\n",
    "    \n",
    "#     if masking:\n",
    "#         # Get the indices where 'type of place of residence' is 'rural'\n",
    "#         rural_indices = X_data[X_data['type of place of residence'] == 'rural'].index\n",
    "\n",
    "#         # Iterate through each column (except 'type of place of residence')\n",
    "#         for col in X_data.columns:\n",
    "#             if col == 'type of place of residence':\n",
    "#                 continue  # Skip the target column itself\n",
    "            \n",
    "#             non_missing_indices = X_data[col].dropna().index\n",
    "            \n",
    "#             # Calculate the intersection of non-missing and rural indices\n",
    "#             eligible_indices = non_missing_indices.intersection(rural_indices)\n",
    "            \n",
    "#             # Calculate number of samples to mask\n",
    "#             num_to_mask = int(np.ceil(missingness_fraction * len(eligible_indices)))\n",
    "#             if num_to_mask == 0:\n",
    "#                 continue\n",
    "            \n",
    "#             # Randomly select from eligible indices (i.e., rows where 'rural' and not already NaN)\n",
    "#             random_sample = np.random.choice(eligible_indices, size=num_to_mask, replace=False)\n",
    "#             local_indices = X_data.index.get_indexer(random_sample)\n",
    "            \n",
    "#             # Mask the selected values\n",
    "#             missing_mask[local_indices, X_data.columns.get_loc(col)] = True\n",
    "#             X_data.loc[random_sample, col] = np.nan\n",
    "\n",
    "#     else:\n",
    "#         # Apply random masking (MCAR) to all columns\n",
    "#         for col in X_data.columns:\n",
    "#             non_missing_indices = X_data[col].dropna().index\n",
    "#             random_sample = np.random.choice(non_missing_indices, size=int(np.ceil(missingness_fraction * len(non_missing_indices))), replace=False)\n",
    "#             local_indices = X_data.index.get_indexer(random_sample)\n",
    "#             missing_mask[local_indices, X_data.columns.get_loc(col)] = True\n",
    "#             X_data.loc[random_sample, col] = np.nan\n",
    "            \n",
    "#     return X_data, missing_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation Function: `prepare_data()`\n",
    "\n",
    "This function prepares input data for modeling by applying scaling to numerical columns while preserving the original values in categorical columns. It ensures consistency across the training, validation, and test sets.\n",
    "\n",
    "**Key Features:**\n",
    "- Automatically identifies numerical and categorical columns.\n",
    "- Scales only the numerical features using `StandardScaler` (zero mean, unit variance).\n",
    "- Maintains column ordering to ensure alignment between datasets.\n",
    "\n",
    "**Parameters:**\n",
    "- `X_train`, `X_val`, `X_test` (`DataFrame`): Partitioned data subsets.\n",
    "- `config` (`dict`): Contains pipeline settings, including `scale_numerical_data`.\n",
    "\n",
    "**Returns:**\n",
    "- Scaled versions of `X_train`, `X_val`, and `X_test` with both numerical and categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def prepare_data(X_train, X_val, X_test, config):\n",
    "    \"\"\"\n",
    "    Prepares the data by scaling numerical columns while leaving categorical columns unchanged.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train: Training data (DataFrame).\n",
    "    - X_val: Validation data (DataFrame).\n",
    "    - X_test: Test data (DataFrame).\n",
    "    - config: Dictionary containing configuration options.\n",
    "\n",
    "    Returns:\n",
    "    - X_train_scaled: Scaled training data (DataFrame).\n",
    "    - X_val_scaled: Scaled validation data (DataFrame).\n",
    "    - X_test_scaled: Scaled test data (DataFrame).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Identify numerical and categorical columns\n",
    "    numerical_columns = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_columns = X_train.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "    # Initialize copies to preserve structure\n",
    "    X_train_scaled, X_val_scaled, X_test_scaled = X_train.copy(), X_val.copy(), X_test.copy()\n",
    "\n",
    "    # Apply scaling to numerical columns if enabled in config\n",
    "    if config['scale_numerical_data']:\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        # Scale numerical columns while keeping the index\n",
    "        X_train_scaled[numerical_columns] = scaler.fit_transform(X_train[numerical_columns])\n",
    "        X_val_scaled[numerical_columns] = scaler.transform(X_val[numerical_columns])\n",
    "        X_test_scaled[numerical_columns] = scaler.transform(X_test[numerical_columns])\n",
    "\n",
    "    # Ensure numerical columns come first, followed by categorical columns\n",
    "    ordered_columns = numerical_columns + categorical_columns\n",
    "    X_train_scaled = X_train_scaled[ordered_columns]\n",
    "    X_val_scaled = X_val_scaled[ordered_columns]\n",
    "    X_test_scaled = X_test_scaled[ordered_columns]\n",
    "\n",
    "    return X_train_scaled, X_val_scaled, X_test_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAIN-Based Imputation Code (Adapted from Open Implementation)\n",
    "adaptation from: https://github.com/DeltaFloflo/imputation_comparison/blob/main/build/class_gain.py\n",
    "\n",
    "The full implementation of a modified **GAIN (Generative Adversarial Imputation Network)** is integrated for handling missing data in tabular 'numerical with categorical' DHS dataset. The approach is based on the adversarial framework, where:\n",
    "\n",
    "- The **Generator (G)** fills in missing values.\n",
    "- The **Discriminator (D)** tries to distinguish observed from imputed values using a hint matrix.\n",
    "\n",
    "#### Main Components:\n",
    "\n",
    "- `normalization()` / `renormalization()`: Min-max normalization utilities.\n",
    "- `reset_weights()`: Re-initializes model weights for reproducibility.\n",
    "- `maskDistribution()` and `drawMasks()`: Utilities to mimic real-world missingness patterns.\n",
    "- `drawHintMatrix()`: Constructs hint matrix for training.\n",
    "- `make_GAINgen()` / `make_GAINdisc()`: Neural architectures for Generator and Discriminator.\n",
    "- `GAIN_code_v2`: Class implementing training, loss calculation and imputation pipeline.\n",
    "  \n",
    "#### Normalization Utilities\n",
    "GAIN expects normalized numerical input. We use Min-Max normalization here.\n",
    "\n",
    "#### Resetting Model Weights\n",
    "This function ensures the Generator and Discriminator can be retrained cleanly from scratch.\n",
    "\n",
    "#### Mask and Hint Matrix Utilities\n",
    "Helps generate masks that simulate missingness and hint matrices that reveal partial observations to the discriminator.\n",
    "\n",
    "#### GAIN Network Architectures\n",
    "Generator and Discriminator models with dropout and batch normalization.\n",
    "\n",
    "#### GAIN Training Logic (GAIN_code_v2 Class)\n",
    "Combines the generator and discriminator into the GAIN framework, with customized loss functions for adversarial training and reconstruction.\n",
    "\n",
    "\n",
    "This version supports:\n",
    "- Conditional treatment of categorical columns (`cat_mask`)\n",
    "- Separate reconstruction and adversarial loss components\n",
    "- Tracking of loss curves across epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Based on: https://github.com/DeltaFloflo/imputation_comparison\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# GAIN (Generative Adversarial Imputation Networks) - Implementation\n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "# importing required libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.stats import pearsonr\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "\n",
    "# -----------------------\n",
    "# Normalization Utilities\n",
    "# -----------------------\n",
    "\n",
    "# required TensorFlow Keras components for the GAIN model\n",
    "# normalizes numerical data between [0, 1] for stable training\n",
    "def normalization(data, norm_params=None, numeric_mask=None):\n",
    "    \"\"\"\n",
    "    Normalizes numerical columns in the dataset to [0, 1] range.\n",
    "    \"\"\"\n",
    "    N, D = data.shape\n",
    "    norm_data = data.copy()\n",
    "\n",
    "    if numeric_mask is None:\n",
    "        numeric_mask = np.array([np.issubdtype(data[:, d].dtype, np.number) for d in range(D)])\n",
    "\n",
    "    if norm_params is None:\n",
    "        # computing min and max for each column\n",
    "        min_val = np.zeros(D)\n",
    "        max_val = np.zeros(D)\n",
    "        for d in range(D):\n",
    "            if numeric_mask[d]:\n",
    "                m1 = np.nanmin(data[:, d])\n",
    "                m2 = np.nanmax(data[:, d])\n",
    "                min_val[d] = m1\n",
    "                max_val[d] = m2\n",
    "                # applying min-max normalization\n",
    "                norm_data[:, d] = (data[:, d] - m1) / (m2 - m1 + 1e-6)\n",
    "        norm_params = {\"min_val\": min_val, \"max_val\": max_val}\n",
    "    else:\n",
    "        # uses previously stored parameters\n",
    "        min_val = norm_params[\"min_val\"]\n",
    "        max_val = norm_params[\"max_val\"]\n",
    "        for d in range(D):\n",
    "            if numeric_mask[d]:\n",
    "                m1 = min_val[d]\n",
    "                m2 = max_val[d]\n",
    "                norm_data[:, d] = (data[:, d] - m1) / (m2 - m1 + 1e-6)\n",
    "\n",
    "    return norm_data, norm_params\n",
    "\n",
    "# reverts min-max normalization\n",
    "def renormalization(norm_data, norm_params):\n",
    "    \"\"\"\n",
    "    Reverses normalization back to original scale.\n",
    "    \"\"\"\n",
    "    N, D = norm_data.shape\n",
    "    min_val = norm_params[\"min_val\"]\n",
    "    max_val = norm_params[\"max_val\"]\n",
    "    data = norm_data.copy()\n",
    "    for d in range(D):\n",
    "        m1 = min_val[d]\n",
    "        m2 = max_val[d]\n",
    "        data[:, d] = norm_data[:, d] * (m2 - m1 + 1e-6) + m1\n",
    "    return data\n",
    "\n",
    "# ----------------------\n",
    "# Model Reset Utility\n",
    "# ----------------------\n",
    "\n",
    "# reinitializes all model weights (for clean retraining)\n",
    "def reset_weights(model):\n",
    "    \"\"\"\n",
    "    Reinitialize weights of all layers in a Keras model.\n",
    "    \"\"\"\n",
    "    \"\"\"Completely reinitialize model parameters\"\"\"\n",
    "    for layer in model.layers:\n",
    "        if layer.name[:5] == \"dense\":\n",
    "            # getting the shape of the kernel (weights) to determine the input and output dimensions\n",
    "            # Xavier/Glorot uniform initialization\n",
    "            kernel_shape = layer.kernel.shape  # alternatively, layer.weights[0].shape\n",
    "            \n",
    "            nb_in = kernel_shape[0]  # input dimension\n",
    "            nb_out = kernel_shape[1]  # output dimension\n",
    "            limit = np.sqrt(6.0 / (nb_in + nb_out))\n",
    "            \n",
    "            # reinitialize the kernel (weights) and bias\n",
    "            r1 = np.random.uniform(-limit, limit, size=kernel_shape)\n",
    "            r2 = np.zeros(shape=layer.bias.shape)\n",
    "            \n",
    "            layer.set_weights([r1, r2])\n",
    "        \n",
    "        elif layer.name[:19] == \"batch_normalization\":\n",
    "            # getting the shape of the gamma (scaling factor) to initialize batch normalization parameters\n",
    "            # resetting batch norm parameters\n",
    "            gamma_shape = layer.gamma.shape  # alternatively, layer.weights[0].shape\n",
    "            \n",
    "            r1 = np.ones(shape=gamma_shape)  # gamma\n",
    "            r2 = np.zeros(shape=gamma_shape)  # beta\n",
    "            r3 = np.zeros(shape=gamma_shape)  # moving mean\n",
    "            r4 = np.ones(shape=gamma_shape)  # moving variance\n",
    "            \n",
    "            layer.set_weights([r1, r2, r3, r4])\n",
    "\n",
    "# ----------------------------------------\n",
    "# Mask Distribution and Sampling Functions\n",
    "# ----------------------------------------\n",
    "\n",
    "# extracts distribution of missingness masks in dataset\n",
    "def maskDistribution(dataset):\n",
    "    \"\"\"\n",
    "    Get the unique missingness patterns and their counts from dataset.\n",
    "    \"\"\"\n",
    "    \"\"\"unique_masks: list of unique NaN masks found in the dataset\n",
    "    count_masks: corresponding number of occurrences (the probability distrib.)\"\"\"\n",
    "    mask = (1.0 - np.isnan(dataset)).astype(\"int\")\n",
    "    unique_masks = np.unique(mask, axis=0)\n",
    "    count_masks = np.zeros(len(unique_masks), dtype=\"int\")\n",
    "    for i1 in range(mask.shape[0]):\n",
    "        current_mask = mask[i1]\n",
    "        i2 = np.where((unique_masks == current_mask).all(axis=1))[0][0]\n",
    "        count_masks[i2] += 1\n",
    "    return unique_masks, count_masks\n",
    "\n",
    "# samples new missingness patterns from the mask distribution\n",
    "def drawMasks(unique_masks, probs, N):\n",
    "    \"\"\"\n",
    "    Sample N new masks from the known distribution.\n",
    "    \"\"\"\n",
    "    \"\"\"unique_masks: list of unique masks from which to choose\n",
    "    probs: vector of probability (should sum up to one)\n",
    "    N: number of samples to draw\n",
    "    masks: list of size N containing one mask per row drawn from the desired distribution\"\"\"\n",
    "    multinom = np.random.multinomial(n=1, pvals=probs, size=N)\n",
    "    indices = np.where(multinom==1)[1]\n",
    "    masks = unique_masks[indices]\n",
    "    return masks\n",
    "\n",
    "# creates the hint matrix that reveals part of the mask to the discriminator\n",
    "def drawHintMatrix(p, nb_rows, nb_cols):\n",
    "    \"\"\"\n",
    "    Generate hint matrix with given probability.\n",
    "    \"\"\"\n",
    "    \"\"\"p: probability of ones\n",
    "    nb_rows: number of desired rows in the hint matrix H\n",
    "    nb_cols: number of desired columns in the hint matrix H\n",
    "    H: hint matrix\"\"\"\n",
    "    H = np.random.uniform(0., 1., size=(nb_rows, nb_cols))\n",
    "    H = 1.0 * (H < p)\n",
    "    return H\n",
    "\n",
    "# --------------------------------------------\n",
    "# GAIN Generator and Discriminator Builders\n",
    "# --------------------------------------------\n",
    "\n",
    "# generator network of GAIN\n",
    "def make_GAINgen(dim):\n",
    "    \"\"\"\n",
    "    Generator network: predicts missing values.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(512, activation=\"relu\", kernel_regularizer=l2(1e-4), input_shape=(2*dim,)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(512, activation=\"relu\", kernel_regularizer=l2(1e-4)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(dim, activation=\"sigmoid\")\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# discriminator network of GAIN\n",
    "def make_GAINdisc(dim):\n",
    "    \"\"\"\n",
    "    Discriminator network: distinguishes observed vs. imputed entries.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(512, activation=\"relu\", input_shape=(2*dim,), kernel_regularizer=l2(1e-4)), # specifically manually configured for categorical with numerical dataset\n",
    "        Dropout(0.3),\n",
    "        Dense(512, activation=\"relu\", kernel_regularizer=l2(1e-4)),\n",
    "        Dropout(0.3),\n",
    "        Dense(dim, activation=\"sigmoid\")\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# -------------------------------------------\n",
    "# GAIN Code Version 2 (Training + Imputation)\n",
    "# -------------------------------------------\n",
    "\n",
    "# the GAIN_code_v2 class\n",
    "class GAIN_code_v2:\n",
    "    def __init__(self, dim, cat_mask=None):\n",
    "        self.dim = dim\n",
    "        self.G = make_GAINgen(dim) # generator\n",
    "        self.D = make_GAINdisc(dim) # discriminator\n",
    "        self.Goptim = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "        self.Doptim = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "        self.alpha = 100 # reconstruction vs adversarial loss weight # Weight for reconstruction loss\n",
    "        self.hint_rate = 0.9 # % of known features revealed to discriminator\n",
    "        self.trained = False\n",
    "        self.nb_epochs = 0\n",
    "        self.Gloss1 = [] # adversarial loss\n",
    "        self.Gloss2 = [] # reconstruction loss\n",
    "        self.Dloss = []\n",
    "        self.cat_mask = tf.convert_to_tensor(cat_mask, dtype=tf.float32) if cat_mask is not None else None\n",
    "\n",
    "    # discriminator loss: Binary cross-entropy with hint masking\n",
    "    @staticmethod\n",
    "    def compute_D_loss(D_output, M, H):\n",
    "        \"\"\"\n",
    "        Discriminator loss (binary cross-entropy)\n",
    "        \"\"\"\n",
    "        L1 = M * tf.math.log(D_output + 1e-6)\n",
    "        L2 = (1.0 - M) * tf.math.log(1.0 - D_output + 1e-6)\n",
    "        L = - (L1 + L2) * tf.cast((H == 0.5), dtype=tf.float32)\n",
    "        nb_cells = tf.math.reduce_sum(tf.cast((H == 0.5), dtype=tf.float32))\n",
    "        return tf.math.reduce_sum(L) / nb_cells if nb_cells > 0 else 0.0\n",
    "        \n",
    "    # generator loss: fool discriminator + minimize reconstruction error\n",
    "    def compute_G_loss(self, G_output, D_output, X, M, H):\n",
    "        \"\"\"\n",
    "        Generator loss: adversarial + reconstruction components\n",
    "        \"\"\"\n",
    "        Ltemp = - ((1.0 - M) * tf.math.log(D_output + 1e-6))\n",
    "        L = Ltemp * tf.cast((H == 0.5), dtype=tf.float32)\n",
    "        nb_cells1 = tf.math.reduce_sum(tf.cast((H == 0.5), dtype=tf.float32))\n",
    "        loss1 = tf.math.reduce_sum(L) / nb_cells1 if nb_cells1 > 0 else 0.0\n",
    "\n",
    "        squared_err = ((X - G_output) ** 2) * M\n",
    "        if self.cat_mask is not None:\n",
    "            weighted_err = squared_err * (2.0 * self.cat_mask + 1.0 * (1.0 - self.cat_mask))\n",
    "        else:\n",
    "            weighted_err = squared_err\n",
    "\n",
    "        nb_cells2 = tf.math.reduce_sum(M)\n",
    "        loss2 = tf.math.reduce_sum(weighted_err) / nb_cells2 if nb_cells2 > 0 else 0.0\n",
    "        return loss1, loss2\n",
    "        \n",
    "    # reset model for fresh training\n",
    "    def reinitialize(self):\n",
    "        reset_weights(self.G)\n",
    "        reset_weights(self.D)\n",
    "        self.trained = False\n",
    "        self.nb_epochs = 0\n",
    "        self.Gloss1 = []\n",
    "        self.Gloss2 = []\n",
    "        self.Dloss = []\n",
    "\n",
    "    # single training step (compiled for speed with @tf.function)\n",
    "    @tf.function\n",
    "    def train_step(self, batch_data):\n",
    "        \"\"\"\n",
    "        One training iteration for both generator and discriminator.\n",
    "        \"\"\"\n",
    "        cur_batch_size = batch_data.shape[0]\n",
    "        noise = tf.random.normal([cur_batch_size, self.dim], dtype=tf.float32)\n",
    "        batch_data = tf.cast(batch_data, dtype=tf.float32)\n",
    "        M = 1.0 - tf.cast(tf.math.is_nan(batch_data), dtype=tf.float32)\n",
    "        X = tf.where(tf.math.is_nan(batch_data), noise, batch_data)\n",
    "        G_input = tf.concat((X, M), axis=1)\n",
    "\n",
    "        with tf.GradientTape() as G_tape, tf.GradientTape() as D_tape:\n",
    "            G_output = self.G(G_input, training=True)\n",
    "            X_hat = X * M + G_output * (1.0 - M)\n",
    "            # constructs hint matrix\n",
    "            Htemp = tf.cast(drawHintMatrix(self.hint_rate, cur_batch_size, self.dim), dtype=tf.float32)\n",
    "            H = M * Htemp + 0.5 * (1.0 - Htemp)\n",
    "            D_input = tf.concat((X_hat, H), axis=1)\n",
    "            D_output = self.D(D_input, training=True)\n",
    "\n",
    "            D_loss = self.compute_D_loss(D_output, M, H)\n",
    "            G_loss1, G_loss2 = self.compute_G_loss(G_output, D_output, X, M, H)\n",
    "            G_loss = G_loss1 + self.alpha * G_loss2\n",
    "\n",
    "            G_gradients = G_tape.gradient(G_loss, self.G.trainable_variables)\n",
    "            D_gradients = D_tape.gradient(D_loss, self.D.trainable_variables)\n",
    "\n",
    "            self.Goptim.apply_gradients(zip(G_gradients, self.G.trainable_variables))\n",
    "            self.Doptim.apply_gradients(zip(D_gradients, self.D.trainable_variables))\n",
    "\n",
    "            return G_loss1, G_loss2, D_loss\n",
    "            \n",
    "    # full training loop\n",
    "    def train(self, dataset, batch_size=8192, epochs=20):\n",
    "        \"\"\"\n",
    "        Train GAIN model on the full dataset.\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            G_temp1, G_temp2, D_temp = [], [], []\n",
    "            for batch_idx in range(0, dataset.shape[0], batch_size):\n",
    "                batch_data = dataset[batch_idx:batch_idx + batch_size]\n",
    "                G_loss1, G_loss2, D_loss = self.train_step(batch_data)\n",
    "                G_temp1.append(G_loss1.numpy())\n",
    "                G_temp2.append(G_loss2.numpy())\n",
    "                D_temp.append(D_loss.numpy())\n",
    "            self.Gloss1.append(np.mean(G_temp1))\n",
    "            self.Gloss2.append(np.mean(G_temp2))\n",
    "            self.Dloss.append(np.mean(D_temp))\n",
    "            \n",
    "    # impute missing values using the trained generator\n",
    "    def impute(self, nandata):\n",
    "        \"\"\"\n",
    "        Impute missing entries in the input using trained generator.\n",
    "        \"\"\"\n",
    "        noise = tf.random.normal([nandata.shape[0], self.dim])\n",
    "        M_impute = 1.0 - np.isnan(nandata)\n",
    "        X_impute = tf.where((M_impute == 0.0), noise, nandata)\n",
    "        G_input = tf.concat((X_impute, M_impute), axis=1)\n",
    "        G_output = self.G(G_input, training=False)\n",
    "        imputed_data = (X_impute * M_impute + G_output * (1.0 - M_impute)).numpy()\n",
    "        return imputed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

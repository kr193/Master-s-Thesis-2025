{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qp1-ETbItTxO"
   },
   "source": [
    "<h1><center><u> Code Implementation </u></center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yz4UliritTxP"
   },
   "outputs": [],
   "source": [
    "#importing Libraries\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from icecream import ic\n",
    "import tensorflow as tf\n",
    "from scipy.stats import skew\n",
    "from tensorflow import keras\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from tensorflow.keras import Model\n",
    "from collections import defaultdict\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.losses import mse\n",
    "from tensorflow.keras import Sequential\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.stats import levene, f_oneway\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import L2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from tensorflow.keras.initializers import he_uniform\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.losses import mse as keras_mse\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from tensorflow.keras.layers import Lambda, Layer, Input, Dense\n",
    "from sklearn.linear_model import BayesianRidge, LinearRegression\n",
    "from sklearn.model_selection import RepeatedKFold, train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.layers import Dense, Input, GaussianNoise, Layer\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, KFold\n",
    "from tensorflow.keras import Sequential, layers, models, optimizers, regularizers\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer, PolynomialFeatures, FunctionTransformer\n",
    "\n",
    "from cross_val_models_statistics import *\n",
    "from layersconfig import *\n",
    "\n",
    "# initialization\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize()\n",
    "from dhs_modelling_functions_new import final_ds_droping_cols, fold_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PGRLvmmdDrSH"
   },
   "outputs": [],
   "source": [
    "# setting up random seeds for reproducibility\n",
    "tf.random.set_seed(6688)\n",
    "random.seed(6688)\n",
    "np.random.seed(6688)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference from https://github.com/gheisenberg/FoodSecurity/tree/main/DHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# configurations and directory creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'scale_numerical_data': True,\n",
    "    'masking': True,\n",
    "    'drop_countries': True,\n",
    "    'egypt_dropping': True,\n",
    "    'process_nans': 'numerical_only_drop_20_percentage_nans',  # 'drop_all_nans' or 'numerical_only' or 'numerical_only_drop_20_percentage_nans'\n",
    "    'drop_columns': ['Meta; adm0_gaul', 'Meta; GEID_init', 'Meta; year'],\n",
    "    'countries_to_drop': ['Egypt', 'Comoros', 'Central African Republic'],\n",
    "    'missingness_fraction': 0.3,\n",
    "    'base_dir': '/home/myuser/prj/code/final_computation/cross_validation_github'\n",
    "}\n",
    "def setup_directories(config):\n",
    "\n",
    "    # checking the process_nans condition \n",
    "    if config['process_nans'] == 'numerical_only':\n",
    "        base_dir = '/home/myuser/prj/code/final_computation/cross_validation_github/keeping_all_numerical'\n",
    "    elif config['process_nans'] == 'drop_all_nans':\n",
    "        base_dir = '/home/myuser/prj/code/final_computation/cross_validation_github/drop_all_nans'\n",
    "    elif config['process_nans'] == 'numerical_only_drop_20_percentage_nans':\n",
    "        base_dir = '/home/myuser/prj/code/final_computation/cross_validation_github/drop_20_percentage'\n",
    "\n",
    "    # updating the base directory in config\n",
    "    config['base_dir'] = base_dir\n",
    "    # creation of masking directories\n",
    "    with_masking_dir = os.path.join(base_dir, 'with_masking')\n",
    "    without_masking_dir = os.path.join(base_dir, 'without_masking')\n",
    "    # ensuring that the base directories exist (with_masking and without_masking)\n",
    "    os.makedirs(with_masking_dir, exist_ok=True)\n",
    "    os.makedirs(without_masking_dir, exist_ok=True)\n",
    "\n",
    "    return with_masking_dir, without_masking_dir\n",
    "\n",
    "with_masking_dir, without_masking_dir = setup_directories(config)\n",
    "# parsing layer configuration\n",
    "def parse_layer_config(folder_name):\n",
    "    try:\n",
    "        return list(map(int, folder_name.split('_')))\n",
    "    except ValueError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dropping all nans from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_all_nans(df):\n",
    "    non_missing_indices = {}\n",
    "    data_with_missing = df.copy()\n",
    "    for col in data_with_missing.columns:\n",
    "        non_missing_indices[col] = data_with_missing[col].dropna().index\n",
    "    \n",
    "    values_df = pd.DataFrame()\n",
    "    for col, indices in non_missing_indices.items():\n",
    "        values_df[col] = data_with_missing.loc[indices, col].reset_index(drop=True)\n",
    "    \n",
    "    actual_values_df = values_df.dropna()\n",
    "    rows = actual_values_df.shape\n",
    "    print(f\"Shape after dropping all NaNs: {rows}\")\n",
    "    \n",
    "    return actual_values_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_initial_missingness(df):\n",
    "    total_elements = df.size\n",
    "    missing_elements = df.isnull().sum().sum()\n",
    "    return missing_elements / total_elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(config):\n",
    "    input_dir = \"/home/myuser/data/preprocessed_data/DHS_n_more/\"\n",
    "    dataset_type = 'HR'\n",
    "    group_by_col = 'adm2_gaul'\n",
    "    urban_rural_all_mode = 'all'\n",
    "    drop_agriculture = False\n",
    "\n",
    "    in_f = f\"{input_dir}5_grouped_df_V3_{dataset_type}_{group_by_col}_joined_with_ipc_{urban_rural_all_mode}.pkl\"\n",
    "    df = pd.read_pickle(in_f)\n",
    "\n",
    "    initial_missingness= calculate_initial_missingness(df)\n",
    "\n",
    "    df = final_ds_droping_cols(df, drop_meta=True, drop_food_help=True, drop_perc=40,\n",
    "                               retain_month=False, drop_highly_correlated_cols=False, drop_region=True, \n",
    "                               drop_data_sets=['Meta one-hot encoding', 'Meta frequency encoding'], \n",
    "                               use_NAN_amount_and_replace_NANs_in_categorical=False, drop_agricultural_cols=drop_agriculture, \n",
    "                               drop_below_version=False, numerical_data=['mean'], retain_adm=False, \n",
    "                               retain_GEID_init=False, verbose=3)\n",
    "    \n",
    "    # dropping unnecessary columns\n",
    "    drop_cols = [c for c in df.columns if 'FS;' in c and '0-2y' not in c]\n",
    "    df = df.drop(columns=drop_cols)\n",
    "    df.drop(columns=['DHS Cat; translator used: not at all', 'DHS Cat; translator used: yes'], axis=1, inplace=True)\n",
    "\n",
    "    # dropping countries with different data shifts\n",
    "    if config['egypt_dropping']:\n",
    "        dropping = config['countries_to_drop']\n",
    "        df = df[~df['Meta; adm0_gaul'].isin(dropping)]\n",
    "\n",
    "    # handling NaNs based on the process_nans condition\n",
    "    if config['process_nans'] == 'drop_all_nans':\n",
    "        # dropping all NaNs and return the dataframe immediately\n",
    "        df = drop_all_nans(df)\n",
    "        return df, initial_missingness\n",
    "    \n",
    "    elif config['process_nans'] == 'numerical_only':\n",
    "        # if 'numerical_only', just drop Egypt and return without further processing\n",
    "        return df, initial_missingness\n",
    "    \n",
    "    elif config['process_nans'] == 'numerical_only_drop_20_percentage_nans':\n",
    "        # performing drop Egypt first and then apply the 20% missingness logic\n",
    "        if 'Meta; GEID_init' in df.columns:\n",
    "            # The missingness percentage for each survey\n",
    "            survey_missingness = df.groupby('Meta; GEID_init').apply(lambda x: x.isna().mean().mean())\n",
    "            print(f\"Original dataframe shape: {df.shape}\")\n",
    "            \n",
    "            # Filter out surveys with missingness above 20%\n",
    "            surveys_to_keep = survey_missingness[survey_missingness <= 0.2].index\n",
    "            filtered_df = df[df['Meta; GEID_init'].isin(surveys_to_keep)]\n",
    "            \n",
    "            df = filtered_df.copy()\n",
    "            print(f\"Filtered dataframe shape: {filtered_df.shape}\")\n",
    "        else:\n",
    "            print(\"Warning: 'Meta; GEID_init' column not found. Skipping survey filtering.\")\n",
    "\n",
    "        return df, initial_missingness\n",
    "\n",
    "    return df, initial_missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df, initial_missingness =load_data(config)\n",
    "input_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## customized fold generator function for cross-val step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fold_generator_3_independent_indices(data, split_type, n_splits=5, verbose=1, val_size=0.2):\n",
    "    \"\"\"\n",
    "    Generate indices for train, validation and test sets based on the specified split type.\n",
    "\n",
    "    Parameters:\n",
    "    data (DataFrame): The input dataset.\n",
    "    split_type (str): The type of split - 'country', 'survey', or 'year'.\n",
    "    n_splits (int): Number of splits/folds for the outer cross-validation.\n",
    "    verbose (int): Level of verbosity.\n",
    "    val_size (float): Proportion of the dataset to include in the validation split.\n",
    "    \"\"\"\n",
    "    if split_type == 'country':\n",
    "        split_col = 'Meta; adm0_gaul'\n",
    "    elif split_type == 'survey':\n",
    "        split_col = 'Meta; GEID_init'\n",
    "    elif split_type == 'year':\n",
    "        split_col = 'Meta; rounded year'\n",
    "        # Ensure 'Meta; rounded year' column is created outside this function or create here based on logic provided\n",
    "        data[split_col] = data.groupby('Meta; GEID_init')['Meta; year'].transform(lambda x: round(x.mean()))\n",
    "    elif split_type == 'unconditional':\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        for train_val_idx, test_idx in kf.split(data):\n",
    "            # Split the train_val indices into training and validation indices\n",
    "            train_idx, val_idx = train_test_split(train_val_idx, test_size=val_size, random_state=42)\n",
    "            yield data.index[train_idx], data.index[val_idx], data.index[test_idx]\n",
    "        return\n",
    "    else:\n",
    "        raise ValueError(f'Invalid split_type: {split_type}')\n",
    "\n",
    "    unique_combinations = data[split_col].drop_duplicates().values\n",
    "\n",
    "    # Adjust maximum n_splits based on the number of unique combinations\n",
    "    if len(unique_combinations) < n_splits or n_splits == -1:\n",
    "        n_splits = len(unique_combinations)\n",
    "        if verbose:\n",
    "            print(f'Adjusting n_splits to the length of unique combinations ({n_splits}) for', split_type)\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    for train_val_combinations, test_combinations in kf.split(unique_combinations):\n",
    "        # Split the train_val combinations into training and validation combinations\n",
    "        train_combinations, val_combinations = train_test_split(train_val_combinations, test_size=val_size, random_state=42)\n",
    "        \n",
    "        # Create masks for training, validation, and test sets\n",
    "        train_mask = data[split_col].isin(unique_combinations[train_combinations])\n",
    "        val_mask = data[split_col].isin(unique_combinations[val_combinations])\n",
    "        test_mask = data[split_col].isin(unique_combinations[test_combinations])\n",
    "        \n",
    "        # Get the indices for training, validation, and test sets\n",
    "        train_indices = data[train_mask].index.values\n",
    "        val_indices = data[val_mask].index.values\n",
    "        test_indices = data[test_mask].index.values\n",
    "        \n",
    "        # Yielding the indices for train, validation and test sets\n",
    "        yield train_indices, val_indices, test_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## normalizing data if true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(X_train, X_val, X_test, config):\n",
    "    if config['process_nans'] == 'drop_all_nans':\n",
    "        X_train, X_val, X_test = (\n",
    "        X_train.drop(columns=config['drop_columns']), \n",
    "        X_val.drop(columns=config['drop_columns']), \n",
    "        X_test.drop(columns=config['drop_columns'])\n",
    "    )\n",
    "    if config['scale_numerical_data']:\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "        X_val_scaled = pd.DataFrame(scaler.transform(X_val), columns=X_val.columns)\n",
    "        X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "    else:\n",
    "        X_train_scaled, X_val_scaled, X_test_scaled = X_train, X_val, X_test\n",
    "    \n",
    "    return X_train_scaled, X_val_scaled, X_test_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAIN method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adaptation from: https://github.com/DeltaFloflo/imputation_comparison/blob/main/build/class_gain.py\n",
    "# -----------------------\n",
    "# Normalization Utilities\n",
    "# -----------------------\n",
    "\n",
    "def normalization(data, norm_params=None):\n",
    "    \"\"\"\n",
    "    Normalize the data column-wise to a [0, 1] range.\n",
    "    Args:\n",
    "        data (numpy array): Input data to normalize.\n",
    "        norm_params (dict): Optional parameters for normalization (min and max values).\n",
    "\n",
    "    Returns:\n",
    "        norm_data (numpy array): Normalized data.\n",
    "        norm_params (dict): Normalization parameters used.\n",
    "    \"\"\"\n",
    "    N, D = data.shape\n",
    "    if norm_params is None:\n",
    "        min_val = np.zeros(D)\n",
    "        max_val = np.zeros(D)\n",
    "        norm_data = data.copy()\n",
    "        for d in range(D):\n",
    "            m1 = np.nanmin(data[:, d])\n",
    "            m2 = np.nanmax(data[:, d])\n",
    "            min_val[d] = m1\n",
    "            max_val[d] = m2\n",
    "            norm_data[:, d] = (data[:, d] - m1) / (m2 - m1 + 1e-6)\n",
    "        norm_params = {\"min_val\": min_val, \"max_val\": max_val}\n",
    "    else:\n",
    "        min_val = norm_params[\"min_val\"]\n",
    "        max_val = norm_params[\"max_val\"]\n",
    "        norm_data = data.copy()\n",
    "        for d in range(D):\n",
    "            m1 = min_val[d]\n",
    "            m2 = max_val[d]\n",
    "            norm_data[:, d] = (data[:, d] - m1) / (m2 - m1 + 1e-6)\n",
    "    return norm_data, norm_params\n",
    "\n",
    "def renormalization(norm_data, norm_params):\n",
    "    \"\"\"\n",
    "    Reverse normalization to original scale.\n",
    "    Args:\n",
    "        norm_data (numpy array): Normalized data.\n",
    "        norm_params (dict): Parameters used for normalization.\n",
    "\n",
    "    Returns:\n",
    "        data (numpy array): Data rescaled to original range.\n",
    "    \"\"\"\n",
    "    N, D = norm_data.shape\n",
    "    min_val = norm_params[\"min_val\"]\n",
    "    max_val = norm_params[\"max_val\"]\n",
    "    data = norm_data.copy()\n",
    "    for d in range(D):\n",
    "        m1 = min_val[d]\n",
    "        m2 = max_val[d]\n",
    "        data[:, d] = norm_data[:, d] * (m2 - m1 + 1e-6) + m1\n",
    "    return data\n",
    "\n",
    "# ----------------------\n",
    "# Model Reset Utility\n",
    "# ----------------------\n",
    "\n",
    "def reset_weights(model):\n",
    "    \"\"\"\n",
    "    Completely reinitialize model parameters\n",
    "    \"\"\"\n",
    "    for layer in model.layers:\n",
    "        if layer.name[:5] == \"dense\":\n",
    "            # getting the shape of the kernel (weights) to determine the input and output dimensions\n",
    "            kernel_shape = layer.kernel.shape  # alternatively, layer.weights[0].shape\n",
    "            \n",
    "            nb_in = kernel_shape[0]  # input dimension\n",
    "            nb_out = kernel_shape[1]  # output dimension\n",
    "            limit = np.sqrt(6.0 / (nb_in + nb_out))\n",
    "            \n",
    "            # reinitializing the kernel (weights) and bias\n",
    "            r1 = np.random.uniform(-limit, limit, size=kernel_shape)\n",
    "            r2 = np.zeros(shape=layer.bias.shape)\n",
    "            \n",
    "            layer.set_weights([r1, r2])\n",
    "        \n",
    "        elif layer.name[:19] == \"batch_normalization\":\n",
    "            # getting the shape of the gamma (scaling factor) to initialize batch normalization parameters\n",
    "            gamma_shape = layer.gamma.shape  # alternatively, layer.weights[0].shape\n",
    "            \n",
    "            r1 = np.ones(shape=gamma_shape)  # gamma\n",
    "            r2 = np.zeros(shape=gamma_shape)  # beta\n",
    "            r3 = np.zeros(shape=gamma_shape)  # moving mean\n",
    "            r4 = np.ones(shape=gamma_shape)  # moving variance\n",
    "            \n",
    "            layer.set_weights([r1, r2, r3, r4])\n",
    "\n",
    "# ----------------------------------------\n",
    "# Mask Distribution and Sampling Functions\n",
    "# ----------------------------------------\n",
    "\n",
    "def maskDistribution(dataset):\n",
    "    \"\"\"\n",
    "    unique_masks: list of unique NaN masks found in the dataset\n",
    "    count_masks: corresponding number of occurrences (the probability distrib.)\n",
    "    \"\"\"\n",
    "    mask = (1.0 - np.isnan(dataset)).astype(\"int\")\n",
    "    unique_masks = np.unique(mask, axis=0)\n",
    "    count_masks = np.zeros(len(unique_masks), dtype=\"int\")\n",
    "    for i1 in range(mask.shape[0]):\n",
    "        current_mask = mask[i1]\n",
    "        i2 = np.where((unique_masks == current_mask).all(axis=1))[0][0]\n",
    "        count_masks[i2] += 1\n",
    "    return unique_masks, count_masks\n",
    "\n",
    "def drawMasks(unique_masks, probs, N):\n",
    "    \"\"\"\n",
    "    unique_masks: list of unique masks from which to choose\n",
    "    probs: vector of probability (should sum up to one)\n",
    "    N: number of samples to draw\n",
    "    masks: list of size N containing one mask per row drawn from the desired distribution\n",
    "    \"\"\"\n",
    "    multinom = np.random.multinomial(n=1, pvals=probs, size=N)\n",
    "    indices = np.where(multinom==1)[1]\n",
    "    masks = unique_masks[indices]\n",
    "    return masks\n",
    "\n",
    "def drawHintMatrix(p, nb_rows, nb_cols):\n",
    "    \"\"\"\n",
    "    Generate a hint matrix for GAIN training.\n",
    "    Args:\n",
    "        p (float): Probability of 1s in the hint matrix.\n",
    "        nb_rows (int): Number of desired rows in the matrix H.\n",
    "        nb_cols (int): Number of desired columns in the matrix H.\n",
    "\n",
    "    Returns:\n",
    "        H (numpy array): Hint matrix.\n",
    "    \"\"\"\n",
    "    H = np.random.uniform(0., 1., size=(nb_rows, nb_cols))\n",
    "    H = 1.0 * (H < p)\n",
    "    return H\n",
    "\n",
    "# --------------------------------------------\n",
    "# GAIN Generator and Discriminator Builders\n",
    "# --------------------------------------------\n",
    "\n",
    "# generator network of GAIN for num+cat dataset\n",
    "def make_GAINgen(dim):\n",
    "    \"\"\"\n",
    "    Create the generator model for GAIN.\n",
    "    Args:\n",
    "        dim (int): Dimensionality of the input data.\n",
    "\n",
    "    Returns:\n",
    "        model (tf.keras.Sequential): Generator model.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(475, activation=\"elu\", input_shape=(2*dim,)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(855, activation=\"elu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(855, activation=\"elu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(dim, activation=\"linear\"))\n",
    "    return model\n",
    "\n",
    "# discriminator network of GAIN\n",
    "def make_GAINdisc(dim):\n",
    "    \"\"\"\n",
    "    Create the discriminator model for GAIN.\n",
    "    Args:\n",
    "        dim (int): Dimensionality of the input data.\n",
    "\n",
    "    Returns:\n",
    "        model (tf.keras.Sequential): Discriminator model.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(475, activation=\"elu\", input_shape=(2*dim,))) # specifically manually configured for categorical with numerical dataset\n",
    "    model.add(Dropout(rate=0.4))\n",
    "    model.add(Dense(855, activation=\"elu\"))  \n",
    "    model.add(Dropout(rate=0.4))\n",
    "    model.add(Dense(855, activation=\"elu\"))  \n",
    "    model.add(Dropout(rate=0.4))\n",
    "    model.add(Dense(dim, activation=\"sigmoid\"))\n",
    "    return model\n",
    "\n",
    "# -------------------------------------------\n",
    "# GAIN Code Version 2 (Training + Imputation)\n",
    "# -------------------------------------------\n",
    "\n",
    "class GAIN_code_v2:\n",
    "    \"\"\"\n",
    "    GAIN (Generative Adversarial Imputation Networks) implementation for imputing missing data.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim):\n",
    "        \"\"\"\n",
    "        Initialize the GAIN model with generator and discriminator.\n",
    "        Args:\n",
    "            dim (int): Dimensionality of the input data.\n",
    "        \"\"\"\n",
    "        self.dim = dim\n",
    "        self.G = make_GAINgen(dim) # generator\n",
    "        self.D = make_GAINdisc(dim) # discriminator\n",
    "        self.Goptim = tf.keras.optimizers.Adam(0.001) # reconstruction vs adversarial loss weight # Weight for reconstruction loss\n",
    "        self.Doptim = tf.keras.optimizers.Adam(0.001) # % of known features revealed to discriminator\n",
    "        self.alpha = 50\n",
    "        self.hint_rate = 0.9\n",
    "        self.trained = False\n",
    "        self.nb_epochs = 0\n",
    "        self.Gloss1 = [] # adversarial loss\n",
    "        self.Gloss2 = [] # reconstruction loss\n",
    "        self.Dloss = []\n",
    "\n",
    "    # discriminator loss: Binary cross-entropy with hint masking\n",
    "    @staticmethod\n",
    "    def compute_D_loss(D_output, M, H):\n",
    "        \"\"\"\n",
    "        Compute the discriminator loss during training.\n",
    "        \"\"\"\n",
    "        L1 = M * tf.math.log(D_output + 1e-6)\n",
    "        L2 = (1.0 - M) * tf.math.log(1.0 - D_output + 1e-6)\n",
    "        L = - (L1 + L2) * tf.cast((H == 0.5), dtype=tf.float32)\n",
    "        nb_cells = tf.math.reduce_sum(tf.cast((H == 0.5), dtype=tf.float32))\n",
    "        return tf.math.reduce_sum(L) / nb_cells if nb_cells > 0 else 0.0\n",
    "\n",
    "    # generator loss: fool discriminator + minimize reconstruction error\n",
    "    @staticmethod\n",
    "    def compute_G_loss(G_output, D_output, X, M, H):\n",
    "        \"\"\"\n",
    "        Compute the generator loss during training.\n",
    "        \"\"\"\n",
    "        Ltemp = - ((1.0 - M) * tf.math.log(D_output + 1e-6))\n",
    "        L = Ltemp * tf.cast((H == 0.5), dtype=tf.float32)\n",
    "        nb_cells1 = tf.math.reduce_sum(tf.cast((H == 0.5), dtype=tf.float32))\n",
    "        loss1 = tf.math.reduce_sum(L) / nb_cells1 if nb_cells1 > 0 else 0.0\n",
    "        squared_err = ((X - G_output) ** 2) * M\n",
    "        nb_cells2 = tf.math.reduce_sum(M)\n",
    "        loss2 = tf.math.reduce_sum(squared_err) / nb_cells2 if nb_cells2 > 0 else 0.0\n",
    "        return loss1, loss2\n",
    "\n",
    "    # reset model for fresh training\n",
    "    def reinitialize(self):\n",
    "        \"\"\"\n",
    "        Reinitialize the weights of both generator and discriminator models.\n",
    "        \n",
    "        This is useful for resetting the models to their initial states before retraining.\n",
    "        Also clears the training history (losses and epoch counter).\n",
    "        \"\"\"\n",
    "        reset_weights(self.G)\n",
    "        reset_weights(self.D)\n",
    "        self.trained = False\n",
    "        self.nb_epochs = 0\n",
    "        self.Gloss1 = []\n",
    "        self.Gloss2 = []\n",
    "        self.Dloss = []\n",
    "\n",
    "    # single training step (compiled for speed with @tf.function)\n",
    "    @tf.function  \n",
    "    def train_step(self, batch_data):\n",
    "        \"\"\"\n",
    "        Perform a single training step for the GAIN model.\n",
    "\n",
    "        Args:\n",
    "            batch_data (tf.Tensor): Batch of data with missing values (NaNs).\n",
    "\n",
    "        Returns:\n",
    "            G_loss1 (float): Generator adversarial loss for the batch.\n",
    "            G_loss2 (float): Generator reconstruction loss for the batch.\n",
    "            D_loss (float): Discriminator loss for the batch.\n",
    "        \n",
    "        Steps:\n",
    "        1. Generate a mask matrix `M` indicating observed values (1) and missing values (0).\n",
    "        2. Replace missing values with random noise to create `X`.\n",
    "        3. Train the generator (`G`) and discriminator (`D`) using separate gradient updates.\n",
    "        4. Calculate and return the losses for monitoring.\n",
    "        \"\"\"\n",
    "        cur_batch_size = batch_data.shape[0]\n",
    "        noise = tf.random.normal([cur_batch_size, self.dim], dtype=tf.float32)\n",
    "        batch_data = tf.cast(batch_data, dtype=tf.float32)  # Ensure batch_data is float32\n",
    "        M = 1.0 - tf.cast(tf.math.is_nan(batch_data), dtype=tf.float32)  # 0=NaN, 1=obs.\n",
    "        X = tf.where(tf.math.is_nan(batch_data), noise, batch_data)\n",
    "        G_input = tf.concat((X, M), axis=1)\n",
    "    \n",
    "        with tf.GradientTape() as G_tape, tf.GradientTape() as D_tape:\n",
    "            G_output = self.G(G_input, training=True)\n",
    "            X_hat = X * M + G_output * (1.0 - M)\n",
    "            Htemp = tf.cast(drawHintMatrix(self.hint_rate, cur_batch_size, self.dim), dtype=tf.float32)\n",
    "            H = M * Htemp + 0.5 * (1.0 - Htemp)\n",
    "            D_input = tf.concat((X_hat, H), axis=1)\n",
    "            D_output = self.D(D_input, training=True)\n",
    "    \n",
    "            D_loss = self.compute_D_loss(D_output, M, H)\n",
    "            G_loss1, G_loss2 = self.compute_G_loss(G_output, D_output, X, M, H)\n",
    "            G_loss = G_loss1 + self.alpha * G_loss2\n",
    "    \n",
    "            G_gradients = G_tape.gradient(G_loss, self.G.trainable_variables)\n",
    "            D_gradients = D_tape.gradient(D_loss, self.D.trainable_variables)\n",
    "    \n",
    "            self.Goptim.apply_gradients(zip(G_gradients, self.G.trainable_variables))\n",
    "            self.Doptim.apply_gradients(zip(D_gradients, self.D.trainable_variables))\n",
    "    \n",
    "            return G_loss1, G_loss2, D_loss\n",
    "\n",
    "    # full training loop\n",
    "    def train(self, dataset, batch_size, epochs):\n",
    "        \"\"\"\n",
    "        Train the GAIN model on the given dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset (numpy array): Dataset containing missing values (NaNs).\n",
    "            batch_size (int): Number of samples per batch.\n",
    "            epochs (int): Number of epochs for training.\n",
    "\n",
    "        Process:\n",
    "        - For each epoch, the dataset is divided into batches.\n",
    "        - Each batch is passed through `train_step` to update the model weights.\n",
    "        - Losses (adversarial, reconstruction, and discriminator) are recorded for each epoch.\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            G_temp1, G_temp2, D_temp = [], [], []\n",
    "            for batch_idx in range(0, dataset.shape[0], batch_size):\n",
    "                batch_data = dataset[batch_idx:batch_idx + batch_size]\n",
    "                G_loss1, G_loss2, D_loss = self.train_step(batch_data)\n",
    "                G_temp1.append(G_loss1.numpy())\n",
    "                G_temp2.append(G_loss2.numpy())\n",
    "                D_temp.append(D_loss.numpy())\n",
    "            self.Gloss1.append(np.mean(G_temp1))\n",
    "            self.Gloss2.append(np.mean(G_temp2))\n",
    "            self.Dloss.append(np.mean(D_temp))\n",
    "\n",
    "    # imputing missing values using the trained generator\n",
    "    def impute(self, nandata):\n",
    "        \"\"\"\n",
    "        Impute missing values in the dataset using the trained GAIN model.\n",
    "\n",
    "        Args:\n",
    "            nandata (numpy array): Dataset containing missing values (NaNs).\n",
    "\n",
    "        Returns:\n",
    "            imputed_data (numpy array): Dataset with missing values replaced by imputed values.\n",
    "        \n",
    "        Process:\n",
    "        - Missing values are replaced by the generator's output.\n",
    "        - Observed values remain unchanged.\n",
    "        \"\"\"\n",
    "        noise = tf.random.normal([nandata.shape[0], self.dim])\n",
    "        M_impute = 1.0 - np.isnan(nandata)\n",
    "        X_impute = tf.where((M_impute == 0.0), noise, nandata)\n",
    "        G_input = tf.concat((X_impute, M_impute), axis=1)\n",
    "        G_output = self.G(G_input, training=False)\n",
    "        imputed_data = (X_impute * M_impute + G_output * (1.0 - M_impute)).numpy()\n",
    "        return imputed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Running Final Cross-Validation for Best-Tuned Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this section initializes fold-level execution across all imputers (AE, DAE, VAE, GAIN, etc.)\n",
    "# using 5-fold cross-validation to benchmark methods' performances.\n",
    "\n",
    "def run_scenario(config, masking=True):\n",
    "    \"\"\"\n",
    "    Executes a full pipeline across cross-validation folds to evaluate imputation methods\n",
    "    like AE, DAE, VAE, and GAIN. Applies masking, KNN preprocessing, scaling, and stores metrics.\n",
    "    \"\"\"\n",
    "    # --- 1.1 Setup directories and read input data\n",
    "    masking_dir = os.path.join(config['base_dir'], 'with_masking' if masking else 'without_masking')\n",
    "    \n",
    "    imputation_dir_knn = os.path.join(masking_dir, 'baseline_imputations')\n",
    "    os.makedirs(imputation_dir_knn, exist_ok=True)\n",
    "    \n",
    "    imputation_dir_gain=os.path.join(masking_dir, 'gain_imputation')\n",
    "    imputation_dir_gain_v2 = os.path.join(masking_dir, 'gain_imputation_v2')\n",
    "    os.makedirs(imputation_dir_gain_v2, exist_ok=True)\n",
    "    \n",
    "    input_df, initial_missingness = load_data(config)\n",
    "    rows = input_df.shape[0]\n",
    "    # n_splits=5\n",
    "    fold_gen = list(fold_generator_3_independent_indices(input_df, split_type='survey', n_splits=5)) \n",
    "    print(f\"Cross-validation is executing with masking={masking}...\")\n",
    "    imputer_name = 'KNN_initial_Imputer'\n",
    "\n",
    "    # initializing result trackers\n",
    "    # --- 1.2 Initialize metric containers\n",
    "    results_list, survey_r2_excel_list, column_metrics = [], [], []\n",
    "    fold_info_list, folds_data = [], []\n",
    "    overall_rmse_values, overall_r2_values, overall_correlation_values = [], [], []\n",
    "    all_folds_stats = {} # initializing the fold statistics as a dictionary\n",
    "\n",
    "    results_dir = config['base_dir']\n",
    "    missingness_fraction = config['missingness_fraction']\n",
    "\n",
    "    for fold, (train_index, val_index, test_index) in enumerate(fold_gen):\n",
    "        print(f\"Processing fold {fold} with masking={masking}\")\n",
    "        # --- 1.3 Load and preprocess the fold data\n",
    "        X_train, X_val, X_test = input_df.loc[train_index], input_df.loc[val_index], input_df.loc[test_index]\n",
    "        test_surveys = X_test['Meta; GEID_init'].copy()\n",
    "        X_train_ori = X_train.copy()\n",
    "        X_val_ori = X_val.copy()\n",
    "        X_test_ori = X_test.copy()\n",
    "\n",
    "        if config['process_nans'] == 'numerical_only' or config['process_nans'] == 'numerical_only_drop_20_percentage_nans':\n",
    "        # if config['process_nans'] == 'numerical_only_drop_20_percentage_nans': #'numerical_only_drop_20_percentage_nans'    \n",
    "            X_train = X_train.select_dtypes(include=[np.number])\n",
    "            X_val = X_val.select_dtypes(include=[np.number])\n",
    "            X_test= X_test.select_dtypes(include=[np.number])\n",
    "            # directory for saving imputed data\n",
    "            imputation_dir_numerical = os.path.join(masking_dir, 'numerical_only_initial_fold_imputation_step_1')\n",
    "            os.makedirs(imputation_dir_numerical, exist_ok=True)\n",
    "\n",
    "            # defining file paths for saving imputed data and\n",
    "            # apply or loading KNN-imputed data\n",
    "            train_imputed_file = os.path.join(imputation_dir_numerical, f\"train_imputed_fold_{fold}.pkl\")\n",
    "            val_imputed_file = os.path.join(imputation_dir_numerical, f\"val_imputed_fold_{fold}.pkl\")\n",
    "            test_imputed_file = os.path.join(imputation_dir_numerical, f\"test_imputed_fold_{fold}.pkl\")\n",
    "\n",
    "            if os.path.exists(train_imputed_file) and os.path.exists(val_imputed_file) and os.path.exists(test_imputed_file):\n",
    "                print(f\"Loading pre-imputed data for fold {fold}...\")\n",
    "                with open(train_imputed_file, 'rb') as f:\n",
    "                    X_train = pickle.load(f)\n",
    "                with open(val_imputed_file, 'rb') as f:\n",
    "                    X_val = pickle.load(f)\n",
    "                with open(test_imputed_file, 'rb') as f:\n",
    "                    X_test= pickle.load(f)\n",
    "            else:\n",
    "                print(f\"Applying KNN imputation for fold {fold}...\")\n",
    "\n",
    "                # applying KNN imputation\n",
    "                imputer = KNNImputer(n_neighbors=5)\n",
    "                X_train = pd.DataFrame(imputer.fit_transform(X_train), index=X_train.index, columns=X_train.columns)\n",
    "                X_val = pd.DataFrame(imputer.transform(X_val), index=X_val.index, columns=X_val.columns)\n",
    "                X_test = pd.DataFrame(imputer.transform(X_test), index=X_test.index, columns=X_test.columns)\n",
    "\n",
    "                # saving the imputed data\n",
    "                with open(train_imputed_file, 'wb') as f:\n",
    "                    pickle.dump(X_train, f)\n",
    "                with open(val_imputed_file, 'wb') as f:\n",
    "                    pickle.dump(X_val, f)\n",
    "                with open(test_imputed_file, 'wb') as f:\n",
    "                    pickle.dump(X_test, f)\n",
    "                print(f\"Imputed data saved for fold {fold}.\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Skipping KNN imputation and drop_all_nans for fold {fold}. No imputation will be performed.\")\n",
    "            X_train, X_val, X_test = X_train, X_val, X_test\n",
    "\n",
    "        # storing fold information\n",
    "        fold_info_list.append({\n",
    "            'Fold': fold,\n",
    "            'Train Shape': X_train_ori.shape,\n",
    "            'Validation Shape': X_val_ori.shape,\n",
    "            'Test Shape': X_test_ori.shape,\n",
    "            'Surveys in Train': X_train_ori['Meta; GEID_init'].unique().tolist(),\n",
    "            'Surveys in Validation': X_val_ori['Meta; GEID_init'].unique().tolist(),\n",
    "            'Surveys in Test': X_test_ori['Meta; GEID_init'].unique().tolist(),\n",
    "            'Countries in Train': X_train_ori['Meta; adm0_gaul'].unique().tolist(),\n",
    "            'Countries in Validation': X_val_ori['Meta; adm0_gaul'].unique().tolist(),\n",
    "            'Countries in Test': X_test_ori['Meta; adm0_gaul'].unique().tolist()\n",
    "        })\n",
    "\n",
    "        # --- 1.4 Scale and mask data\n",
    "        # scaling data\n",
    "        X_train_scaled, X_val_scaled, X_test_scaled = prepare_data(X_train, X_val, X_test, config)\n",
    "\n",
    "        # applying masking with mcar and mar missigness\n",
    "        X_val_with_missing, _ = apply_masking(X_val_scaled.copy(), masking, config['missingness_fraction'])\n",
    "        X_test_with_missing, _= apply_masking(X_test_scaled.copy(), masking, config['missingness_fraction'])\n",
    "\n",
    "        # --- 1.5 Initial KNN for AE/DAE/VAE input\n",
    "        # imputing data initially with knn for aes\n",
    "        X_val_imputed, X_test_imputed = initial_knn_imputed_data(X_train_scaled, X_val_with_missing, X_test_with_missing, fold, imputation_dir_knn)\n",
    "        # saving fold data\n",
    "        save_fold_data(fold, X_train, X_val, X_test, X_train_scaled, X_val_scaled, X_test_scaled, X_val_imputed, X_test_imputed, config, masking)\n",
    "\n",
    "        # --- 1.6 Prepare noisy input for DAE\n",
    "        noise_factor = 0.2\n",
    "        X_train_scaled_noisy = X_train_scaled + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_train_scaled.shape)\n",
    "        X_train_scaled_noisy = np.clip(X_train_scaled_noisy, 0., 1.)\n",
    "\n",
    "        ############################## aes' imputation and evaluation of aes' performance #########################\n",
    "\n",
    "        # clear any previous TensorFlow state\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "        # --- 1.7 Run AE, DAE, VAE\n",
    "        # running autoencoder models\n",
    "        # Auto + Manual both (recommended default)\n",
    "        layers_configurations, vae_layers_configuration = get_layer_configurations(config, input_columns_amount=95, use_automated_layers=True,\n",
    "                                                                                  combine_layers=True,  # True = auto + manual, False = only auto\n",
    "                                                                                  max_configs=10)\n",
    "\n",
    "        aes_rmse, aes_r2, aes_corr = run_models(X_train_scaled, X_val_imputed, X_test_imputed, X_val_scaled, X_train_scaled_noisy, \n",
    "                                                layers_configurations, vae_layers_configuration, optimizer_configs, \n",
    "                                                activations, X_test_scaled, test_surveys, fold, masking_dir,\n",
    "                                                rows, initial_missingness, config['missingness_fraction'], masking,\n",
    "                                                results_list, survey_r2_excel_list, column_metrics)\n",
    "        # storing metrics for ae, dae, and vae\n",
    "        overall_rmse_values.append(aes_rmse)\n",
    "        overall_r2_values.append(aes_r2)\n",
    "        overall_correlation_values.append(aes_corr)\n",
    "\n",
    "        # --- 1.8 Run baseline imputers\n",
    "        # running baseline imputation and evaluation of baseline imputers' performance\n",
    "        rmse_single, r2_single, corr_single = run_baseline_imputations(X_train_scaled, X_test_with_missing, X_val_with_missing, X_test_scaled, X_val_scaled, \n",
    "                                              test_surveys, fold, masking, rows, initial_missingness, config['missingness_fraction'], \n",
    "                                              imputation_dir_knn, results_list, survey_r2_excel_list, column_metrics)\n",
    "        \n",
    "        # storing metrics\n",
    "        overall_rmse_values.append(rmse_single)\n",
    "        overall_r2_values.append(r2_single)\n",
    "        overall_correlation_values.append(corr_single)\n",
    "\n",
    "        ###################################### imputing data using GAIN_v2 #######################################\n",
    "        # clear any previous TensorFlow state\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "        gain_model_v2 = GAIN_code_v2(dim=X_train_scaled.shape[1])\n",
    "\n",
    "        # --- 1.9 Run GAIN_v2\n",
    "        # running the GAIN method\n",
    "        gain_rmse_v2, gain_r2_v2, gain_corr_v2 = run_gain_method_v2(X_train_scaled, X_val_scaled, X_test_scaled, X_test_with_missing, \n",
    "                                                 gain_model_v2, test_surveys, fold, results_list, survey_r2_excel_list, \n",
    "                                                 column_metrics, \"GAIN_v2\", rows, initial_missingness, \n",
    "                                                 missingness_fraction, masking, imputation_dir_gain_v2)\n",
    "        \n",
    "        # storing the results\n",
    "        overall_rmse_values.append(gain_rmse_v2)\n",
    "        overall_r2_values.append(gain_r2_v2)\n",
    "        overall_correlation_values.append(gain_corr_v2)\n",
    "\n",
    "        ###################################### imputing data using GAIN_v2 #######################################\n",
    "\n",
    "        # --- 1.10 Fold statistics and ANOVA/Levene data\n",
    "        all_folds_stats = calculate_fold_statistics(X_test, all_folds_stats, fold)\n",
    "        # storing the test data for Levene and ANOVA calculations\n",
    "        folds_data.append(X_test_scaled)\n",
    "\n",
    "    # --- 1.11 Leveneâ€™s and ANOVA Tests\n",
    "    levene_df, anova_df = calculate_levene_anova_stats(folds_data)\n",
    "\n",
    "    # --- 1.12 Save results and return\n",
    "    # processing and saving fold statistics, Levene's Test and ANOVA\n",
    "    results_df, average_metrics, fold_info_df, survey_r2_df, column_metrics_df = process_and_save_fold_statistics(\n",
    "            results_dir, all_folds_stats, overall_rmse_values, overall_r2_values, overall_correlation_values, \n",
    "            results_list, fold_info_list, survey_r2_excel_list, column_metrics, masking, \n",
    "            levene_df, anova_df)\n",
    "\n",
    "    return results_df, average_metrics, fold_info_df, survey_r2_df, column_metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Execute Masked and Unmasked Scenarios and Save Final Results of Cross validation\n",
    "\n",
    "This follwing Excel file summarizes the evaluation of imputation methods with and without masking.\n",
    "It contains **five key sheets**, each serving a distinct purpose:\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Sheet: `Combined Results`\n",
    "- Contains all per-fold evaluation metrics across all models.\n",
    "- Includes both masked and unmasked missingness scenarios.\n",
    "- Metrics include:\n",
    "  - RMSE\n",
    "  - R2 Score\n",
    "  - Correlation\n",
    "  - Method name\n",
    "  - Fold number\n",
    "  - Missingness type\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Sheet: `Average Results`\n",
    "- Shows **mean and standard deviation** of RMSE, R2 and correlation.\n",
    "- Grouped by method and missingness type (with/without masking).\n",
    "- Helps in comparing overall performance of methods.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Sheet: `Fold Info`\n",
    "- Stores metadata about each fold:\n",
    "  - Fold number\n",
    "  - Train/Val/Test shapes\n",
    "  - Surveys and countries in each split\n",
    "- Useful for ensuring fair and consistent fold splits.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Sheet: `Survey R2 Scores`\n",
    "- R2 scores **per survey** for each method.\n",
    "- Helps track method performance on specific countries/surveys.\n",
    "- Important for fairness and generalization analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Sheet: `Column Metrics`\n",
    "- Evaluation metrics **per column** across folds and methods.\n",
    "- Shows how well each method imputes individual features.\n",
    "- Useful for feature-wise imputation quality insights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this part runs the complete scenario with and without masking, aggregates results,\n",
    "# and exports all evaluation metrics into an excel file.\n",
    "\n",
    "def main():\n",
    "    config = {\n",
    "        'scale_numerical_data': True,\n",
    "        'masking': True,\n",
    "        'drop_countries': True,\n",
    "        'egypt_dropping': True,\n",
    "        'process_nans': 'numerical_only_drop_20_percentage_nans',\n",
    "        'drop_columns': ['Meta; adm0_gaul', 'Meta; GEID_init', 'Meta; year'],\n",
    "        'countries_to_drop': ['Egypt', 'Comoros', 'Central African Republic'],\n",
    "        'missingness_fraction': 0.3,\n",
    "        'base_dir': '/home/myuser/prj/code/final_computation/cross_validation_github'\n",
    "    }\n",
    "\n",
    "    # setting up directories based on config\n",
    "    setup_directories(config)\n",
    "\n",
    "    # running scenario with masking\n",
    "    results_with_masking = run_scenario(config, masking=True)\n",
    "\n",
    "    # running scenario without masking\n",
    "    results_without_masking = run_scenario(config, masking=False)\n",
    "\n",
    "    # combining and save results\n",
    "    combined_results = [pd.concat([with_masking, without_masking], ignore_index=True)\n",
    "                        for with_masking, without_masking in zip(results_with_masking, results_without_masking)]\n",
    "\n",
    "    combined_results_file = os.path.join(config['base_dir'], 'combined_cross_results.xlsx')\n",
    "    with pd.ExcelWriter(combined_results_file) as writer:\n",
    "        combined_results[0].to_excel(writer, sheet_name='Combined Results', index=False)\n",
    "        combined_results[1].to_excel(writer, sheet_name='Average Results', index=False)\n",
    "        combined_results[2].to_excel(writer, sheet_name='Fold Info', index=False)\n",
    "        combined_results[3].to_excel(writer, sheet_name='Survey R2 Scores', index=False)\n",
    "        combined_results[4].to_excel(writer, sheet_name='Column Metrics', index=False)\n",
    "\n",
    "    print(f\"Combined results have been saved to {combined_results_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

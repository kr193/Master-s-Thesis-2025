{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qp1-ETbItTxO"
   },
   "source": [
    "<h1><center><u> Code Implementation </u></center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandarallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyreadstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yz4UliritTxP"
   },
   "outputs": [],
   "source": [
    "#importing Libraries\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.regularizers import L2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import Input, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from icecream import ic\n",
    "from dateutil.relativedelta import relativedelta\n",
    "# from dhs_preprocessing_functions import *\n",
    "from pandarallel import pandarallel\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# initialization\n",
    "pandarallel.initialize()\n",
    "from keras import Sequential, layers, regularizers, optimizers\n",
    "from dhs_modelling_functions import final_ds_droping_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PGRLvmmdDrSH"
   },
   "outputs": [],
   "source": [
    "#setting up random seeds for reproducibility\n",
    "tf.random.set_seed(6688)\n",
    "random.seed(6688)\n",
    "np.random.seed(6688)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4D_reWvTDuEt"
   },
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu,True)\n",
    "    except RuntimeError as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EllvAwEotTxQ"
   },
   "source": [
    "<h1><center> <div class=\"alert alert-danger\"> Regular Autoencoder </div> </center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vDmHBTYTtTxQ"
   },
   "outputs": [],
   "source": [
    "#importing data\n",
    "input_df = pd.read_csv(\"5_grouped_df_V3_HR_adm2_gaul_joined_with_ipc_all.csv\")\n",
    "input_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listing all the column names from the dataframe\n",
    "#all_column_names = input_df.columns.tolist()\n",
    "#all_column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting the number of null or missing values in each column\n",
    "#pd.set_option('display.max_rows', 1620)\n",
    "#null_counts = input_df.isnull().sum()\n",
    "#print(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_df = final_ds_droping_cols(input_df, drop_meta=True, drop_food_help=True, drop_perc=25, \n",
    "                           drop_data_sets=['DHS Cat', 'Meta one-hot encoding', 'Meta frequency encoding'], \n",
    "                           numerical_data=['std'], retain_year=True,\n",
    "                 retain_adm=False, retain_month=False, drop_highly_correlated_cols=False, drop_region=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping columns starting with 'Meta' and 'index'\n",
    "#numeric_df= numeric_df.loc[:, ~numeric_df.columns.str.startswith(('Meta', 'index'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_df = final_ds_droping_cols(input_df.copy(), drop_meta=True, drop_food_help=True, drop_perc=25, \n",
    "                           drop_data_sets=['Meta', 'DHS Num', 'Meta one-hot encoding', 'Meta frequency encoding'], \n",
    "                           numerical_data=['std'],\n",
    "                 retain_adm=False, retain_month=False, drop_highly_correlated_cols=False, drop_region=True, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finalized 'std' only as mean, median, skewness etc are not well imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = final_ds_droping_cols(input_df.copy(), drop_meta=True, drop_food_help=True, drop_perc=30, \n",
    "                           drop_data_sets=['Meta', 'Meta one-hot encoding', 'Meta frequency encoding'], \n",
    "                           numerical_data=['std'],\n",
    "                 retain_adm=False, retain_month=False, drop_highly_correlated_cols=False, drop_region=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.loc[:, ~combined_df.columns.str.startswith(('index'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing by multiplication as ae can't capture raw datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiplying all columns starting with 'DHS Num' by 100\n",
    "dhs_num_cols = combined_df.filter(regex='^DHS Num').columns\n",
    "combined_df[dhs_num_cols] = combined_df[dhs_num_cols] * 100\n",
    "\n",
    "# multiplying all columns starting with 'FS' by 100\n",
    "fs_cols = combined_df.filter(regex='^FS').columns\n",
    "combined_df[fs_cols] = combined_df[fs_cols] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiplying all columns starting with 'DHS Cat' by 1000\n",
    "dhs_cat_cols = combined_df.filter(regex='^DHS Cat').columns\n",
    "combined_df[dhs_cat_cols] = combined_df[dhs_cat_cols] * 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# temporarily filling up missing spaces with median and mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in combined_df.columns:\n",
    "    if col.startswith('DHS Num') or col.startswith('FS'):\n",
    "        median_value = combined_df[col].median()\n",
    "        combined_df[col].fillna(median_value, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in combined_df.columns:\n",
    "    if 'DHS Cat' in col:  \n",
    "        mode_value = combined_df[col].mode()[0]\n",
    "        if pd.notna(mode_value): \n",
    "            combined_df[col].fillna(mode_value, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "null_counts = combined_df.isnull().sum()\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset columns' are highly skewed with imbalanced datapoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - applied smote function to oversample, but could not get balanced datapoints out of DHS Cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in combined_df.columns:\n",
    "    #plot histogram for the current column\n",
    "    plt.hist(combined_df[col], bins=30, edgecolor='black')\n",
    "    plt.title(f'Histogram of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in combined_df.columns:\n",
    "    print(col, len(combined_df[col].dropna())/len(combined_df) * 100) #the proportion of non-missing values in the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "null_counts = combined_df.isnull().sum()\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=combined_df.copy()\n",
    "#splitting the data into train, test, and validation sets\n",
    "train1, test1 = train_test_split(df1, test_size=0.2, random_state=42)\n",
    "train1, val1= train_test_split(train1, test_size=0.2, random_state=42)\n",
    "actual_ae1=test1.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# artificial missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the proportion of rows that will have missing values\n",
    "missing_row_proportion = 0.3  # 30% of the rows\n",
    "\n",
    "# the range of the number of columns to have missing values in each row\n",
    "min_missing_columns = 1  # minimum number of columns with missing values\n",
    "max_missing_columns = int(0.3 * 18)  # maximum number of columns with missing values (e.g., 20% of 18)\n",
    "\n",
    "# selecting the rows that will have missing values\n",
    "n_rows_with_missing = int(test1.shape[0] * missing_row_proportion)\n",
    "rows_to_have_missing = np.random.choice(test1.index, size=n_rows_with_missing, replace=False)\n",
    "\n",
    "for i in rows_to_have_missing:\n",
    "    # a random number of columns for missing values for each row\n",
    "    n_missing_columns = np.random.randint(min_missing_columns, max_missing_columns)\n",
    "    cols_to_have_missing = np.random.choice([col for col in test1.columns if col not in ['Meta; year', 'index']], size=n_missing_columns, replace=False)\n",
    "    test1.loc[i, cols_to_have_missing] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "null_counts = test1.isnull().sum()\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# filling up missing values temporarily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in test1.columns:\n",
    "    if col.startswith('DHS Num') or col.startswith('FS'):\n",
    "        median_value = test1[col].median()\n",
    "        test1[col].fillna(median_value, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in test1.columns:\n",
    "    if col.startswith('DHS Cat'):\n",
    "        median_value = test1[col].median()\n",
    "        test1[col].fillna(median_value, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "null_counts = test1.isnull().sum()\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simple autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = train1.shape[1]\n",
    "\n",
    "# increased bottleneck size\n",
    "bottleneck_size = 32\n",
    "# larger and more complex model\n",
    "final_ae = Sequential()\n",
    "final_ae.add(layers.Dense(128, activation='relu', input_dim=input_dim, kernel_initializer='he_uniform', activity_regularizer=regularizers.l2(0.001)))\n",
    "#final_ae.add(layers.Dense(256, activation='relu', kernel_initializer='he_uniform', activity_regularizer=regularizers.l2(0.01)))\n",
    "#final_ae.add(layers.Dense(128, activation='relu', kernel_initializer='he_uniform', activity_regularizer=regularizers.l2(0.001)))\n",
    "final_ae.add(layers.Dense(64, activation='relu', kernel_initializer='he_uniform', activity_regularizer=regularizers.l2(0.001)))\n",
    "# bottleneck layer\n",
    "final_ae.add(layers.Dense(bottleneck_size, activation='relu', name='bottleneck'))\n",
    "# decoder part mirroring the encoder\n",
    "final_ae.add(layers.Dense(64, activation='relu', kernel_initializer='he_uniform'))\n",
    "final_ae.add(layers.Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "#final_ae.add(layers.Dense(256, activation='relu', kernel_initializer='he_uniform'))\n",
    "#final_ae.add(layers.Dense(512, activation='relu', kernel_initializer='he_uniform'))\n",
    "final_ae.add(layers.Dense(input_dim, activation='relu'))  # Adjust the activation function if needed\n",
    "# customized RMSE function\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
    "\n",
    "# compiling the model\n",
    "final_ae.compile(optimizer=optimizers.Adam(learning_rate=0.001), loss='mse', metrics=['accuracy', 'mean_absolute_error', root_mean_squared_error])\n",
    "# model summary\n",
    "final_ae.summary()\n",
    "# fitting the model\n",
    "start_time = time.time()\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n",
    "history = final_ae.fit(train1, train1, epochs=500, batch_size=1024, shuffle=True, callbacks=[es], validation_split=0.2, verbose=1)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Training time: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputing the missing values in the test set\n",
    "start_time_imp1 = time.time()\n",
    "imputed_test_f1 = final_ae.predict(test1)\n",
    "end_time_imp1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_test_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating mse for each column\n",
    "mse_ae = ((actual_ae1 - imputed_test_f1) ** 2).mean()\n",
    "#calculating rmse for each column\n",
    "rmse_ae = np.sqrt(mse_ae)\n",
    "#displaying rmse and mse values\n",
    "#print(\"MSE:\", mse_ae)\n",
    "#print(\"RMSE:\", rmse_ae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy for simple autoencoder\n",
    "accuracy_simple = np.mean(actual_ae1 == imputed_test_f1) * 100\n",
    "#accuracy for simple autoencoder\n",
    "print(\"Accuracy for Simple Autoencoder:\", accuracy_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting training & validation loss values\n",
    "plt.plot(history.history['loss'], color='red')\n",
    "plt.plot(history.history['val_loss'], color='green')\n",
    "plt.title('MSE for AE')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "#plotting training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'], color='red')\n",
    "plt.plot(history.history['val_accuracy'], color='green')\n",
    "plt.title('Accuracy for AE')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "#plotting training & validation MAE values\n",
    "plt.plot(history.history['mean_absolute_error'], label='Training MAE', color='red')\n",
    "plt.plot(history.history['val_mean_absolute_error'], label='Validation MAE', color='green')\n",
    "plt.title('MAE for AE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual vs Imputed Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting actual vs imputed values\n",
    "j=0\n",
    "for col in actual_ae1.columns:\n",
    "    plt.scatter([i for i in range(2000)], np.array(actual_ae1[col])[1000:3000], color=\"red\")\n",
    "    plt.scatter([i for i in range(2000)], [i[j]*1.5 if j==2 else i[j] for i in imputed_test_f1[1000:3000]], color=\"green\")\n",
    "    plt.legend([\"Actual Values\", \"Imputed Values\"])\n",
    "    plt.title(f\"Actual vs Imputed for column {col}\")\n",
    "    plt.show()\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# comments:\n",
    "## For most of the columns, one particular value exists with highest frequency. So, for this highly imbalanced dataset,  missing values for those are not imputed well. For this issue, dataset has to be balanced well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nlKRycjXtTxR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3dPN590tTxS"
   },
   "source": [
    "<h1><center> <div class=\"alert alert-success\"> De-noising Autoencoder </div> </center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45P0XaxP5nB6"
   },
   "source": [
    "# De-noising Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade pydot\n",
    "#!pip uninstall pydot -y\n",
    "#!pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I7V6zg0MtTxd",
    "outputId": "b94ee6e5-3101-4f5e-931e-f935e78dc28e"
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing Libraries\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.regularizers import L2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import Input, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from icecream import ic\n",
    "from dateutil.relativedelta import relativedelta\n",
    "# from dhs_preprocessing_functions import *\n",
    "from pandarallel import pandarallel\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# initialization\n",
    "pandarallel.initialize()\n",
    "from keras import Sequential, layers, regularizers, optimizers\n",
    "from dhs_modelling_functions import final_ds_droping_cols\n",
    "\n",
    "#setting up random seeds for reproducibility\n",
    "tf.random.set_seed(6688)\n",
    "random.seed(6688)\n",
    "np.random.seed(6688)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu,True)\n",
    "    except RuntimeError as e:\n",
    "        raise e\n",
    "\n",
    "#importing data\n",
    "input_df2 = pd.read_csv(\"5_grouped_df_V3_HR_adm2_gaul_joined_with_ipc_all.csv\")\n",
    "input_df2.head()\n",
    "\n",
    "combined_df2 = final_ds_droping_cols(input_df2.copy(), drop_meta=True, drop_food_help=True, drop_perc=30, \n",
    "                           drop_data_sets=['Meta', 'Meta one-hot encoding', 'Meta frequency encoding'], \n",
    "                           numerical_data=['std'],\n",
    "                 retain_adm=False, retain_month=False, drop_highly_correlated_cols=False, drop_region=True, verbose=1)\n",
    "\n",
    "combined_df2 = combined_df2.loc[:, ~combined_df2.columns.str.startswith(('index'))]\n",
    "\n",
    "# multiplying all columns starting with 'DHS Num' by 100\n",
    "dhs_num_cols = combined_df2.filter(regex='^DHS Num').columns\n",
    "combined_df2[dhs_num_cols] = combined_df2[dhs_num_cols] * 100\n",
    "\n",
    "# multiplying all columns starting with 'FS' by 100\n",
    "fs_cols = combined_df2.filter(regex='^FS').columns\n",
    "combined_df2[fs_cols] = combined_df2[fs_cols] * 100\n",
    "\n",
    "# multiplying all columns starting with 'DHS Cat' by 1000\n",
    "dhs_cat_cols = combined_df2.filter(regex='^DHS Cat').columns\n",
    "combined_df2[dhs_cat_cols] = combined_df2[dhs_cat_cols] * 1000\n",
    "\n",
    "for col in combined_df2.columns:\n",
    "    if col.startswith('DHS Num') or col.startswith('FS'):\n",
    "        median_value = combined_df2[col].median()\n",
    "        combined_df2[col].fillna(median_value, inplace=True)\n",
    "\n",
    "for col in combined_df2.columns:\n",
    "    if 'DHS Cat' in col:  \n",
    "        mode_value = combined_df2[col].mode()[0]\n",
    "        if pd.notna(mode_value): \n",
    "            combined_df2[col].fillna(mode_value, inplace=True)\n",
    "\n",
    "df2=combined_df2.copy()\n",
    "#splitting the data into train, test, and validation sets\n",
    "train2, test2 = train_test_split(df2, test_size=0.2, random_state=42)\n",
    "train2, val2= train_test_split(train2, test_size=0.2, random_state=42)\n",
    "actual_dae2=test2.copy()\n",
    "\n",
    "# the proportion of rows that will have missing values\n",
    "missing_row_proportion = 0.3  # 30% of the rows\n",
    "\n",
    "# the range of the number of columns to have missing values in each row\n",
    "min_missing_columns = 1  # minimum number of columns with missing values\n",
    "max_missing_columns = int(0.3 * 18)  # maximum number of columns with missing values (e.g., 20% of 18)\n",
    "\n",
    "# selecting the rows that will have missing values\n",
    "n_rows_with_missing = int(test2.shape[0] * missing_row_proportion)\n",
    "rows_to_have_missing = np.random.choice(test2.index, size=n_rows_with_missing, replace=False)\n",
    "\n",
    "for i in rows_to_have_missing:\n",
    "    # a random number of columns for missing values for each row\n",
    "    n_missing_columns = np.random.randint(min_missing_columns, max_missing_columns)\n",
    "    cols_to_have_missing = np.random.choice([col for col in test2.columns if col not in ['Meta; year', 'index']], size=n_missing_columns, replace=False)\n",
    "    test2.loc[i, cols_to_have_missing] = np.nan\n",
    "\n",
    "for col in test2.columns:\n",
    "    if col.startswith('DHS Num') or col.startswith('FS'):\n",
    "        median_value = test2[col].median()\n",
    "        test2[col].fillna(median_value, inplace=True)\n",
    "\n",
    "for col in test2.columns:\n",
    "    if col.startswith('DHS Cat'):\n",
    "        median_value = test2[col].median()\n",
    "        test2[col].fillna(median_value, inplace=True)\n",
    "\n",
    "#building and training the dae autoencoder\n",
    "#different combination of layer sizes have been compared and stored and finally (64,10) architecture has been shown in ipynb file\n",
    "input_dim = train2.shape[1]\n",
    "final_dae = keras.Sequential()\n",
    "final_dae.add(layers.Dense(128,activation='relu', input_dim=input_dim, kernel_initializer='he_uniform',activity_regularizer=L2(0.001)))\n",
    "#final_dae.add(layers.Dense(128,activation='relu', kernel_initializer='he_uniform',activity_regularizer=L2(0.001)))\n",
    "#final_dae.add(layers.Dense(64,activation='relu', kernel_initializer='he_uniform',activity_regularizer=L2(0.001)))\n",
    "final_dae.add(layers.Dense(64,activation='relu', kernel_initializer='he_uniform',activity_regularizer=L2(0.001)))\n",
    "final_dae.add(layers.Dense(32,activation='relu', kernel_initializer='he_uniform', name='bottleneck'))\n",
    "final_dae.add(layers.Dense(64,activation='relu', kernel_initializer='he_uniform'))\n",
    "final_dae.add(layers.Dense(128,activation='relu', kernel_initializer='he_uniform'))\n",
    "#final_dae.add(layers.Dense(128,activation='relu', kernel_initializer='he_uniform'))\n",
    "#final_dae.add(layers.Dense(256,activation='relu', kernel_initializer='he_uniform'))\n",
    "final_dae.add(layers.Dense(input_dim,activation='relu', kernel_initializer='he_uniform'))\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "final_dae.compile(optimizer=optimizer, loss='mse', metrics=['accuracy', 'mean_absolute_error']) #or 'mae'\n",
    "final_dae.summary()\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n",
    "#tf.keras.utils.plot_model(final_dae, show_shapes=True,rankdir='LR')\n",
    "#es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=50)\n",
    "\n",
    "#the input data is first corrupted by adding random noise to it\n",
    "def make_noisy(np_data, noise_factor=0.1):\n",
    "    noise = np.random.normal(loc=0.0, scale=noise_factor, size=np_data.shape)\n",
    "    np_ret = np_data + noise\n",
    "    return np_ret\n",
    "\n",
    "#the corrupted input data\n",
    "noise_X = make_noisy(train2.values)\n",
    "import time\n",
    "start_time2 = time.time()        \n",
    "#training the autoencoder model on the training set and validate on the validation set\n",
    "his_dae2 = final_dae.fit(noise_X, train2.values, epochs=500, batch_size=1024, shuffle=True, callbacks=[es], validation_split=0.2, verbose=1)\n",
    "end_time2 = time.time()\n",
    "\n",
    "start_time_imp2 = time.time()\n",
    "#imputing missing values in the test set\n",
    "imputed_test2 = final_dae.predict(test2)\n",
    "end_time_imp2 = time.time()\n",
    "\n",
    "\n",
    "#imputing the missing values in the test set\n",
    "start_time_imp1 = time.time()\n",
    "imputed_test2 = final_dae.predict(test2)\n",
    "end_time_imp1 = time.time()\n",
    "\n",
    "#calculating mse for each column\n",
    "mse_dae = ((actual_dae2 - imputed_test2) ** 2).mean()\n",
    "#calculating rmse for each column\n",
    "rmse_dae = np.sqrt(mse_dae)\n",
    "#displaying rmse and mse values\n",
    "#print(\"MSE:\", mse_dae)\n",
    "#print(\"RMSE:\", rmse_dae)\n",
    "\n",
    "#accuracy for simple autoencoder\n",
    "accuracy_dae = np.mean(actual_dae2 == imputed_test2) * 100\n",
    "#accuracy for simple autoencoder\n",
    "print(\"Accuracy for Simple Autoencoder:\", accuracy_dae)\n",
    "\n",
    "#plotting training & validation loss values\n",
    "plt.plot(his_dae2.history['loss'], color='red')\n",
    "plt.plot(his_dae2.history['val_loss'], color='green')\n",
    "plt.title('MSE for AE')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "#plotting training & validation accuracy values\n",
    "plt.plot(his_dae2.history['accuracy'], color='red')\n",
    "plt.plot(his_dae2.history['val_accuracy'], color='green')\n",
    "plt.title('Accuracy for AE')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "#plotting training & validation MAE values\n",
    "plt.plot(his_dae2.history['mean_absolute_error'], label='Training MAE', color='red')\n",
    "plt.plot(his_dae2.history['val_mean_absolute_error'], label='Validation MAE', color='green')\n",
    "plt.title('MAE for AE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "#plotting actual vs imputed values\n",
    "j=0\n",
    "for col in actual_dae2.columns:\n",
    "    plt.scatter([i for i in range(2000)], np.array(actual_dae2[col])[1000:3000], color=\"red\")\n",
    "    plt.scatter([i for i in range(2000)], [i[j]*1.5 if j==2 else i[j] for i in imputed_test2[1000:3000]], color=\"green\")\n",
    "    plt.legend([\"Actual Values\", \"Imputed Values\"])\n",
    "    plt.title(f\"Actual vs Imputed for column {col}\")\n",
    "    plt.show()\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPFb2KnQtTxd"
   },
   "source": [
    "\n",
    "<h1><center> <div class=\"alert alert-warning\"> Variational Autoencoder </div> </center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GiKkKgLO55Ls"
   },
   "source": [
    "# Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing Libraries\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.regularizers import L2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import Input, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from icecream import ic\n",
    "from dateutil.relativedelta import relativedelta\n",
    "# from dhs_preprocessing_functions import *\n",
    "from pandarallel import pandarallel\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# initialization\n",
    "pandarallel.initialize()\n",
    "from keras import Sequential, layers, regularizers, optimizers\n",
    "from dhs_modelling_functions import final_ds_droping_cols\n",
    "\n",
    "#setting up random seeds for reproducibility\n",
    "tf.random.set_seed(6688)\n",
    "random.seed(6688)\n",
    "np.random.seed(6688)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu,True)\n",
    "    except RuntimeError as e:\n",
    "        raise e\n",
    "\n",
    "#importing data\n",
    "input_df3 = pd.read_csv(\"5_grouped_df_V3_HR_adm2_gaul_joined_with_ipc_all.csv\")\n",
    "input_df3.head()\n",
    "\n",
    "combined_df3 = final_ds_droping_cols(input_df3.copy(), drop_meta=True, drop_food_help=True, drop_perc=30, \n",
    "                           drop_data_sets=['Meta', 'Meta one-hot encoding', 'Meta frequency encoding'], \n",
    "                           numerical_data=['std'],\n",
    "                 retain_adm=False, retain_month=False, drop_highly_correlated_cols=False, drop_region=True, verbose=1)\n",
    "\n",
    "combined_df3 = combined_df3.loc[:, ~combined_df3.columns.str.startswith(('index'))]\n",
    "\n",
    "# multiplying all columns starting with 'DHS Num' by 100\n",
    "dhs_num_cols = combined_df3.filter(regex='^DHS Num').columns\n",
    "combined_df3[dhs_num_cols] = combined_df3[dhs_num_cols] * 100\n",
    "\n",
    "# multiplying all columns starting with 'FS' by 100\n",
    "fs_cols = combined_df3.filter(regex='^FS').columns\n",
    "combined_df3[fs_cols] = combined_df3[fs_cols] * 100\n",
    "\n",
    "# multiplying all columns starting with 'DHS Cat' by 1000\n",
    "dhs_cat_cols = combined_df3.filter(regex='^DHS Cat').columns\n",
    "combined_df3[dhs_cat_cols] = combined_df3[dhs_cat_cols] * 1000\n",
    "\n",
    "for col in combined_df3.columns:\n",
    "    if col.startswith('DHS Num') or col.startswith('FS'):\n",
    "        median_value = combined_df3[col].median()\n",
    "        combined_df3[col].fillna(median_value, inplace=True)\n",
    "\n",
    "for col in combined_df3.columns:\n",
    "    if 'DHS Cat' in col:  \n",
    "        mode_value = combined_df3[col].mode()[0]\n",
    "        if pd.notna(mode_value): \n",
    "            combined_df3[col].fillna(mode_value, inplace=True)\n",
    "\n",
    "df3=combined_df3.copy()\n",
    "#splitting the data into train, test, and validation sets\n",
    "train3, test3 = train_test_split(df3, test_size=0.2, random_state=42)\n",
    "train3, val3= train_test_split(train3, test_size=0.2, random_state=42)\n",
    "actual_vae3=test3.copy()\n",
    "\n",
    "# the proportion of rows that will have missing values\n",
    "missing_row_proportion = 0.3  # 30% of the rows\n",
    "\n",
    "# the range of the number of columns to have missing values in each row\n",
    "min_missing_columns = 1  # minimum number of columns with missing values\n",
    "max_missing_columns = int(0.3 * 18)  # maximum number of columns with missing values (e.g., 20% of 18)\n",
    "\n",
    "# selecting the rows that will have missing values\n",
    "n_rows_with_missing = int(test3.shape[0] * missing_row_proportion)\n",
    "rows_to_have_missing = np.random.choice(test3.index, size=n_rows_with_missing, replace=False)\n",
    "\n",
    "for i in rows_to_have_missing:\n",
    "    # a random number of columns for missing values for each row\n",
    "    n_missing_columns = np.random.randint(min_missing_columns, max_missing_columns)\n",
    "    cols_to_have_missing = np.random.choice([col for col in test3.columns if col not in ['Meta; year', 'index']], size=n_missing_columns, replace=False)\n",
    "    test3.loc[i, cols_to_have_missing] = np.nan\n",
    "\n",
    "for col in test3.columns:\n",
    "    if col.startswith('DHS Num') or col.startswith('FS'):\n",
    "        median_value = test3[col].median()\n",
    "        test3[col].fillna(median_value, inplace=True)\n",
    "\n",
    "for col in test3.columns:\n",
    "    if col.startswith('DHS Cat'):\n",
    "        median_value = test3[col].median()\n",
    "        test3[col].fillna(median_value, inplace=True)\n",
    "\n",
    "#different combination of layer sizes have been compared and stored and finally (64,10) architecture has been shown in ipynb file\n",
    "#constructing the vae model\n",
    "#defining the encoder network\n",
    "from tensorflow.keras import regularizers, initializers\n",
    "\n",
    "def make_encoder_model(input_shape, latent_dim):\n",
    "    model0 = keras.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        #layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.1), kernel_initializer=initializers.he_uniform()),\n",
    "        #layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.1), kernel_initializer=initializers.he_uniform()),\n",
    "        layers.Dense(128, activation='relu',  kernel_regularizer=regularizers.l2(0.01), kernel_initializer=initializers.he_uniform()),\n",
    "        layers.Dense(64, activation='relu',  kernel_regularizer=regularizers.l2(0.01), kernel_initializer=initializers.he_uniform()),\n",
    "        layers.Dense(latent_dim, activation='relu',kernel_regularizer=regularizers.l2(0.01), kernel_initializer=initializers.he_uniform()),\n",
    "    ])\n",
    "    return model0\n",
    "\n",
    "#defining the decoder network\n",
    "def make_decoder_model(latent_dim, output_shape):\n",
    "    model0 = keras.Sequential([\n",
    "        layers.Input(shape=(latent_dim,)),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        #layers.Dense(128, activation='relu'),\n",
    "        #layers.Dense(256, activation='relu'),\n",
    "        layers.Dense(output_shape, activation='relu'),\n",
    "    ])\n",
    "    return model0\n",
    "\n",
    "#vae_final\n",
    "def make_vae_model(input_shape, latent_dim):\n",
    "    \n",
    "    encoder = make_encoder_model(input_shape, latent_dim)\n",
    "    decoder = make_decoder_model(latent_dim, input_shape)\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    z = encoder(inputs)\n",
    "    reconstruction = decoder(z)\n",
    "    vae_final = keras.Model(inputs, reconstruction)\n",
    "    reconstruction_loss = tf.keras.losses.MeanSquaredError()\n",
    "    kl_loss = -0.5 * tf.reduce_mean(1 + tf.math.log(tf.square(tf.math.reduce_std(z))) - tf.square(tf.math.reduce_mean(z)) - tf.square(tf.math.reduce_std(z)))\n",
    "    \n",
    "    #total loss adding both\n",
    "    vae_loss = reconstruction_loss(inputs, reconstruction) + kl_loss\n",
    "    vae_final.add_loss(vae_loss)\n",
    "    return vae_final\n",
    "\n",
    "input_shape = 134\n",
    "output_shape =134\n",
    "latent_dim = 32\n",
    "    \n",
    "vae_final = make_vae_model(input_shape, latent_dim)\n",
    "vae_final.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=['accuracy', 'mean_absolute_error'])\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n",
    "        \n",
    "#training the VAE\n",
    "start_time3 = time.time()\n",
    "vae_final1 = vae_final.fit(train3, train3, epochs=500, batch_size=1024, shuffle= True, callbacks=[es], validation_split=0.2, verbose=1)\n",
    "end_time3 = time.time()\n",
    "\n",
    "#imputing missing values in the test set\n",
    "start_time_imp3 = time.time()\n",
    "imputed_test3 = vae_final.predict(test3)\n",
    "end_time_imp3 = time.time()\n",
    "\n",
    "#calculating mse for each column\n",
    "mse_vae = ((actual_vae3 - imputed_test3) ** 2).mean()\n",
    "#calculating rmse for each column\n",
    "rmse_vae = np.sqrt(mse_vae)\n",
    "#displaying rmse and mse values\n",
    "#print(\"MSE:\", mse_dae)\n",
    "#print(\"RMSE:\", rmse_dae)\n",
    "\n",
    "#accuracy for simple autoencoder\n",
    "accuracy_vae = np.mean(actual_vae3 == imputed_test3) * 100\n",
    "#accuracy for simple autoencoder\n",
    "print(\"Accuracy for Simple Autoencoder:\", accuracy_vae)\n",
    "\n",
    "#plotting training & validation loss values\n",
    "plt.plot(vae_final1.history['loss'], color='red')\n",
    "plt.plot(vae_final1.history['val_loss'], color='green')\n",
    "plt.title('MSE for AE')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "#plotting training & validation accuracy values\n",
    "plt.plot(vae_final1.history['accuracy'], color='red')\n",
    "plt.plot(vae_final1.history['val_accuracy'], color='green')\n",
    "plt.title('Accuracy for AE')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "#plotting training & validation MAE values\n",
    "plt.plot(vae_final1.history['mean_absolute_error'], label='Training MAE', color='red')\n",
    "plt.plot(vae_final1.history['val_mean_absolute_error'], label='Validation MAE', color='green')\n",
    "plt.title('MAE for AE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "#plotting actual vs imputed values\n",
    "j=0\n",
    "for col in actual_vae3.columns:\n",
    "    plt.scatter([i for i in range(2000)], np.array(actual_vae3[col])[1000:3000], color=\"red\")\n",
    "    plt.scatter([i for i in range(2000)], [i[j]*1.5 if j==2 else i[j] for i in imputed_test3[1000:3000]], color=\"green\")\n",
    "    plt.legend([\"Actual Values\", \"Imputed Values\"])\n",
    "    plt.title(f\"Actual vs Imputed for column {col}\")\n",
    "    plt.show()\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCB01SyIySr3"
   },
   "source": [
    "# comparison between AE, DAE and VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xvFH3uC9ySty"
   },
   "outputs": [],
   "source": [
    "# plotting validation MSE for AE, DAE, VAE\n",
    "plt.figure(figsize=(12, 4))\n",
    "metrics = ['val_loss', 'val_accuracy', 'val_mean_absolute_error']\n",
    "titles = ['MSE for Validation Set', 'Accuracy for Validation Set', 'MAE for Validation Set']\n",
    "y_labels = ['MSE', 'Accuracy', 'MAE']\n",
    "history_objects = [history, his_dae2, vae_final1]\n",
    "colors = ['blue', 'red', 'green']\n",
    "names = ['AE', 'DAE', 'VAE']\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    for histo, color, name in zip(history_objects, colors, names):\n",
    "        plt.plot(histo.history[metric], label=f'{name} Validation', color=color)\n",
    "    plt.title(titles[i])\n",
    "    plt.ylabel(y_labels[i])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tFd81Y3pySwA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BetSpMMhySx-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H-P-vH7XyS1M"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sTJA1O-CyS4z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bFgFCv2-yS6G"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4BkKiMOUtTxg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4WfFX3LWwUqc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "22WDv1IDwUtM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5inezS83wUvx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9WZD-paYwUy8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MsfXgDnWwU1w"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mZ_iyGimwU44"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Hj9K_FNwU7g"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TX1zmKUiwU_C"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MYOvUP8EwVCT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MhPzE2ZswVFB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ukMFNYU-wVHk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cgli6iFAwVKp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Os0aLk_AwVb3"
   },
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

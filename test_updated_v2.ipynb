{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qp1-ETbItTxO"
   },
   "source": [
    "<h1><center><u> Code Implementation </u></center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yz4UliritTxP"
   },
   "outputs": [],
   "source": [
    "#importing Libraries\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.regularizers import L2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import Input, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from icecream import ic\n",
    "from sklearn.metrics import accuracy_score\n",
    "from dateutil.relativedelta import relativedelta\n",
    "#from sklearn.impute import IterativeImputer, SimpleImputer, KNNImputer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.linear_model import BayesianRidge, LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.impute import SimpleImputer\n",
    "from tensorflow.keras import layers, models, optimizers, regularizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "# from dhs_preprocessing_functions import *\n",
    "from pandarallel import pandarallel\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# initialization\n",
    "pandarallel.initialize()\n",
    "from keras import Sequential, layers, regularizers, optimizers\n",
    "from dhs_modelling_functions_new import final_ds_droping_cols, fold_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PGRLvmmdDrSH"
   },
   "outputs": [],
   "source": [
    "#setting up random seeds for reproducibility\n",
    "tf.random.set_seed(6688)\n",
    "random.seed(6688)\n",
    "np.random.seed(6688)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4D_reWvTDuEt"
   },
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu,True)\n",
    "    except RuntimeError as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** given csv (5_grouped_df_V3_HR_adm2_gaul_joined_with_ipc_all.csv) has no 'GEID; init' column which was creating error with fold_generator function.\n",
    "so, took reference from https://github.com/gheisenberg/FoodSecurity/tree/main/DHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"C:/Users/rubai/remote_server_sven_dataset_2024/\"\n",
    "# min_version = 3\n",
    "overwrite_pqt = True\n",
    "dataset_type = 'HR' \n",
    "urban_rural_all_mode = 'all'\n",
    "group_by_col = 'adm2_gaul'\n",
    "out_f = f\"{input_dir}5_grouped_df_V3_{dataset_type}_{group_by_col}_joined_with_ipc_{urban_rural_all_mode}.pkl\"\n",
    "\n",
    "# Scale options\n",
    "scale_numerical_data = False\n",
    "scale_all_data = True\n",
    "leave_out_encodings = True\n",
    "zero_one_scale_categorical = False\n",
    "scale_labels = True\n",
    "\n",
    "# More Options\n",
    "drop_agriculture = False\n",
    "\n",
    "in_f = f\"{input_dir}5_grouped_df_V3_{dataset_type}_{group_by_col}_joined_with_ipc_{urban_rural_all_mode}.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df_0 = pd.read_pickle(in_f)\n",
    "#for c in input_df.columns:\n",
    "    #if 'year' in c or 'Year' in c:\n",
    "        #print(c)\n",
    "\n",
    "# if urban_rural_all_mode == 'U':\n",
    "#     drop_agriculture = True\n",
    "df = final_ds_droping_cols(input_df_0, drop_meta=True, drop_food_help=True, drop_perc=40,\n",
    "                           retain_month=False, drop_highly_correlated_cols=False, drop_region=True, \n",
    "                 drop_data_sets=['Meta one-hot encoding', 'Meta frequency encoding'], \n",
    "                 use_NAN_amount_and_replace_NANs_in_categorical=False, drop_agricultural_cols=drop_agriculture, \n",
    "                 drop_below_version=False, numerical_data=['mean'], retain_adm=False, \n",
    "                 retain_GEID_init=False, verbose=3)\n",
    "#for col in df.columns:\n",
    "    #print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> <div class=\"alert alert-danger\"> Regular Autoencoder </div> </center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# controlled cross validation using fold_generator for simple autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-by-Step Process \n",
    "\n",
    "1. **Step 1:**  **Getting the Data**\n",
    "   - data has been loaded from <a href=\"https://github.com/gheisenberg/FoodSecurity/tree/main/DHS\" target=\"_blank\">Dataset</a>\n",
    "   - dataset has been tuned according to our needs using final_ds_droping_cols function\n",
    "   - temporarily dataset has been imputed using simple imputer (mean, median)\n",
    "   - in next step, more imputers will be applied \n",
    "\n",
    "2. **Step 2:**  **Applying controlled cross validation**\n",
    "   - used fold_generator function during cross validation\n",
    "   - only simple autoencoder is analyzed on test_updated_v2\n",
    "\n",
    "3. **Step 3:**  **Finalized simple AE training and evaluation**\n",
    "   - during AE training, have not used fold_generator function\n",
    "   - rmse, r2 score, correlation metrics are evaluated on unseen test data\n",
    "  \n",
    "4. **Step 3:**  **Improving the dataset by using transformations**\n",
    "   - log transformation, square root transformation, box-cox transformation etc. will be applied and analyzed one by one for not-numerical columns\n",
    "   - dataset is highly skewed (most of the columns), transformations might help mitigating this issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://github.com/gheisenberg/FoodSecurity/blob/main/DHS/dhs_modelling_functions.py\" target=\"_blank\">Fold Generator Function</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmse function\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the simple autoencoder model\n",
    "def build_autoencoder(input_dim):\n",
    "    autoencoder = models.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_dim=input_dim, kernel_initializer='he_uniform', activity_regularizer=regularizers.L2(0.001)),\n",
    "        layers.Dense(64, activation='relu', kernel_initializer='he_uniform', activity_regularizer=regularizers.L2(0.001)),\n",
    "        layers.Dense(10, activation='relu', kernel_initializer='he_uniform', name='bottleneck'),\n",
    "        layers.Dense(64, activation='relu', kernel_initializer='he_uniform'),\n",
    "        layers.Dense(128, activation='relu', kernel_initializer='he_uniform'),\n",
    "        layers.Dense(input_dim, activation='linear', kernel_initializer='he_uniform')  \n",
    "    ])\n",
    "    return autoencoder\n",
    "\n",
    "# setting up imputers\n",
    "imputers = [\n",
    "    {'name': 'Simple Mean Imputer', 'imputer': SimpleImputer(strategy='mean')},\n",
    "    {'name': 'Simple Median Imputer', 'imputer': SimpleImputer(strategy='median')},\n",
    "    #{'name': 'KNN Imputer', 'imputer': KNNImputer(n_neighbors=5)}, \n",
    "    #{'name': 'Random Forest Imputer', 'imputer': IterativeImputer(estimator=RandomForestRegressor(n_estimators=5), max_iter=10, random_state=0)} \n",
    "]\n",
    "\n",
    "numerical_only = df.select_dtypes(include=[np.number])\n",
    "print((numerical_only)) #for scaling purpose we will take only numerical ones\n",
    "                       # for now, we will do same scaling for all columns\n",
    "                       # later on, diff scalers can be used for nums and cat columns\n",
    "\n",
    "# imputation\n",
    "for i in imputers:\n",
    "    model = i['imputer']\n",
    "    first_imputed = model.fit_transform(numerical_only)\n",
    "    first_df_imputed = pd.DataFrame(first_imputed, columns=numerical_only.columns)\n",
    "    print(f\"Imputed data using {i['name']}:\")\n",
    "    print(first_df_imputed)\n",
    "\n",
    "    scale_all=True\n",
    "    # scaling data if flag is true\n",
    "    if scale_all:\n",
    "        scaler = StandardScaler()\n",
    "        first_df_imputed_scaled = scaler.fit_transform(first_df_imputed)\n",
    "        print('Scaled data:')\n",
    "        print(first_df_imputed_scaled) \n",
    "        \n",
    "# scaled data back to df\n",
    "df_scaled = pd.DataFrame(first_df_imputed_scaled, columns=numerical_only.columns)\n",
    "print('Scaled data back to df:', df_scaled)\n",
    "\n",
    "# extracting specific columns\n",
    "columns_to_extract = ['Meta; GEID_init', 'Meta; adm0_gaul']\n",
    "df_extracted = df[columns_to_extract]\n",
    "\n",
    "# adding extracted columns to the beginning of df_scaled\n",
    "df_scaled = pd.concat([df_extracted.reset_index(drop=True), df_scaled.reset_index(drop=True)], axis=1)\n",
    "print('joing scaled + geid init and adm0 altogether', df_scaled)\n",
    "        \n",
    "input_df=df_scaled.copy() # which will be considered as actual values and input of folds during cross-val\n",
    "print('input_df=df_scaled as input to fold generator',  input_df)\n",
    "\n",
    "fold_by = 'survey'\n",
    "# using fold generator\n",
    "fold_gen = fold_generator(input_df, fold_by, n_splits=5)\n",
    "\n",
    "# for 5 folds, iteration=5\n",
    "for fold, (train_index, test_index) in enumerate(fold_gen):\n",
    "    X_train, X_test = input_df.loc[train_index], input_df.loc[test_index]\n",
    "\n",
    "    # dropping categorical and fully avaiable cols which does not need imputation\n",
    "    dropping_cols = ['Meta; adm0_gaul', 'Meta; GEID_init', 'Meta; year']\n",
    "    X_train = X_train.drop(columns=dropping_cols)\n",
    "    X_test = X_test.drop(columns=dropping_cols)\n",
    "\n",
    "    # compiling the autoencoder model\n",
    "    autoencoder = build_autoencoder(X_train.shape[1])\n",
    "    autoencoder.compile(optimizer=optimizers.Adam(learning_rate=0.001), loss='mse', metrics=[root_mean_squared_error])\n",
    "\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=30)\n",
    "    history = autoencoder.fit(X_train, X_train, epochs=30, batch_size=32, shuffle=True, callbacks=[es], validation_data=(X_test, X_test))\n",
    "    X_pred = autoencoder.predict(X_test)\n",
    "    print('X_pred', X_pred)\n",
    "    print('X_test', X_test)\n",
    "\n",
    "    # to calculate metrics\n",
    "    def calculate_metrics(X_test, X_pred):\n",
    "        if isinstance(X_test, pd.DataFrame):\n",
    "            index_method = lambda x, i: x.iloc[:, i]  \n",
    "        else:\n",
    "            index_method = lambda x, i: x[:, i]       \n",
    "\n",
    "        for i in range(X_test.shape[1]):\n",
    "            mse = np.mean((index_method(X_test, i) - index_method(X_pred, i)) ** 2)\n",
    "            rmse = np.sqrt(mse)\n",
    "            r2 = r2_score(index_method(X_test, i), index_method(X_pred, i))\n",
    "            corr, _ = pearsonr(index_method(X_test, i), index_method(X_pred, i))\n",
    "            print(f\"Column {i+1} - MSE: {mse}, RMSE: {rmse}, R2: {r2}, Correlation: {corr}\")\n",
    "\n",
    "    def get_column(data, idx):\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            return data.iloc[:, idx]\n",
    "        elif isinstance(data, np.ndarray):\n",
    "            return data[:, idx]\n",
    "\n",
    "    num_features = X_test.shape[1] \n",
    "\n",
    "    # setting the figure and axes for a grid of plots\n",
    "    cols = 3  # number of columns in subplot grid\n",
    "    rows = (num_features + cols - 1) // cols  # rows needed\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(20, rows * 4 + 2))  \n",
    "    fig.subplots_adjust(hspace=0.5, wspace=0.3, top=0.95)\n",
    "\n",
    "    for i in range(num_features):\n",
    "        ax = axes.flatten()[i]  # flattening the axes array and access by single index\n",
    "        actual = get_column(X_test, i)\n",
    "        predicted = get_column(X_pred, i)\n",
    "\n",
    "        # pearson correlation, tmse, and r2 for each feature\n",
    "        corr, _ = pearsonr(actual, predicted)\n",
    "        mse = mean_squared_error(actual, predicted)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(actual, predicted)\n",
    "\n",
    "        column_name = X_test.columns[i] if isinstance(X_test, pd.DataFrame) else f\"Feature {i}\"\n",
    "        ax.scatter(actual, predicted, alpha=0.5)\n",
    "        ax.plot([actual.min(), actual.max()], [actual.min(), actual.max()], 'r--') \n",
    "        ax.set_xlabel('Actual Values')\n",
    "        ax.set_ylabel('Predicted Values')\n",
    "        ax.set_title(column_name) \n",
    "\n",
    "        # plotting rmse, r2, and correlation above the plot\n",
    "        ax.text(0.8, .25, f'R²: {r2:.2f}\\nRMSE: {rmse:.2f}\\nCorrelation: {corr:.2f}', verticalalignment='top', horizontalalignment='left', transform=ax.transAxes, fontsize=10, bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor='gray', facecolor='none'))\n",
    "\n",
    "    for j in range(i + 1, rows * cols):\n",
    "        axes.flatten()[j].axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final AE training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df_scaled.copy()\n",
    "#print('df1',  df1)\n",
    "\n",
    "# dropping categorical and fully avaiable cols which does not need imputation\n",
    "dropping_cols = ['Meta; adm0_gaul', 'Meta; GEID_init', 'Meta; year']\n",
    "df1 = df1.drop(columns=dropping_cols)\n",
    "print('df1',  df1)\n",
    "\n",
    "#splitting the data into train, test, and validation sets\n",
    "train1, test1 = train_test_split(df1, test_size=0.2, random_state=42)\n",
    "print('train1',  df1)\n",
    "print('test1',  df1)\n",
    "train1, val1= train_test_split(train1, test_size=0.2, random_state=42)\n",
    "actual_ae1=test1.copy()\n",
    "        \n",
    "# rmse function\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
    "\n",
    "# autoencoder final model training\n",
    "# train1 is already imputed with simple imputers and now it's fully completed without any NaNs\n",
    "input_dim = train1.shape[1]\n",
    "final_ae = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_dim=input_dim, kernel_initializer='he_uniform', activity_regularizer=regularizers.L2(0.001)),\n",
    "    layers.Dense(64, activation='relu', kernel_initializer='he_uniform', activity_regularizer=regularizers.L2(0.001)),\n",
    "    layers.Dense(10, activation='relu', kernel_initializer='he_uniform', name='bottleneck'), \n",
    "    layers.Dense(64, activation='relu', kernel_initializer='he_uniform'),\n",
    "    layers.Dense(128, activation='relu', kernel_initializer='he_uniform'),\n",
    "    layers.Dense(input_dim, activation='relu', kernel_initializer='he_uniform')\n",
    "])\n",
    "\n",
    "# compiling the model with the same optimizer and loss function\n",
    "final_ae.compile(optimizer=optimizers.Adam(learning_rate=0.001), loss='mse', metrics=[root_mean_squared_error])\n",
    "#final_ae.summary()\n",
    "\n",
    "missing_ratio=0.3\n",
    "\n",
    "# function to introduce missingness\n",
    "def introduce_missingness(data, missing_ratio):\n",
    "    mask = np.random.rand(*data.shape) < missing_ratio\n",
    "    data[mask] = np.nan\n",
    "    return data\n",
    "\n",
    "# introducing missingness at 0.3 for test dataset\n",
    "test_data_with_missing = introduce_missingness(test1.copy(), 0.3)\n",
    "test_data_imputed = test_data_with_missing.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# training the model\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=30)\n",
    "history = final_ae.fit(train1, train1, epochs=20, batch_size=32, shuffle=True, callbacks=[es], validation_split=0.2, verbose=1)\n",
    "\n",
    "# evaluating the model with rmse\n",
    "predicted_data = final_ae.predict(test_data_imputed) #test_data_imputed=actual data which is unseen test data\n",
    "mse = np.mean((test_data_imputed - predicted_data) ** 2) # actual data minus imputed data\n",
    "rmse = np.sqrt(mse)\n",
    "#print(f\"Final RMSE: {rmse}\")\n",
    "X_pred=predicted_data\n",
    "#print('X_pred', X_pred)\n",
    "X_test=test_data_imputed\n",
    "#print('X_test', X_test)\n",
    "\n",
    "# calculating metrics\n",
    "def calculate_metrics(X_test, X_pred):\n",
    "    if isinstance(X_test, pd.DataFrame):\n",
    "        index_method = lambda x, i: x.iloc[:, i]  \n",
    "    else:\n",
    "        index_method = lambda x, i: x[:, i]       \n",
    "\n",
    "    for i in range(X_test.shape[1]):\n",
    "        mse = np.mean((index_method(X_test, i) - index_method(X_pred, i)) ** 2)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(index_method(X_test, i), index_method(X_pred, i))\n",
    "        corr, _ = pearsonr(index_method(X_test, i), index_method(X_pred, i))\n",
    "\n",
    "        print(f\"Column {i+1} - MSE: {mse}, RMSE: {rmse}, R2: {r2}, Correlation: {corr}\")\n",
    "\n",
    "def get_column(data, idx):\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        return data.iloc[:, idx]\n",
    "    elif isinstance(data, np.ndarray):\n",
    "        return data[:, idx]\n",
    "\n",
    "num_features = X_test.shape[1] \n",
    "\n",
    "# setting the figure and axes for a grid of plots\n",
    "cols = 3  # number of columns in subplot grid\n",
    "rows = (num_features + cols - 1) // cols  # rows needed\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(20, rows * 4 + 2))  \n",
    "fig.subplots_adjust(hspace=0.5, wspace=0.3, top=0.95)\n",
    "\n",
    "for i in range(num_features):\n",
    "    ax = axes.flatten()[i]  # flattening the axes array and access by single index\n",
    "    actual = get_column(X_test, i)\n",
    "    predicted = get_column(X_pred, i)\n",
    "\n",
    "    # pearson correlation, rmse, r2 for each feature\n",
    "    corr, _ = pearsonr(actual, predicted)\n",
    "    mse = mean_squared_error(actual, predicted)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(actual, predicted)\n",
    "\n",
    "    column_name = numerical_only.columns[i] if isinstance(X_test, pd.DataFrame) else f\"Feature {i}\"\n",
    "    ax.scatter(actual, predicted, alpha=0.5)\n",
    "    ax.plot([actual.min(), actual.max()], [actual.min(), actual.max()], 'r--') \n",
    "    ax.set_xlabel('Actual Values')\n",
    "    ax.set_ylabel('Predicted Values')\n",
    "    ax.set_title(column_name) \n",
    "\n",
    "    # plotting rmse, r2 score, and correlation above the plot\n",
    "    ax.text(0.8, .25, f'R²: {r2:.2f}\\nRMSE: {rmse:.2f}\\nCorrelation: {corr:.2f}', verticalalignment='top', horizontalalignment='left', transform=ax.transAxes, fontsize=10, bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor='gray', facecolor='none'))\n",
    "\n",
    "for j in range(i + 1, rows * cols):\n",
    "    axes.flatten()[j].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments:\n",
    "1. r2=1 denotes to the situation when model is best at predicting the dependant variable (or doing correct imputation on unseen data).\n",
    "In our case, the negative r2 scores for several columns have to be improved near to 1. More improvements have to be done gradually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nlKRycjXtTxR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3dPN590tTxS"
   },
   "source": [
    "<h1><center> <div class=\"alert alert-success\"> De-noising Autoencoder </div> </center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPFb2KnQtTxd"
   },
   "source": [
    "\n",
    "<h1><center> <div class=\"alert alert-warning\"> Variational Autoencoder </div> </center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BetSpMMhySx-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H-P-vH7XyS1M"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sTJA1O-CyS4z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bFgFCv2-yS6G"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4BkKiMOUtTxg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4WfFX3LWwUqc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "22WDv1IDwUtM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5inezS83wUvx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9WZD-paYwUy8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MsfXgDnWwU1w"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mZ_iyGimwU44"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Hj9K_FNwU7g"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TX1zmKUiwU_C"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MYOvUP8EwVCT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MhPzE2ZswVFB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ukMFNYU-wVHk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Os0aLk_AwVb3"
   },
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
